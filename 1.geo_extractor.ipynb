{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3ee6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import requests\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "class NorthKoreaLocationExtractor:\n",
    "    def __init__(self, model_name=\"ko_core_news_lg\"):\n",
    "        \"\"\"\n",
    "        북한 관련 지역, 장소, 건물 정보를 추출하는 클래스\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.nlp = spacy.load(model_name)\n",
    "            print(f\"✅ {model_name} 모델이 성공적으로 로드되었습니다.\")\n",
    "        except OSError:\n",
    "            print(f\"❌ {model_name} 모델을 찾을 수 없습니다.\")\n",
    "            print(\"다음 명령어로 설치해주세요:\")\n",
    "            if model_name.startswith(\"ko\"):\n",
    "                print(\"pip install spacy\")\n",
    "                print(\"python -m spacy download ko_core_news_lg\")\n",
    "            else:\n",
    "                print(\"pip install spacy\")\n",
    "                print(\"python -m spacy download en_core_web_sm\")\n",
    "            raise\n",
    "            \n",
    "        # JSON 파일 경로 정의\n",
    "        self.location_file = '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_region.json'\n",
    "        \n",
    "        # 조합에 사용할 군사 시설 사전 경로\n",
    "        self.military_facility_file = '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_military.json'\n",
    "        \n",
    "        # 다른 시설 사전 경로\n",
    "        self.other_facility_files = [\n",
    "            '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_military.json',\n",
    "            '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_edu_cul.json',\n",
    "            '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_landmark.json',\n",
    "            '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_politics.json',\n",
    "            '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_edu_cul.json'\n",
    "        ]\n",
    "        \n",
    "        # JSON 파일에서 사전 데이터 불러오기\n",
    "        self.nk_locations = self._load_json_to_set([self.location_file], \"북한 지역명\")\n",
    "        \n",
    "        # 군사 시설 사전만 별도로 로드\n",
    "        self.nk_military_facilities = self._load_json_to_set([self.military_facility_file], \"북한 군사 시설\")\n",
    "        \n",
    "        # 전체 시설 사전을 통합하여 개별 단어 추출에 사용\n",
    "        all_facility_files = [self.military_facility_file] + self.other_facility_files\n",
    "        self.nk_facilities = self._load_json_to_set(all_facility_files, \"북한 전체 시설/건물명\")\n",
    "\n",
    "        # 북한 관련 키워드 (맥락 판단용)\n",
    "        self.nk_keywords = {\n",
    "            '북한', '조선민주주의인민공화국', '조선', 'DPRK',\n",
    "            '김정은', '김정일', '김일성', '김여정',\n",
    "            '조선로동당', '노동당', '최고지도자', '원수님', '위원장',\n",
    "            '핵실험', '미사일', '로켓', '인공위성', '탄도미사일',\n",
    "            '대남', '남조선', '통일', '6자회담'\n",
    "        }\n",
    "        \n",
    "        # 제외할 일반 단어들 (오탐 방지)\n",
    "        self.exclude_words = {\n",
    "            '해주', '순천', '개천', '성천', '신천', '안주', \n",
    "            '강계', '회창', '온천', '영원', '신원', '고원', \n",
    "            '대흥', '신양', '봉천', '송화', '과일', '신흥',\n",
    "            '덕성', '영광', '고성', '철원', '평강', '김화'\n",
    "        }\n",
    "        \n",
    "        # 맥락 패턴 (정규식)\n",
    "        self.nk_context_patterns = [\n",
    "            r'북한.*?([가-힣]+(?:시|군|구|동|리))', \n",
    "            r'조선.*?([가-힣]+(?:시|군|구|동|리))', \n",
    "            r'DPRK.*?([A-Za-z가-힣]+)',\n",
    "            r'김정은.*?([가-힣]+(?:시|군|구|리))', \n",
    "            r'평양.*?([가-힣]+(?:구|동|리))', \n",
    "        ]\n",
    "\n",
    "    def _load_json_to_set(self, file_paths, dict_name):\n",
    "        \"\"\"\n",
    "        JSON 파일(들)을 읽어 세트(set)로 변환\n",
    "        \"\"\"\n",
    "        data_set = set()\n",
    "        for path in file_paths:\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"⚠️ 경고: {dict_name} 사전 파일 '{path}'을(를) 찾을 수 없습니다. 해당 사전은 비어있습니다.\")\n",
    "                continue\n",
    "            try:\n",
    "                with open(path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    if isinstance(data, list):\n",
    "                        data_set.update(data)\n",
    "                    else:\n",
    "                        print(f\"❌ 오류: '{path}' 파일의 형식이 올바르지 않습니다. 리스트 형태여야 합니다.\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"❌ 오류: '{path}' 파일의 JSON 형식이 올바르지 않습니다.\")\n",
    "        return data_set\n",
    "\n",
    "    def _extract_combined_locations_with_regex(self, text):\n",
    "        found_combined = []\n",
    "\n",
    "        # 지역명과 군사 시설명 패턴만 사용하여 조합\n",
    "        nk_locations_pattern = '|'.join(re.escape(loc) for loc in self.nk_locations)\n",
    "        nk_facilities_pattern = '|'.join(re.escape(fac) for fac in self.nk_military_facilities)\n",
    "\n",
    "        # '지역명'과 '시설명'이 인접한 패턴을 검색 (띄어쓰기 허용)\n",
    "        combined_pattern = fr'({nk_locations_pattern})\\s*({nk_facilities_pattern})'\n",
    "        \n",
    "        sentences = re.split(r'[.!?]\\s+', text)\n",
    "        for sentence in sentences:\n",
    "            if len(sentence.strip()) < 10:\n",
    "                continue\n",
    "            \n",
    "            for match in re.finditer(combined_pattern, sentence):\n",
    "                location = match.group(1)\n",
    "                facility = match.group(2)\n",
    "                combined_name = f\"{location}{' ' if ' ' in sentence[match.start():match.end()] else ''}{facility}\"\n",
    "                \n",
    "                # 중복 및 맥락 검증\n",
    "                if self._is_valid_context(sentence, location):\n",
    "                    found_combined.append({\n",
    "                        'location': combined_name,\n",
    "                        'context': sentence,\n",
    "                        'confidence': 'high',\n",
    "                        'type': 'combined_facility'\n",
    "                    })\n",
    "        \n",
    "        return found_combined\n",
    "\n",
    "    def _is_valid_context(self, sentence, location):\n",
    "        \"\"\"\n",
    "        문맥상 해당 지역이 북한과 관련있는지 판단\n",
    "        \"\"\"\n",
    "        sentence_lower = sentence.lower()\n",
    "        location_lower = location.lower()\n",
    "        \n",
    "        for keyword in self.nk_keywords:\n",
    "            if keyword.lower() in sentence_lower:\n",
    "                return True\n",
    "        \n",
    "        clear_nk_locations = {'평양', '김정은', '조선로동당', '노동당'}\n",
    "        for nk_loc in clear_nk_locations:\n",
    "            if nk_loc in sentence and location in sentence:\n",
    "                return True\n",
    "        \n",
    "        negative_keywords = ['한국', '남한', '우리나라', '국내', '서울', '부산']\n",
    "        for neg_keyword in negative_keywords:\n",
    "            if neg_keyword in sentence and location in sentence:\n",
    "                return False\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def _extract_with_spacy_ner(self, text):\n",
    "        \"\"\"\n",
    "        spaCy NER을 사용한 지역 추출 (개선된 버전)\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        locations = []\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in ['GPE', 'LOC']:\n",
    "                location = ent.text.strip()\n",
    "                sentence = ent.sent.text\n",
    "                if self._is_valid_context(sentence, location):\n",
    "                    locations.append({\n",
    "                        'location': location,\n",
    "                        'context': sentence,\n",
    "                        'confidence': 'high'\n",
    "                    })\n",
    "        \n",
    "        return locations\n",
    "\n",
    "    def _extract_with_dictionary(self, text):\n",
    "        \"\"\"\n",
    "        사전 기반 추출 (개선된 버전)\n",
    "        \"\"\"\n",
    "        sentences = re.split(r'[.!?]\\s+', text)\n",
    "        found_locations = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if len(sentence.strip()) < 10:\n",
    "                continue\n",
    "                \n",
    "            # 북한 지역명 검색\n",
    "            for location in self.nk_locations:\n",
    "                if location in sentence:\n",
    "                    if location in self.exclude_words:\n",
    "                        if self._is_valid_context(sentence, location):\n",
    "                            found_locations.append({\n",
    "                                'location': location,\n",
    "                                'context': sentence,\n",
    "                                'confidence': 'medium',\n",
    "                                'type': 'nk_location'\n",
    "                            })\n",
    "                    else:\n",
    "                        found_locations.append({\n",
    "                            'location': location,\n",
    "                            'context': sentence,\n",
    "                            'confidence': 'high',\n",
    "                            'type': 'nk_location'\n",
    "                        })\n",
    "            \n",
    "            # 북한 시설명 검색\n",
    "            for facility in self.nk_facilities:\n",
    "                if facility in sentence:\n",
    "                    found_locations.append({\n",
    "                        'location': facility,\n",
    "                        'context': sentence,\n",
    "                        'confidence': 'high',\n",
    "                        'type': 'nk_facility'\n",
    "                    })\n",
    "        \n",
    "        return found_locations\n",
    "\n",
    "    def _filter_by_confidence(self, locations, min_confidence='medium'):\n",
    "        \"\"\"\n",
    "        신뢰도에 따른 필터링\n",
    "        \"\"\"\n",
    "        confidence_levels = {'low': 1, 'medium': 2, 'high': 3}\n",
    "        min_level = confidence_levels.get(min_confidence, 2)\n",
    "        \n",
    "        filtered = []\n",
    "        for loc in locations:\n",
    "            loc_level = confidence_levels.get(loc.get('confidence', 'low'), 1)\n",
    "            if loc_level >= min_level:\n",
    "                filtered.append(loc)\n",
    "        \n",
    "        return filtered\n",
    "\n",
    "    def extract_nk_locations(self, text):\n",
    "        \"\"\"\n",
    "        텍스트에서 북한 관련 지역/장소 정보 추출 (개선된 버전)\n",
    "        \"\"\"\n",
    "        # 1. spaCy NER 사용\n",
    "        spacy_results = self._extract_with_spacy_ner(text)\n",
    "        \n",
    "        # 2. 사전 기반 추출 (개별 단어)\n",
    "        dict_results = self._extract_with_dictionary(text)\n",
    "        \n",
    "        # 3. 조합된 단어 추출 (지역 + 군사시설)\n",
    "        combined_results = self._extract_combined_locations_with_regex(text)\n",
    "        \n",
    "        # 4. 결과 통합 및 중복 제거\n",
    "        all_results = spacy_results + dict_results + combined_results\n",
    "        unique_locations = {}\n",
    "        \n",
    "        for result in all_results:\n",
    "            location = result['location']\n",
    "            if location not in unique_locations:\n",
    "                unique_locations[location] = result\n",
    "            else:\n",
    "                if result.get('confidence') == 'high' and unique_locations[location].get('confidence') != 'high':\n",
    "                    unique_locations[location] = result\n",
    "        \n",
    "        # 5. 신뢰도 필터링\n",
    "        filtered_results = self._filter_by_confidence(\n",
    "            list(unique_locations.values()), \n",
    "            min_confidence='medium'\n",
    "        )\n",
    "        \n",
    "        # 6. 북한 관련 키워드 확인\n",
    "        nk_context_keywords = [keyword for keyword in self.nk_keywords if keyword in text]\n",
    "        \n",
    "        # 7. 관련성 점수 계산\n",
    "        relevance_score = self._calculate_relevance_score(text, filtered_results)\n",
    "        \n",
    "        return {\n",
    "            'locations': filtered_results,\n",
    "            'nk_context_keywords': nk_context_keywords,\n",
    "            'has_nk_context': len(nk_context_keywords) > 0,\n",
    "            'relevance_score': relevance_score,\n",
    "            'total_found': len(filtered_results)\n",
    "        }\n",
    "\n",
    "    def _calculate_relevance_score(self, text, locations):\n",
    "        \"\"\"\n",
    "        개선된 관련성 점수 계산\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        keyword_count = sum(1 for keyword in self.nk_keywords if keyword in text)\n",
    "        score += min(keyword_count * 8, 40)\n",
    "        \n",
    "        high_conf_count = sum(1 for loc in locations if loc.get('confidence') == 'high')\n",
    "        score += min(high_conf_count * 10, 40)\n",
    "        \n",
    "        med_conf_count = sum(1 for loc in locations if loc.get('confidence') == 'medium')\n",
    "        score += min(med_conf_count * 5, 20)\n",
    "        \n",
    "        return min(score, 100)\n",
    "\n",
    "    def analyze_news_article(self, text, title=\"\"):\n",
    "        \"\"\"\n",
    "        뉴스 기사를 종합적으로 분석 (개선된 버전)\n",
    "        \"\"\"\n",
    "        full_text = f\"{title} {text}\" if title else text\n",
    "        results = self.extract_nk_locations(full_text)\n",
    "        \n",
    "        summary = {\n",
    "            'title': title,\n",
    "            'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'text_length': len(full_text),\n",
    "            'relevance_score': results['relevance_score'],\n",
    "            'is_nk_related': results['has_nk_context'] and results['relevance_score'] > 30,\n",
    "            'locations_found': results['locations'],\n",
    "            'confidence_distribution': self._get_confidence_distribution(results['locations']),\n",
    "            'full_results': results\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "    def _get_confidence_distribution(self, locations):\n",
    "        \"\"\"\n",
    "        신뢰도별 분포 계산\n",
    "        \"\"\"\n",
    "        distribution = {'high': 0, 'medium': 0, 'low': 0}\n",
    "        for loc in locations:\n",
    "            confidence = loc.get('confidence', 'low')\n",
    "            distribution[confidence] += 1\n",
    "        return distribution\n",
    "\n",
    "    def print_analysis_results(self, results):\n",
    "        \"\"\"\n",
    "        개선된 분석 결과 출력\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"🏴‍☠️ 북한 관련 지역·장소 분석 결과 \")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"📊 북한 관련성 점수: {results['relevance_score']}/100\")\n",
    "        print(f\"🎯 북한 관련 기사 여부: {'✅ 예' if results['is_nk_related'] else '❌ 아니오'}\")\n",
    "        \n",
    "        if results['locations_found']:\n",
    "            print(f\"\\n📍 발견된 위치 정보 ({len(results['locations_found'])}개):\")\n",
    "            \n",
    "            high_conf = [loc for loc in results['locations_found'] if loc.get('confidence') == 'high']\n",
    "            med_conf = [loc for loc in results['locations_found'] if loc.get('confidence') == 'medium']\n",
    "            \n",
    "            if high_conf:\n",
    "                print(f\"\\n🟢 높은 신뢰도 ({len(high_conf)}개):\")\n",
    "                for loc in high_conf:\n",
    "                    location_type = loc.get('type', 'unknown')\n",
    "                    type_icon = '🏛️' if location_type == 'nk_facility' else '🌍'\n",
    "                    print(f\"   {type_icon} {loc['location']} (타입: {location_type})\")\n",
    "                    print(f\"     맥락: {loc['context'][:80]}...\")\n",
    "            \n",
    "            if med_conf:\n",
    "                print(f\"\\n🟡 중간 신뢰도 ({len(med_conf)}개):\")\n",
    "                for loc in med_conf:\n",
    "                    location_type = loc.get('type', 'unknown')\n",
    "                    type_icon = '🏛️' if location_type == 'nk_facility' else '🌍'\n",
    "                    print(f\"   {type_icon} {loc['location']} (타입: {location_type})\")\n",
    "                    print(f\"     맥락: {loc['context'][:80]}...\")\n",
    "        else:\n",
    "            print(f\"\\n❌ 북한 관련 지역/장소 정보를 찾을 수 없습니다.\")\n",
    "        \n",
    "        if results['full_results']['nk_context_keywords']:\n",
    "            print(f\"\\n🔑 북한 관련 키워드:\")\n",
    "            for keyword in results['full_results']['nk_context_keywords'][:10]:\n",
    "                print(f\"   • {keyword}\")\n",
    "\n",
    "def analyze_sample_file(extractor, filename=\"/home/ds4_sia_nolb/#FINAL_POLARIS/04_plus_preprocessing/preprocessing_final_data/re_final_preprocessing.json\"):\n",
    "    \"\"\"\n",
    "    샘플 JSON 파일을 분석하여 지역 정보 추출 (개선된 버전)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            articles = json.load(f)\n",
    "        \n",
    "        print(f\"📁 {filename} 파일에서 {len(articles)}개 기사를 읽었습니다.\")\n",
    "        \n",
    "        extracted_results = []\n",
    "        \n",
    "        for i, article in enumerate(articles, 1):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"📰 기사 {i}/{len(articles)}: {article['metadata']['title']}\")\n",
    "            print(f\"🆔 ID: {article['id_']}\")\n",
    "            print(f\"📅 날짜: {article['metadata']['pubDate']}\")\n",
    "            print(f\"🏷️ 카테고리: {article['metadata']['category']}\")\n",
    "            \n",
    "            full_text = f\"{article['metadata']['title']} {article['text']} {article.get('summary', '')}\"\n",
    "            \n",
    "            results = extractor.analyze_news_article(full_text, article['metadata']['title'])\n",
    "            \n",
    "            extractor.print_analysis_results(results)\n",
    "            \n",
    "            if results['locations_found'] and results['is_nk_related']:\n",
    "                location_names = [loc['location'] for loc in results['locations_found']]\n",
    "                extracted_results.append({\n",
    "                    'id_': article['id_'],\n",
    "                    'locations': location_names,\n",
    "                    'confidence_info': [\n",
    "                        {\n",
    "                            'location': loc['location'],\n",
    "                            'confidence': loc['confidence'],\n",
    "                            'type': loc.get('type', 'unknown')\n",
    "                        }\n",
    "                        for loc in results['locations_found']\n",
    "                    ]\n",
    "                })\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"🎯 전체 분석 요약\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"📊 분석된 기사 수: {len(articles)}개\")\n",
    "        print(f\"📍 지역정보 발견 기사 수: {len(extracted_results)}개\")\n",
    "        \n",
    "        if extracted_results:\n",
    "            print(f\"\\n📋 추출된 결과 (JSON 형태):\")\n",
    "            print(json.dumps(extracted_results, ensure_ascii=False, indent=2))\n",
    "            \n",
    "            output_filename = \"test_extracted_locations_improved3_2017_01_03.json\"\n",
    "            with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(extracted_results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"\\n💾 결과가 {output_filename} 파일로 저장되었습니다.\")\n",
    "        else:\n",
    "            print(\"\\n❌ 북한 관련 지역 정보가 발견된 기사가 없습니다.\")\n",
    "            \n",
    "        return extracted_results\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ 오류: {filename} 파일을 찾을 수 없습니다.\")\n",
    "        print(\"파일이 Python 스크립트와 같은 디렉터리에 있는지 확인해주세요.\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"❌ 오류: {filename} 파일의 JSON 형식이 올바르지 않습니다.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류: {e}\")\n",
    "        return []\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    메인 실행 함수\n",
    "    \"\"\"\n",
    "    print(\"🏴‍☠️ 북한 관련 지역·장소 추출기 (개선 버전)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        extractor = NorthKoreaLocationExtractor(\"ko_core_news_lg\")\n",
    "    except:\n",
    "        print(\"\\n중형 모델로 시도해보겠습니다...\")\n",
    "        try:\n",
    "            extractor = NorthKoreaLocationExtractor(\"ko_core_news_md\")\n",
    "        except:\n",
    "            print(\"\\n소형 모델로 시도해보겠습니다...\")\n",
    "            try:\n",
    "                extractor = NorthKoreaLocationExtractor(\"ko_core_news_sm\")\n",
    "            except:\n",
    "                print(\"\\n영어 모델로 시도해보겠습니다...\")\n",
    "                try:\n",
    "                    extractor = NorthKoreaLocationExtractor(\"en_core_web_sm\")\n",
    "                except:\n",
    "                    print(\"spaCy 모델을 설치한 후 다시 실행해주세요.\")\n",
    "                    return\n",
    "    \n",
    "    print(\"\\n🔄 샘플 파일 분석을 시작합니다...\")\n",
    "    extracted_results = analyze_sample_file(extractor)\n",
    "    \n",
    "    if extracted_results:\n",
    "        print(f\"\\n✅ 분석 완료! {len(extracted_results)}개 기사에서 신뢰할 수 있는 지역 정보를 추출했습니다.\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  분석은 완료되었지만 신뢰할 수 있는 북한 관련 지역 정보가 발견되지 않았습니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56a25069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏴‍☠️ 북한 관련 지역·장소 추출기 (최적화 버전)\n",
      "============================================================\n",
      "✅ GPU 사용을 시도합니다.\n",
      "📁 /home/ds4_sia_nolb/#FINAL_POLARIS/04_plus_preprocessing/preprocessing_final_data/re_final_preprocessing.json 파일에서 80434개 기사를 읽었습니다.\n",
      "\n",
      "🔄 총 80434개 기사를 배치 단위로 분석합니다...\n",
      "📊 1000/80434개 기사 분석 완료. (경과 시간: 40.00초)\n",
      "📊 2000/80434개 기사 분석 완료. (경과 시간: 62.72초)\n",
      "📊 3000/80434개 기사 분석 완료. (경과 시간: 86.30초)\n",
      "📊 4000/80434개 기사 분석 완료. (경과 시간: 109.16초)\n",
      "📊 5000/80434개 기사 분석 완료. (경과 시간: 133.76초)\n",
      "📊 6000/80434개 기사 분석 완료. (경과 시간: 159.09초)\n",
      "📊 7000/80434개 기사 분석 완료. (경과 시간: 183.98초)\n",
      "📊 8000/80434개 기사 분석 완료. (경과 시간: 207.82초)\n",
      "📊 9000/80434개 기사 분석 완료. (경과 시간: 231.85초)\n",
      "📊 10000/80434개 기사 분석 완료. (경과 시간: 257.49초)\n",
      "📊 11000/80434개 기사 분석 완료. (경과 시간: 281.12초)\n",
      "📊 12000/80434개 기사 분석 완료. (경과 시간: 305.94초)\n",
      "📊 13000/80434개 기사 분석 완료. (경과 시간: 330.40초)\n",
      "📊 14000/80434개 기사 분석 완료. (경과 시간: 355.00초)\n",
      "📊 15000/80434개 기사 분석 완료. (경과 시간: 379.63초)\n",
      "📊 16000/80434개 기사 분석 완료. (경과 시간: 404.74초)\n",
      "📊 17000/80434개 기사 분석 완료. (경과 시간: 431.01초)\n",
      "📊 18000/80434개 기사 분석 완료. (경과 시간: 456.66초)\n",
      "📊 19000/80434개 기사 분석 완료. (경과 시간: 481.58초)\n",
      "📊 20000/80434개 기사 분석 완료. (경과 시간: 504.92초)\n",
      "📊 21000/80434개 기사 분석 완료. (경과 시간: 527.39초)\n",
      "📊 22000/80434개 기사 분석 완료. (경과 시간: 549.68초)\n",
      "📊 23000/80434개 기사 분석 완료. (경과 시간: 572.43초)\n",
      "📊 24000/80434개 기사 분석 완료. (경과 시간: 593.46초)\n",
      "📊 25000/80434개 기사 분석 완료. (경과 시간: 617.13초)\n",
      "📊 26000/80434개 기사 분석 완료. (경과 시간: 639.37초)\n",
      "📊 27000/80434개 기사 분석 완료. (경과 시간: 661.83초)\n",
      "📊 28000/80434개 기사 분석 완료. (경과 시간: 685.03초)\n",
      "📊 29000/80434개 기사 분석 완료. (경과 시간: 709.25초)\n",
      "📊 30000/80434개 기사 분석 완료. (경과 시간: 734.12초)\n",
      "📊 31000/80434개 기사 분석 완료. (경과 시간: 760.01초)\n",
      "📊 32000/80434개 기사 분석 완료. (경과 시간: 784.95초)\n",
      "📊 33000/80434개 기사 분석 완료. (경과 시간: 809.25초)\n",
      "📊 34000/80434개 기사 분석 완료. (경과 시간: 833.71초)\n",
      "📊 35000/80434개 기사 분석 완료. (경과 시간: 858.61초)\n",
      "📊 36000/80434개 기사 분석 완료. (경과 시간: 882.96초)\n",
      "📊 37000/80434개 기사 분석 완료. (경과 시간: 907.30초)\n",
      "📊 38000/80434개 기사 분석 완료. (경과 시간: 931.61초)\n",
      "📊 39000/80434개 기사 분석 완료. (경과 시간: 957.12초)\n",
      "📊 40000/80434개 기사 분석 완료. (경과 시간: 982.80초)\n",
      "📊 41000/80434개 기사 분석 완료. (경과 시간: 1008.69초)\n",
      "📊 42000/80434개 기사 분석 완료. (경과 시간: 1033.71초)\n",
      "📊 43000/80434개 기사 분석 완료. (경과 시간: 1057.64초)\n",
      "📊 44000/80434개 기사 분석 완료. (경과 시간: 1082.53초)\n",
      "📊 45000/80434개 기사 분석 완료. (경과 시간: 1105.89초)\n",
      "📊 46000/80434개 기사 분석 완료. (경과 시간: 1128.59초)\n",
      "📊 47000/80434개 기사 분석 완료. (경과 시간: 1150.53초)\n",
      "📊 48000/80434개 기사 분석 완료. (경과 시간: 1172.66초)\n",
      "📊 49000/80434개 기사 분석 완료. (경과 시간: 1195.99초)\n",
      "📊 50000/80434개 기사 분석 완료. (경과 시간: 1220.16초)\n",
      "📊 51000/80434개 기사 분석 완료. (경과 시간: 1243.29초)\n",
      "📊 52000/80434개 기사 분석 완료. (경과 시간: 1265.38초)\n",
      "📊 53000/80434개 기사 분석 완료. (경과 시간: 1288.53초)\n",
      "📊 54000/80434개 기사 분석 완료. (경과 시간: 1313.04초)\n",
      "📊 55000/80434개 기사 분석 완료. (경과 시간: 1336.04초)\n",
      "📊 56000/80434개 기사 분석 완료. (경과 시간: 1358.31초)\n",
      "📊 57000/80434개 기사 분석 완료. (경과 시간: 1381.07초)\n",
      "📊 58000/80434개 기사 분석 완료. (경과 시간: 1404.54초)\n",
      "📊 59000/80434개 기사 분석 완료. (경과 시간: 1427.73초)\n",
      "📊 60000/80434개 기사 분석 완료. (경과 시간: 1450.32초)\n",
      "📊 61000/80434개 기사 분석 완료. (경과 시간: 1473.01초)\n",
      "📊 62000/80434개 기사 분석 완료. (경과 시간: 1494.76초)\n",
      "📊 63000/80434개 기사 분석 완료. (경과 시간: 1516.52초)\n",
      "📊 64000/80434개 기사 분석 완료. (경과 시간: 1537.99초)\n",
      "📊 65000/80434개 기사 분석 완료. (경과 시간: 1559.54초)\n",
      "📊 66000/80434개 기사 분석 완료. (경과 시간: 1581.97초)\n",
      "📊 67000/80434개 기사 분석 완료. (경과 시간: 1606.84초)\n",
      "📊 68000/80434개 기사 분석 완료. (경과 시간: 1630.68초)\n",
      "📊 69000/80434개 기사 분석 완료. (경과 시간: 1654.59초)\n",
      "📊 70000/80434개 기사 분석 완료. (경과 시간: 1679.89초)\n",
      "📊 71000/80434개 기사 분석 완료. (경과 시간: 1704.42초)\n",
      "📊 72000/80434개 기사 분석 완료. (경과 시간: 1727.53초)\n",
      "📊 73000/80434개 기사 분석 완료. (경과 시간: 1751.59초)\n",
      "📊 74000/80434개 기사 분석 완료. (경과 시간: 1775.36초)\n",
      "📊 75000/80434개 기사 분석 완료. (경과 시간: 1799.69초)\n",
      "📊 76000/80434개 기사 분석 완료. (경과 시간: 1823.58초)\n",
      "📊 77000/80434개 기사 분석 완료. (경과 시간: 1847.23초)\n",
      "📊 78000/80434개 기사 분석 완료. (경과 시간: 1870.71초)\n",
      "📊 79000/80434개 기사 분석 완료. (경과 시간: 1894.74초)\n",
      "📊 80000/80434개 기사 분석 완료. (경과 시간: 1917.74초)\n",
      "📊 80434/80434개 기사 분석 완료. (경과 시간: 1927.95초)\n",
      "\n",
      "✅ 전체 분석 완료! 총 80434개 기사 분석에 1927.95초 소요되었습니다.\n",
      "💾 결과가 re_extracted_locations_ten_year_all.jsonl 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.pipeline import EntityRuler\n",
    "import requests\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "class NorthKoreaLocationExtractor:\n",
    "    def __init__(self, model_name=\"ko_core_news_lg\"):\n",
    "        \"\"\"\n",
    "        북한 관련 지역, 장소, 건물 정보를 추출하는 클래스\n",
    "        \"\"\"\n",
    "        try:\n",
    "            spacy.prefer_gpu()\n",
    "            print(\"✅ GPU 사용을 시도합니다.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ GPU 활성화 실패: {e}\")\n",
    "            print(\"CPU를 사용합니다.\")\n",
    "\n",
    "        try:\n",
    "            self.nlp = spacy.load(model_name)\n",
    "            if self.nlp.meta['name'] == model_name:\n",
    "                if spacy.prefer_gpu():\n",
    "                    print(f\"✅ {model_name} 모델이 GPU에서 성공적으로 로드되었습니다.\")\n",
    "                else:\n",
    "                    print(f\"✅ {model_name} 모델이 CPU에서 성공적으로 로드되었습니다.\")\n",
    "        except OSError:\n",
    "            print(f\"❌ {model_name} 모델을 찾을 수 없습니다.\")\n",
    "            print(\"다음 명령어로 설치해주세요:\")\n",
    "            if model_name.startswith(\"ko\"):\n",
    "                print(\"pip install spacy\")\n",
    "                print(\"python -m spacy download ko_core_news_lg\")\n",
    "            else:\n",
    "                print(\"pip install spacy\")\n",
    "                print(\"python -m spacy download en_core_web_sm\")\n",
    "            raise\n",
    "\n",
    "        # JSON 파일 경로 정의\n",
    "        self.location_file = '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_region.json'\n",
    "        \n",
    "        # 조합에 사용할 군사 시설 사전 경로\n",
    "        self.military_facility_file = '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_military.json'\n",
    "        \n",
    "        # 다른 시설 사전 경로\n",
    "        self.other_facility_files = [\n",
    "            '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_military.json',\n",
    "            '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_edu_cul.json',\n",
    "            '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_landmark.json',\n",
    "            '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_politics.json',\n",
    "            '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_edu_cul.json'\n",
    "        ]\n",
    "\n",
    "        # JSON 파일에서 사전 데이터 불러오기\n",
    "        self.nk_locations = self._load_json_to_set([self.location_file], \"북한 지역명\")\n",
    "        self.nk_military_facilities = self._load_json_to_set([self.military_facility_file], \"북한 군사 시설\")\n",
    "        all_facility_files = [self.military_facility_file] + self.other_facility_files\n",
    "        self.nk_facilities = self._load_json_to_set(all_facility_files, \"북한 전체 시설/건물명\")\n",
    "\n",
    "        # Flashtext 키워드 프로세서 초기화\n",
    "        self.keyword_processor_locations = KeywordProcessor()\n",
    "        self.keyword_processor_locations.add_keywords_from_list(list(self.nk_locations))\n",
    "        self.keyword_processor_facilities = KeywordProcessor()\n",
    "        self.keyword_processor_facilities.add_keywords_from_list(list(self.nk_facilities))\n",
    "\n",
    "        # 북한 관련 키워드 (맥락 판단용)\n",
    "        self.nk_keywords = {\n",
    "            '북한', '조선민주주의인민공화국', '조선', 'DPRK',\n",
    "            '김정은', '김정일', '김일성', '김여정',\n",
    "            '조선로동당', '노동당', '최고지도자', '원수님', '위원장',\n",
    "            '핵실험', '미사일', '로켓', '인공위성', '탄도미사일',\n",
    "            '대남', '남조선', '통일', '6자회담'\n",
    "        }\n",
    "        \n",
    "        # 제외할 일반 단어들 (오탐 방지)\n",
    "        self.exclude_words = {\n",
    "            '해주', '순천', '개천', '성천', '신천', '안주', \n",
    "            '강계', '회창', '온천', '영원', '신원', '고원', \n",
    "            '대흥', '신양', '봉천', '송화', '과일', '신흥',\n",
    "            '덕성', '영광', '고성', '철원', '평강', '김화'\n",
    "        }\n",
    "\n",
    "    def _load_json_to_set(self, file_paths, dict_name):\n",
    "        \"\"\"\n",
    "        JSON 파일(들)을 읽어 세트(set)로 변환\n",
    "        \"\"\"\n",
    "        data_set = set()\n",
    "        for path in file_paths:\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"⚠️ 경고: {dict_name} 사전 파일 '{path}'을(를) 찾을 수 없습니다. 해당 사전은 비어있습니다.\")\n",
    "                continue\n",
    "            try:\n",
    "                with open(path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    if isinstance(data, list):\n",
    "                        data_set.update(data)\n",
    "                    else:\n",
    "                        print(f\"❌ 오류: '{path}' 파일의 형식이 올바르지 않습니다. 리스트 형태여야 합니다.\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"❌ 오류: '{path}' 파일의 JSON 형식이 올바르지 않습니다.\")\n",
    "        return data_set\n",
    "\n",
    "    def _is_valid_context(self, sentence, location):\n",
    "        \"\"\"\n",
    "        문맥상 해당 지역이 북한과 관련있는지 판단\n",
    "        \"\"\"\n",
    "        sentence_lower = sentence.lower()\n",
    "        \n",
    "        for keyword in self.nk_keywords:\n",
    "            if keyword.lower() in sentence_lower:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def _extract_with_spacy_ner(self, doc):\n",
    "        \"\"\"\n",
    "        spaCy NER을 사용한 지역 추출\n",
    "        \"\"\"\n",
    "        locations = []\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in ['GPE', 'LOC']:\n",
    "                location = ent.text.strip()\n",
    "                sentence = ent.sent.text\n",
    "                if self._is_valid_context(sentence, location):\n",
    "                    locations.append({\n",
    "                        'location': location,\n",
    "                        'context': sentence,\n",
    "                        'confidence': 'high',\n",
    "                        'method': 'spacy_ner'\n",
    "                    })\n",
    "        return locations\n",
    "\n",
    "    def _extract_with_flashtext(self, doc):\n",
    "        \"\"\"\n",
    "        Flashtext를 사용한 사전 기반 추출\n",
    "        \"\"\"\n",
    "        found_locations = []\n",
    "        for sentence in doc.sents:\n",
    "            sentence_text = sentence.text\n",
    "            if len(sentence_text.strip()) < 10:\n",
    "                continue\n",
    "\n",
    "            # 지역명 검색\n",
    "            locations_in_sentence = self.keyword_processor_locations.extract_keywords(sentence_text)\n",
    "            for location in set(locations_in_sentence):\n",
    "                if location in self.exclude_words:\n",
    "                    if self._is_valid_context(sentence_text, location):\n",
    "                        found_locations.append({\n",
    "                            'location': location,\n",
    "                            'context': sentence_text,\n",
    "                            'confidence': 'medium',\n",
    "                            'type': 'nk_location',\n",
    "                            'method': 'flashtext'\n",
    "                        })\n",
    "                else:\n",
    "                    found_locations.append({\n",
    "                        'location': location,\n",
    "                        'context': sentence_text,\n",
    "                        'confidence': 'high',\n",
    "                        'type': 'nk_location',\n",
    "                        'method': 'flashtext'\n",
    "                    })\n",
    "\n",
    "            # 시설명 검색\n",
    "            facilities_in_sentence = self.keyword_processor_facilities.extract_keywords(sentence_text)\n",
    "            for facility in set(facilities_in_sentence):\n",
    "                found_locations.append({\n",
    "                    'location': facility,\n",
    "                    'context': sentence_text,\n",
    "                    'confidence': 'high',\n",
    "                    'type': 'nk_facility',\n",
    "                    'method': 'flashtext'\n",
    "                })\n",
    "        return found_locations\n",
    "\n",
    "    def _extract_combined_locations_with_regex(self, text):\n",
    "        \"\"\"\n",
    "        지역명과 군사 시설명 조합 패턴 추출 (정규식 사용)\n",
    "        \"\"\"\n",
    "        found_combined = []\n",
    "        \n",
    "        # Flashtext의 키워드 목록을 사용해 정규식 패턴 생성\n",
    "        nk_locations_pattern = '|'.join(re.escape(loc) for loc in self.nk_locations if len(loc) > 1)\n",
    "        nk_facilities_pattern = '|'.join(re.escape(fac) for fac in self.nk_military_facilities if len(fac) > 1)\n",
    "\n",
    "        if not nk_locations_pattern or not nk_facilities_pattern:\n",
    "            return []\n",
    "\n",
    "        combined_pattern = fr'({nk_locations_pattern})\\s*({nk_facilities_pattern})'\n",
    "        \n",
    "        sentences = re.split(r'[.!?]\\s+', text)\n",
    "        for sentence in sentences:\n",
    "            for match in re.finditer(combined_pattern, sentence):\n",
    "                location = match.group(1)\n",
    "                facility = match.group(2)\n",
    "                combined_name = f\"{location}{' ' if ' ' in sentence[match.start():match.end()] else ''}{facility}\"\n",
    "                \n",
    "                if self._is_valid_context(sentence, location):\n",
    "                    found_combined.append({\n",
    "                        'location': combined_name,\n",
    "                        'context': sentence,\n",
    "                        'confidence': 'high',\n",
    "                        'type': 'combined_facility',\n",
    "                        'method': 'regex'\n",
    "                    })\n",
    "        return found_combined\n",
    "\n",
    "    def extract_nk_locations_from_doc(self, doc):\n",
    "        \"\"\"\n",
    "        spaCy Doc 객체에서 북한 관련 지역/장소 정보 추출\n",
    "        \"\"\"\n",
    "        # 1. spaCy NER 사용\n",
    "        spacy_results = self._extract_with_spacy_ner(doc)\n",
    "        \n",
    "        # 2. Flashtext 기반 사전 추출\n",
    "        dict_results = self._extract_with_flashtext(doc)\n",
    "        \n",
    "        # 3. 조합된 단어 추출 (정규식)\n",
    "        combined_results = self._extract_combined_locations_with_regex(doc.text)\n",
    "        \n",
    "        all_results = spacy_results + dict_results + combined_results\n",
    "        unique_locations = {}\n",
    "        \n",
    "        for result in all_results:\n",
    "            location = result['location']\n",
    "            if location not in unique_locations:\n",
    "                unique_locations[location] = result\n",
    "            else:\n",
    "                if result.get('confidence') == 'high' and unique_locations[location].get('confidence') != 'high':\n",
    "                    unique_locations[location] = result\n",
    "        \n",
    "        return list(unique_locations.values())\n",
    "    \n",
    "    def _calculate_relevance_score(self, text, locations):\n",
    "        \"\"\"\n",
    "        관련성 점수 계산\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        keyword_count = sum(1 for keyword in self.nk_keywords if keyword in text)\n",
    "        score += min(keyword_count * 8, 40)\n",
    "        \n",
    "        high_conf_count = sum(1 for loc in locations if loc.get('confidence') == 'high')\n",
    "        score += min(high_conf_count * 10, 40)\n",
    "        \n",
    "        med_conf_count = sum(1 for loc in locations if loc.get('confidence') == 'medium')\n",
    "        score += min(med_conf_count * 5, 20)\n",
    "        \n",
    "        return min(score, 100)\n",
    "\n",
    "    def analyze_results(self, full_text, locations_found, title=\"\"):\n",
    "        \"\"\"\n",
    "        분석 결과를 요약\n",
    "        \"\"\"\n",
    "        relevance_score = self._calculate_relevance_score(full_text, locations_found)\n",
    "        is_nk_related = (len([kw for kw in self.nk_keywords if kw in full_text]) > 0 and relevance_score > 30) or len(locations_found) > 0\n",
    "        \n",
    "        return {\n",
    "            'title': title,\n",
    "            'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'relevance_score': relevance_score,\n",
    "            'is_nk_related': is_nk_related,\n",
    "            'locations_found': locations_found,\n",
    "        }\n",
    "\n",
    "def process_articles_in_batches(extractor, articles, output_filename=\"re_extracted_locations_ten_year_all.jsonl\", batch_size=1000):\n",
    "    \"\"\"\n",
    "    기사를 배치 단위로 처리하고 JSONL 형식으로 실시간 저장\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔄 총 {len(articles)}개 기사를 배치 단위로 분석합니다...\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    processed_count = 0\n",
    "    \n",
    "    with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
    "        # 텍스트 추출 및 배치 생성\n",
    "        texts = [f\"{article['metadata']['title']} {article['text']} {article.get('summary', '')}\" for article in articles]\n",
    "        \n",
    "        for i, doc in enumerate(extractor.nlp.pipe(texts, batch_size=batch_size), 1):\n",
    "            article_index = i - 1\n",
    "            article = articles[article_index]\n",
    "            \n",
    "            locations_found = extractor.extract_nk_locations_from_doc(doc)\n",
    "            \n",
    "            summary = extractor.analyze_results(doc.text, locations_found, article['metadata']['title'])\n",
    "            \n",
    "            if summary['locations_found'] and summary['is_nk_related']:\n",
    "                result_entry = {\n",
    "                    'id_': article['id_'],\n",
    "                    'locations': [loc['location'] for loc in summary['locations_found']],\n",
    "                    'confidence_info': [\n",
    "                        {\n",
    "                            'location': loc['location'],\n",
    "                            'confidence': loc['confidence'],\n",
    "                            'type': loc.get('type', 'unknown'),\n",
    "                            'method': loc.get('method', 'unknown')\n",
    "                        }\n",
    "                        for loc in summary['locations_found']\n",
    "                    ]\n",
    "                }\n",
    "                outfile.write(json.dumps(result_entry, ensure_ascii=False) + '\\n')\n",
    "            \n",
    "            processed_count += 1\n",
    "            if processed_count % batch_size == 0 or processed_count == len(articles):\n",
    "                elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "                print(f\"📊 {processed_count}/{len(articles)}개 기사 분석 완료. (경과 시간: {elapsed_time:.2f}초)\")\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    print(f\"\\n✅ 전체 분석 완료! 총 {len(articles)}개 기사 분석에 { (end_time - start_time).total_seconds():.2f}초 소요되었습니다.\")\n",
    "    print(f\"💾 결과가 {output_filename} 파일로 저장되었습니다.\")\n",
    "    \n",
    "def main():\n",
    "    \"\"\"\n",
    "    메인 실행 함수\n",
    "    \"\"\"\n",
    "    print(\"🏴‍☠️ 북한 관련 지역·장소 추출기 (최적화 버전)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        extractor = NorthKoreaLocationExtractor(\"ko_core_news_lg\")\n",
    "    except:\n",
    "        print(\"\\n모델 로드에 실패했습니다. 다른 모델로 시도합니다...\")\n",
    "        try:\n",
    "            extractor = NorthKoreaLocationExtractor(\"ko_core_news_md\")\n",
    "        except:\n",
    "            print(\"\\n다른 모델 로드에도 실패했습니다. spaCy 모델을 설치한 후 다시 실행해주세요.\")\n",
    "            return\n",
    "\n",
    "    input_filename = \"/home/ds4_sia_nolb/#FINAL_POLARIS/04_plus_preprocessing/preprocessing_final_data/re_final_preprocessing.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(input_filename, 'r', encoding='utf-8') as f:\n",
    "            articles = json.load(f)\n",
    "        print(f\"📁 {input_filename} 파일에서 {len(articles)}개 기사를 읽었습니다.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ 오류: {input_filename} 파일을 찾을 수 없습니다.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"❌ 오류: {input_filename} 파일의 JSON 형식이 올바르지 않습니다.\")\n",
    "        return\n",
    "\n",
    "    process_articles_in_batches(extractor, articles)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
