{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3ee6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import requests\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "class NorthKoreaLocationExtractor:\n",
    "    def __init__(self, model_name=\"ko_core_news_lg\"):\n",
    "        \"\"\"\n",
    "        ë¶í•œ ê´€ë ¨ ì§€ì—­, ì¥ì†Œ, ê±´ë¬¼ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.nlp = spacy.load(model_name)\n",
    "            print(f\"âœ… {model_name} ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        except OSError:\n",
    "            print(f\"âŒ {model_name} ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            print(\"ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:\")\n",
    "            if model_name.startswith(\"ko\"):\n",
    "                print(\"pip install spacy\")\n",
    "                print(\"python -m spacy download ko_core_news_lg\")\n",
    "            else:\n",
    "                print(\"pip install spacy\")\n",
    "                print(\"python -m spacy download en_core_web_sm\")\n",
    "            raise\n",
    "            \n",
    "        # JSON íŒŒì¼ ê²½ë¡œ ì •ì˜\n",
    "        self.location_file = '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_region.json'\n",
    "        \n",
    "        # ì¡°í•©ì— ì‚¬ìš©í•  êµ°ì‚¬ ì‹œì„¤ ì‚¬ì „ ê²½ë¡œ\n",
    "        self.military_facility_file = '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_military.json'\n",
    "        \n",
    "        # ë‹¤ë¥¸ ì‹œì„¤ ì‚¬ì „ ê²½ë¡œ\n",
    "        self.other_facility_files = [\n",
    "            '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_military.json',\n",
    "            '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_edu_cul.json',\n",
    "            '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_landmark.json',\n",
    "            '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_politics.json',\n",
    "            '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_edu_cul.json'\n",
    "        ]\n",
    "        \n",
    "        # JSON íŒŒì¼ì—ì„œ ì‚¬ì „ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "        self.nk_locations = self._load_json_to_set([self.location_file], \"ë¶í•œ ì§€ì—­ëª…\")\n",
    "        \n",
    "        # êµ°ì‚¬ ì‹œì„¤ ì‚¬ì „ë§Œ ë³„ë„ë¡œ ë¡œë“œ\n",
    "        self.nk_military_facilities = self._load_json_to_set([self.military_facility_file], \"ë¶í•œ êµ°ì‚¬ ì‹œì„¤\")\n",
    "        \n",
    "        # ì „ì²´ ì‹œì„¤ ì‚¬ì „ì„ í†µí•©í•˜ì—¬ ê°œë³„ ë‹¨ì–´ ì¶”ì¶œì— ì‚¬ìš©\n",
    "        all_facility_files = [self.military_facility_file] + self.other_facility_files\n",
    "        self.nk_facilities = self._load_json_to_set(all_facility_files, \"ë¶í•œ ì „ì²´ ì‹œì„¤/ê±´ë¬¼ëª…\")\n",
    "\n",
    "        # ë¶í•œ ê´€ë ¨ í‚¤ì›Œë“œ (ë§¥ë½ íŒë‹¨ìš©)\n",
    "        self.nk_keywords = {\n",
    "            'ë¶í•œ', 'ì¡°ì„ ë¯¼ì£¼ì£¼ì˜ì¸ë¯¼ê³µí™”êµ­', 'ì¡°ì„ ', 'DPRK',\n",
    "            'ê¹€ì •ì€', 'ê¹€ì •ì¼', 'ê¹€ì¼ì„±', 'ê¹€ì—¬ì •',\n",
    "            'ì¡°ì„ ë¡œë™ë‹¹', 'ë…¸ë™ë‹¹', 'ìµœê³ ì§€ë„ì', 'ì›ìˆ˜ë‹˜', 'ìœ„ì›ì¥',\n",
    "            'í•µì‹¤í—˜', 'ë¯¸ì‚¬ì¼', 'ë¡œì¼“', 'ì¸ê³µìœ„ì„±', 'íƒ„ë„ë¯¸ì‚¬ì¼',\n",
    "            'ëŒ€ë‚¨', 'ë‚¨ì¡°ì„ ', 'í†µì¼', '6ìíšŒë‹´'\n",
    "        }\n",
    "        \n",
    "        # ì œì™¸í•  ì¼ë°˜ ë‹¨ì–´ë“¤ (ì˜¤íƒ ë°©ì§€)\n",
    "        self.exclude_words = {\n",
    "            'í•´ì£¼', 'ìˆœì²œ', 'ê°œì²œ', 'ì„±ì²œ', 'ì‹ ì²œ', 'ì•ˆì£¼', \n",
    "            'ê°•ê³„', 'íšŒì°½', 'ì˜¨ì²œ', 'ì˜ì›', 'ì‹ ì›', 'ê³ ì›', \n",
    "            'ëŒ€í¥', 'ì‹ ì–‘', 'ë´‰ì²œ', 'ì†¡í™”', 'ê³¼ì¼', 'ì‹ í¥',\n",
    "            'ë•ì„±', 'ì˜ê´‘', 'ê³ ì„±', 'ì² ì›', 'í‰ê°•', 'ê¹€í™”'\n",
    "        }\n",
    "        \n",
    "        # ë§¥ë½ íŒ¨í„´ (ì •ê·œì‹)\n",
    "        self.nk_context_patterns = [\n",
    "            r'ë¶í•œ.*?([ê°€-í£]+(?:ì‹œ|êµ°|êµ¬|ë™|ë¦¬))', \n",
    "            r'ì¡°ì„ .*?([ê°€-í£]+(?:ì‹œ|êµ°|êµ¬|ë™|ë¦¬))', \n",
    "            r'DPRK.*?([A-Za-zê°€-í£]+)',\n",
    "            r'ê¹€ì •ì€.*?([ê°€-í£]+(?:ì‹œ|êµ°|êµ¬|ë¦¬))', \n",
    "            r'í‰ì–‘.*?([ê°€-í£]+(?:êµ¬|ë™|ë¦¬))', \n",
    "        ]\n",
    "\n",
    "    def _load_json_to_set(self, file_paths, dict_name):\n",
    "        \"\"\"\n",
    "        JSON íŒŒì¼(ë“¤)ì„ ì½ì–´ ì„¸íŠ¸(set)ë¡œ ë³€í™˜\n",
    "        \"\"\"\n",
    "        data_set = set()\n",
    "        for path in file_paths:\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"âš ï¸ ê²½ê³ : {dict_name} ì‚¬ì „ íŒŒì¼ '{path}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•´ë‹¹ ì‚¬ì „ì€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n",
    "                continue\n",
    "            try:\n",
    "                with open(path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    if isinstance(data, list):\n",
    "                        data_set.update(data)\n",
    "                    else:\n",
    "                        print(f\"âŒ ì˜¤ë¥˜: '{path}' íŒŒì¼ì˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¦¬ìŠ¤íŠ¸ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"âŒ ì˜¤ë¥˜: '{path}' íŒŒì¼ì˜ JSON í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "        return data_set\n",
    "\n",
    "    def _extract_combined_locations_with_regex(self, text):\n",
    "        found_combined = []\n",
    "\n",
    "        # ì§€ì—­ëª…ê³¼ êµ°ì‚¬ ì‹œì„¤ëª… íŒ¨í„´ë§Œ ì‚¬ìš©í•˜ì—¬ ì¡°í•©\n",
    "        nk_locations_pattern = '|'.join(re.escape(loc) for loc in self.nk_locations)\n",
    "        nk_facilities_pattern = '|'.join(re.escape(fac) for fac in self.nk_military_facilities)\n",
    "\n",
    "        # 'ì§€ì—­ëª…'ê³¼ 'ì‹œì„¤ëª…'ì´ ì¸ì ‘í•œ íŒ¨í„´ì„ ê²€ìƒ‰ (ë„ì–´ì“°ê¸° í—ˆìš©)\n",
    "        combined_pattern = fr'({nk_locations_pattern})\\s*({nk_facilities_pattern})'\n",
    "        \n",
    "        sentences = re.split(r'[.!?]\\s+', text)\n",
    "        for sentence in sentences:\n",
    "            if len(sentence.strip()) < 10:\n",
    "                continue\n",
    "            \n",
    "            for match in re.finditer(combined_pattern, sentence):\n",
    "                location = match.group(1)\n",
    "                facility = match.group(2)\n",
    "                combined_name = f\"{location}{' ' if ' ' in sentence[match.start():match.end()] else ''}{facility}\"\n",
    "                \n",
    "                # ì¤‘ë³µ ë° ë§¥ë½ ê²€ì¦\n",
    "                if self._is_valid_context(sentence, location):\n",
    "                    found_combined.append({\n",
    "                        'location': combined_name,\n",
    "                        'context': sentence,\n",
    "                        'confidence': 'high',\n",
    "                        'type': 'combined_facility'\n",
    "                    })\n",
    "        \n",
    "        return found_combined\n",
    "\n",
    "    def _is_valid_context(self, sentence, location):\n",
    "        \"\"\"\n",
    "        ë¬¸ë§¥ìƒ í•´ë‹¹ ì§€ì—­ì´ ë¶í•œê³¼ ê´€ë ¨ìˆëŠ”ì§€ íŒë‹¨\n",
    "        \"\"\"\n",
    "        sentence_lower = sentence.lower()\n",
    "        location_lower = location.lower()\n",
    "        \n",
    "        for keyword in self.nk_keywords:\n",
    "            if keyword.lower() in sentence_lower:\n",
    "                return True\n",
    "        \n",
    "        clear_nk_locations = {'í‰ì–‘', 'ê¹€ì •ì€', 'ì¡°ì„ ë¡œë™ë‹¹', 'ë…¸ë™ë‹¹'}\n",
    "        for nk_loc in clear_nk_locations:\n",
    "            if nk_loc in sentence and location in sentence:\n",
    "                return True\n",
    "        \n",
    "        negative_keywords = ['í•œêµ­', 'ë‚¨í•œ', 'ìš°ë¦¬ë‚˜ë¼', 'êµ­ë‚´', 'ì„œìš¸', 'ë¶€ì‚°']\n",
    "        for neg_keyword in negative_keywords:\n",
    "            if neg_keyword in sentence and location in sentence:\n",
    "                return False\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def _extract_with_spacy_ner(self, text):\n",
    "        \"\"\"\n",
    "        spaCy NERì„ ì‚¬ìš©í•œ ì§€ì—­ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        locations = []\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in ['GPE', 'LOC']:\n",
    "                location = ent.text.strip()\n",
    "                sentence = ent.sent.text\n",
    "                if self._is_valid_context(sentence, location):\n",
    "                    locations.append({\n",
    "                        'location': location,\n",
    "                        'context': sentence,\n",
    "                        'confidence': 'high'\n",
    "                    })\n",
    "        \n",
    "        return locations\n",
    "\n",
    "    def _extract_with_dictionary(self, text):\n",
    "        \"\"\"\n",
    "        ì‚¬ì „ ê¸°ë°˜ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)\n",
    "        \"\"\"\n",
    "        sentences = re.split(r'[.!?]\\s+', text)\n",
    "        found_locations = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if len(sentence.strip()) < 10:\n",
    "                continue\n",
    "                \n",
    "            # ë¶í•œ ì§€ì—­ëª… ê²€ìƒ‰\n",
    "            for location in self.nk_locations:\n",
    "                if location in sentence:\n",
    "                    if location in self.exclude_words:\n",
    "                        if self._is_valid_context(sentence, location):\n",
    "                            found_locations.append({\n",
    "                                'location': location,\n",
    "                                'context': sentence,\n",
    "                                'confidence': 'medium',\n",
    "                                'type': 'nk_location'\n",
    "                            })\n",
    "                    else:\n",
    "                        found_locations.append({\n",
    "                            'location': location,\n",
    "                            'context': sentence,\n",
    "                            'confidence': 'high',\n",
    "                            'type': 'nk_location'\n",
    "                        })\n",
    "            \n",
    "            # ë¶í•œ ì‹œì„¤ëª… ê²€ìƒ‰\n",
    "            for facility in self.nk_facilities:\n",
    "                if facility in sentence:\n",
    "                    found_locations.append({\n",
    "                        'location': facility,\n",
    "                        'context': sentence,\n",
    "                        'confidence': 'high',\n",
    "                        'type': 'nk_facility'\n",
    "                    })\n",
    "        \n",
    "        return found_locations\n",
    "\n",
    "    def _filter_by_confidence(self, locations, min_confidence='medium'):\n",
    "        \"\"\"\n",
    "        ì‹ ë¢°ë„ì— ë”°ë¥¸ í•„í„°ë§\n",
    "        \"\"\"\n",
    "        confidence_levels = {'low': 1, 'medium': 2, 'high': 3}\n",
    "        min_level = confidence_levels.get(min_confidence, 2)\n",
    "        \n",
    "        filtered = []\n",
    "        for loc in locations:\n",
    "            loc_level = confidence_levels.get(loc.get('confidence', 'low'), 1)\n",
    "            if loc_level >= min_level:\n",
    "                filtered.append(loc)\n",
    "        \n",
    "        return filtered\n",
    "\n",
    "    def extract_nk_locations(self, text):\n",
    "        \"\"\"\n",
    "        í…ìŠ¤íŠ¸ì—ì„œ ë¶í•œ ê´€ë ¨ ì§€ì—­/ì¥ì†Œ ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)\n",
    "        \"\"\"\n",
    "        # 1. spaCy NER ì‚¬ìš©\n",
    "        spacy_results = self._extract_with_spacy_ner(text)\n",
    "        \n",
    "        # 2. ì‚¬ì „ ê¸°ë°˜ ì¶”ì¶œ (ê°œë³„ ë‹¨ì–´)\n",
    "        dict_results = self._extract_with_dictionary(text)\n",
    "        \n",
    "        # 3. ì¡°í•©ëœ ë‹¨ì–´ ì¶”ì¶œ (ì§€ì—­ + êµ°ì‚¬ì‹œì„¤)\n",
    "        combined_results = self._extract_combined_locations_with_regex(text)\n",
    "        \n",
    "        # 4. ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±°\n",
    "        all_results = spacy_results + dict_results + combined_results\n",
    "        unique_locations = {}\n",
    "        \n",
    "        for result in all_results:\n",
    "            location = result['location']\n",
    "            if location not in unique_locations:\n",
    "                unique_locations[location] = result\n",
    "            else:\n",
    "                if result.get('confidence') == 'high' and unique_locations[location].get('confidence') != 'high':\n",
    "                    unique_locations[location] = result\n",
    "        \n",
    "        # 5. ì‹ ë¢°ë„ í•„í„°ë§\n",
    "        filtered_results = self._filter_by_confidence(\n",
    "            list(unique_locations.values()), \n",
    "            min_confidence='medium'\n",
    "        )\n",
    "        \n",
    "        # 6. ë¶í•œ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸\n",
    "        nk_context_keywords = [keyword for keyword in self.nk_keywords if keyword in text]\n",
    "        \n",
    "        # 7. ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°\n",
    "        relevance_score = self._calculate_relevance_score(text, filtered_results)\n",
    "        \n",
    "        return {\n",
    "            'locations': filtered_results,\n",
    "            'nk_context_keywords': nk_context_keywords,\n",
    "            'has_nk_context': len(nk_context_keywords) > 0,\n",
    "            'relevance_score': relevance_score,\n",
    "            'total_found': len(filtered_results)\n",
    "        }\n",
    "\n",
    "    def _calculate_relevance_score(self, text, locations):\n",
    "        \"\"\"\n",
    "        ê°œì„ ëœ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        keyword_count = sum(1 for keyword in self.nk_keywords if keyword in text)\n",
    "        score += min(keyword_count * 8, 40)\n",
    "        \n",
    "        high_conf_count = sum(1 for loc in locations if loc.get('confidence') == 'high')\n",
    "        score += min(high_conf_count * 10, 40)\n",
    "        \n",
    "        med_conf_count = sum(1 for loc in locations if loc.get('confidence') == 'medium')\n",
    "        score += min(med_conf_count * 5, 20)\n",
    "        \n",
    "        return min(score, 100)\n",
    "\n",
    "    def analyze_news_article(self, text, title=\"\"):\n",
    "        \"\"\"\n",
    "        ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„ (ê°œì„ ëœ ë²„ì „)\n",
    "        \"\"\"\n",
    "        full_text = f\"{title} {text}\" if title else text\n",
    "        results = self.extract_nk_locations(full_text)\n",
    "        \n",
    "        summary = {\n",
    "            'title': title,\n",
    "            'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'text_length': len(full_text),\n",
    "            'relevance_score': results['relevance_score'],\n",
    "            'is_nk_related': results['has_nk_context'] and results['relevance_score'] > 30,\n",
    "            'locations_found': results['locations'],\n",
    "            'confidence_distribution': self._get_confidence_distribution(results['locations']),\n",
    "            'full_results': results\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "    def _get_confidence_distribution(self, locations):\n",
    "        \"\"\"\n",
    "        ì‹ ë¢°ë„ë³„ ë¶„í¬ ê³„ì‚°\n",
    "        \"\"\"\n",
    "        distribution = {'high': 0, 'medium': 0, 'low': 0}\n",
    "        for loc in locations:\n",
    "            confidence = loc.get('confidence', 'low')\n",
    "            distribution[confidence] += 1\n",
    "        return distribution\n",
    "\n",
    "    def print_analysis_results(self, results):\n",
    "        \"\"\"\n",
    "        ê°œì„ ëœ ë¶„ì„ ê²°ê³¼ ì¶œë ¥\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ´â€â˜ ï¸ ë¶í•œ ê´€ë ¨ ì§€ì—­Â·ì¥ì†Œ ë¶„ì„ ê²°ê³¼ \")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"ğŸ“Š ë¶í•œ ê´€ë ¨ì„± ì ìˆ˜: {results['relevance_score']}/100\")\n",
    "        print(f\"ğŸ¯ ë¶í•œ ê´€ë ¨ ê¸°ì‚¬ ì—¬ë¶€: {'âœ… ì˜ˆ' if results['is_nk_related'] else 'âŒ ì•„ë‹ˆì˜¤'}\")\n",
    "        \n",
    "        if results['locations_found']:\n",
    "            print(f\"\\nğŸ“ ë°œê²¬ëœ ìœ„ì¹˜ ì •ë³´ ({len(results['locations_found'])}ê°œ):\")\n",
    "            \n",
    "            high_conf = [loc for loc in results['locations_found'] if loc.get('confidence') == 'high']\n",
    "            med_conf = [loc for loc in results['locations_found'] if loc.get('confidence') == 'medium']\n",
    "            \n",
    "            if high_conf:\n",
    "                print(f\"\\nğŸŸ¢ ë†’ì€ ì‹ ë¢°ë„ ({len(high_conf)}ê°œ):\")\n",
    "                for loc in high_conf:\n",
    "                    location_type = loc.get('type', 'unknown')\n",
    "                    type_icon = 'ğŸ›ï¸' if location_type == 'nk_facility' else 'ğŸŒ'\n",
    "                    print(f\"   {type_icon} {loc['location']} (íƒ€ì…: {location_type})\")\n",
    "                    print(f\"     ë§¥ë½: {loc['context'][:80]}...\")\n",
    "            \n",
    "            if med_conf:\n",
    "                print(f\"\\nğŸŸ¡ ì¤‘ê°„ ì‹ ë¢°ë„ ({len(med_conf)}ê°œ):\")\n",
    "                for loc in med_conf:\n",
    "                    location_type = loc.get('type', 'unknown')\n",
    "                    type_icon = 'ğŸ›ï¸' if location_type == 'nk_facility' else 'ğŸŒ'\n",
    "                    print(f\"   {type_icon} {loc['location']} (íƒ€ì…: {location_type})\")\n",
    "                    print(f\"     ë§¥ë½: {loc['context'][:80]}...\")\n",
    "        else:\n",
    "            print(f\"\\nâŒ ë¶í•œ ê´€ë ¨ ì§€ì—­/ì¥ì†Œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        if results['full_results']['nk_context_keywords']:\n",
    "            print(f\"\\nğŸ”‘ ë¶í•œ ê´€ë ¨ í‚¤ì›Œë“œ:\")\n",
    "            for keyword in results['full_results']['nk_context_keywords'][:10]:\n",
    "                print(f\"   â€¢ {keyword}\")\n",
    "\n",
    "def analyze_sample_file(extractor, filename=\"/home/ds4_sia_nolb/#FINAL_POLARIS/04_plus_preprocessing/preprocessing_final_data/re_final_preprocessing.json\"):\n",
    "    \"\"\"\n",
    "    ìƒ˜í”Œ JSON íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì§€ì—­ ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            articles = json.load(f)\n",
    "        \n",
    "        print(f\"ğŸ“ {filename} íŒŒì¼ì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ë¥¼ ì½ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        extracted_results = []\n",
    "        \n",
    "        for i, article in enumerate(articles, 1):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"ğŸ“° ê¸°ì‚¬ {i}/{len(articles)}: {article['metadata']['title']}\")\n",
    "            print(f\"ğŸ†” ID: {article['id_']}\")\n",
    "            print(f\"ğŸ“… ë‚ ì§œ: {article['metadata']['pubDate']}\")\n",
    "            print(f\"ğŸ·ï¸ ì¹´í…Œê³ ë¦¬: {article['metadata']['category']}\")\n",
    "            \n",
    "            full_text = f\"{article['metadata']['title']} {article['text']} {article.get('summary', '')}\"\n",
    "            \n",
    "            results = extractor.analyze_news_article(full_text, article['metadata']['title'])\n",
    "            \n",
    "            extractor.print_analysis_results(results)\n",
    "            \n",
    "            if results['locations_found'] and results['is_nk_related']:\n",
    "                location_names = [loc['location'] for loc in results['locations_found']]\n",
    "                extracted_results.append({\n",
    "                    'id_': article['id_'],\n",
    "                    'locations': location_names,\n",
    "                    'confidence_info': [\n",
    "                        {\n",
    "                            'location': loc['location'],\n",
    "                            'confidence': loc['confidence'],\n",
    "                            'type': loc.get('type', 'unknown')\n",
    "                        }\n",
    "                        for loc in results['locations_found']\n",
    "                    ]\n",
    "                })\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"ğŸ¯ ì „ì²´ ë¶„ì„ ìš”ì•½\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"ğŸ“Š ë¶„ì„ëœ ê¸°ì‚¬ ìˆ˜: {len(articles)}ê°œ\")\n",
    "        print(f\"ğŸ“ ì§€ì—­ì •ë³´ ë°œê²¬ ê¸°ì‚¬ ìˆ˜: {len(extracted_results)}ê°œ\")\n",
    "        \n",
    "        if extracted_results:\n",
    "            print(f\"\\nğŸ“‹ ì¶”ì¶œëœ ê²°ê³¼ (JSON í˜•íƒœ):\")\n",
    "            print(json.dumps(extracted_results, ensure_ascii=False, indent=2))\n",
    "            \n",
    "            output_filename = \"test_extracted_locations_improved3_2017_01_03.json\"\n",
    "            with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(extracted_results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"\\nğŸ’¾ ê²°ê³¼ê°€ {output_filename} íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            print(\"\\nâŒ ë¶í•œ ê´€ë ¨ ì§€ì—­ ì •ë³´ê°€ ë°œê²¬ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            \n",
    "        return extracted_results\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: {filename} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"íŒŒì¼ì´ Python ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ ë””ë ‰í„°ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: {filename} íŒŒì¼ì˜ JSON í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: {e}\")\n",
    "        return []\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    print(\"ğŸ´â€â˜ ï¸ ë¶í•œ ê´€ë ¨ ì§€ì—­Â·ì¥ì†Œ ì¶”ì¶œê¸° (ê°œì„  ë²„ì „)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        extractor = NorthKoreaLocationExtractor(\"ko_core_news_lg\")\n",
    "    except:\n",
    "        print(\"\\nì¤‘í˜• ëª¨ë¸ë¡œ ì‹œë„í•´ë³´ê² ìŠµë‹ˆë‹¤...\")\n",
    "        try:\n",
    "            extractor = NorthKoreaLocationExtractor(\"ko_core_news_md\")\n",
    "        except:\n",
    "            print(\"\\nì†Œí˜• ëª¨ë¸ë¡œ ì‹œë„í•´ë³´ê² ìŠµë‹ˆë‹¤...\")\n",
    "            try:\n",
    "                extractor = NorthKoreaLocationExtractor(\"ko_core_news_sm\")\n",
    "            except:\n",
    "                print(\"\\nì˜ì–´ ëª¨ë¸ë¡œ ì‹œë„í•´ë³´ê² ìŠµë‹ˆë‹¤...\")\n",
    "                try:\n",
    "                    extractor = NorthKoreaLocationExtractor(\"en_core_web_sm\")\n",
    "                except:\n",
    "                    print(\"spaCy ëª¨ë¸ì„ ì„¤ì¹˜í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "                    return\n",
    "    \n",
    "    print(\"\\nğŸ”„ ìƒ˜í”Œ íŒŒì¼ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    extracted_results = analyze_sample_file(extractor)\n",
    "    \n",
    "    if extracted_results:\n",
    "        print(f\"\\nâœ… ë¶„ì„ ì™„ë£Œ! {len(extracted_results)}ê°œ ê¸°ì‚¬ì—ì„œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì§€ì—­ ì •ë³´ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  ë¶„ì„ì€ ì™„ë£Œë˜ì—ˆì§€ë§Œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë¶í•œ ê´€ë ¨ ì§€ì—­ ì •ë³´ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56a25069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ´â€â˜ ï¸ ë¶í•œ ê´€ë ¨ ì§€ì—­Â·ì¥ì†Œ ì¶”ì¶œê¸° (ìµœì í™” ë²„ì „)\n",
      "============================================================\n",
      "âœ… GPU ì‚¬ìš©ì„ ì‹œë„í•©ë‹ˆë‹¤.\n",
      "ğŸ“ /home/ds4_sia_nolb/#FINAL_POLARIS/04_plus_preprocessing/preprocessing_final_data/re_final_preprocessing.json íŒŒì¼ì—ì„œ 80434ê°œ ê¸°ì‚¬ë¥¼ ì½ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ”„ ì´ 80434ê°œ ê¸°ì‚¬ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¶„ì„í•©ë‹ˆë‹¤...\n",
      "ğŸ“Š 1000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 40.00ì´ˆ)\n",
      "ğŸ“Š 2000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 62.72ì´ˆ)\n",
      "ğŸ“Š 3000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 86.30ì´ˆ)\n",
      "ğŸ“Š 4000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 109.16ì´ˆ)\n",
      "ğŸ“Š 5000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 133.76ì´ˆ)\n",
      "ğŸ“Š 6000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 159.09ì´ˆ)\n",
      "ğŸ“Š 7000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 183.98ì´ˆ)\n",
      "ğŸ“Š 8000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 207.82ì´ˆ)\n",
      "ğŸ“Š 9000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 231.85ì´ˆ)\n",
      "ğŸ“Š 10000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 257.49ì´ˆ)\n",
      "ğŸ“Š 11000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 281.12ì´ˆ)\n",
      "ğŸ“Š 12000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 305.94ì´ˆ)\n",
      "ğŸ“Š 13000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 330.40ì´ˆ)\n",
      "ğŸ“Š 14000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 355.00ì´ˆ)\n",
      "ğŸ“Š 15000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 379.63ì´ˆ)\n",
      "ğŸ“Š 16000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 404.74ì´ˆ)\n",
      "ğŸ“Š 17000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 431.01ì´ˆ)\n",
      "ğŸ“Š 18000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 456.66ì´ˆ)\n",
      "ğŸ“Š 19000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 481.58ì´ˆ)\n",
      "ğŸ“Š 20000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 504.92ì´ˆ)\n",
      "ğŸ“Š 21000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 527.39ì´ˆ)\n",
      "ğŸ“Š 22000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 549.68ì´ˆ)\n",
      "ğŸ“Š 23000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 572.43ì´ˆ)\n",
      "ğŸ“Š 24000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 593.46ì´ˆ)\n",
      "ğŸ“Š 25000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 617.13ì´ˆ)\n",
      "ğŸ“Š 26000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 639.37ì´ˆ)\n",
      "ğŸ“Š 27000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 661.83ì´ˆ)\n",
      "ğŸ“Š 28000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 685.03ì´ˆ)\n",
      "ğŸ“Š 29000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 709.25ì´ˆ)\n",
      "ğŸ“Š 30000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 734.12ì´ˆ)\n",
      "ğŸ“Š 31000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 760.01ì´ˆ)\n",
      "ğŸ“Š 32000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 784.95ì´ˆ)\n",
      "ğŸ“Š 33000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 809.25ì´ˆ)\n",
      "ğŸ“Š 34000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 833.71ì´ˆ)\n",
      "ğŸ“Š 35000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 858.61ì´ˆ)\n",
      "ğŸ“Š 36000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 882.96ì´ˆ)\n",
      "ğŸ“Š 37000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 907.30ì´ˆ)\n",
      "ğŸ“Š 38000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 931.61ì´ˆ)\n",
      "ğŸ“Š 39000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 957.12ì´ˆ)\n",
      "ğŸ“Š 40000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 982.80ì´ˆ)\n",
      "ğŸ“Š 41000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1008.69ì´ˆ)\n",
      "ğŸ“Š 42000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1033.71ì´ˆ)\n",
      "ğŸ“Š 43000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1057.64ì´ˆ)\n",
      "ğŸ“Š 44000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1082.53ì´ˆ)\n",
      "ğŸ“Š 45000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1105.89ì´ˆ)\n",
      "ğŸ“Š 46000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1128.59ì´ˆ)\n",
      "ğŸ“Š 47000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1150.53ì´ˆ)\n",
      "ğŸ“Š 48000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1172.66ì´ˆ)\n",
      "ğŸ“Š 49000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1195.99ì´ˆ)\n",
      "ğŸ“Š 50000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1220.16ì´ˆ)\n",
      "ğŸ“Š 51000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1243.29ì´ˆ)\n",
      "ğŸ“Š 52000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1265.38ì´ˆ)\n",
      "ğŸ“Š 53000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1288.53ì´ˆ)\n",
      "ğŸ“Š 54000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1313.04ì´ˆ)\n",
      "ğŸ“Š 55000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1336.04ì´ˆ)\n",
      "ğŸ“Š 56000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1358.31ì´ˆ)\n",
      "ğŸ“Š 57000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1381.07ì´ˆ)\n",
      "ğŸ“Š 58000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1404.54ì´ˆ)\n",
      "ğŸ“Š 59000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1427.73ì´ˆ)\n",
      "ğŸ“Š 60000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1450.32ì´ˆ)\n",
      "ğŸ“Š 61000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1473.01ì´ˆ)\n",
      "ğŸ“Š 62000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1494.76ì´ˆ)\n",
      "ğŸ“Š 63000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1516.52ì´ˆ)\n",
      "ğŸ“Š 64000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1537.99ì´ˆ)\n",
      "ğŸ“Š 65000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1559.54ì´ˆ)\n",
      "ğŸ“Š 66000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1581.97ì´ˆ)\n",
      "ğŸ“Š 67000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1606.84ì´ˆ)\n",
      "ğŸ“Š 68000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1630.68ì´ˆ)\n",
      "ğŸ“Š 69000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1654.59ì´ˆ)\n",
      "ğŸ“Š 70000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1679.89ì´ˆ)\n",
      "ğŸ“Š 71000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1704.42ì´ˆ)\n",
      "ğŸ“Š 72000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1727.53ì´ˆ)\n",
      "ğŸ“Š 73000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1751.59ì´ˆ)\n",
      "ğŸ“Š 74000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1775.36ì´ˆ)\n",
      "ğŸ“Š 75000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1799.69ì´ˆ)\n",
      "ğŸ“Š 76000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1823.58ì´ˆ)\n",
      "ğŸ“Š 77000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1847.23ì´ˆ)\n",
      "ğŸ“Š 78000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1870.71ì´ˆ)\n",
      "ğŸ“Š 79000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1894.74ì´ˆ)\n",
      "ğŸ“Š 80000/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1917.74ì´ˆ)\n",
      "ğŸ“Š 80434/80434ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: 1927.95ì´ˆ)\n",
      "\n",
      "âœ… ì „ì²´ ë¶„ì„ ì™„ë£Œ! ì´ 80434ê°œ ê¸°ì‚¬ ë¶„ì„ì— 1927.95ì´ˆ ì†Œìš”ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ğŸ’¾ ê²°ê³¼ê°€ re_extracted_locations_ten_year_all.jsonl íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.pipeline import EntityRuler\n",
    "import requests\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "class NorthKoreaLocationExtractor:\n",
    "    def __init__(self, model_name=\"ko_core_news_lg\"):\n",
    "        \"\"\"\n",
    "        ë¶í•œ ê´€ë ¨ ì§€ì—­, ì¥ì†Œ, ê±´ë¬¼ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤\n",
    "        \"\"\"\n",
    "        try:\n",
    "            spacy.prefer_gpu()\n",
    "            print(\"âœ… GPU ì‚¬ìš©ì„ ì‹œë„í•©ë‹ˆë‹¤.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ GPU í™œì„±í™” ì‹¤íŒ¨: {e}\")\n",
    "            print(\"CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "        try:\n",
    "            self.nlp = spacy.load(model_name)\n",
    "            if self.nlp.meta['name'] == model_name:\n",
    "                if spacy.prefer_gpu():\n",
    "                    print(f\"âœ… {model_name} ëª¨ë¸ì´ GPUì—ì„œ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "                else:\n",
    "                    print(f\"âœ… {model_name} ëª¨ë¸ì´ CPUì—ì„œ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        except OSError:\n",
    "            print(f\"âŒ {model_name} ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            print(\"ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:\")\n",
    "            if model_name.startswith(\"ko\"):\n",
    "                print(\"pip install spacy\")\n",
    "                print(\"python -m spacy download ko_core_news_lg\")\n",
    "            else:\n",
    "                print(\"pip install spacy\")\n",
    "                print(\"python -m spacy download en_core_web_sm\")\n",
    "            raise\n",
    "\n",
    "        # JSON íŒŒì¼ ê²½ë¡œ ì •ì˜\n",
    "        self.location_file = '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_region.json'\n",
    "        \n",
    "        # ì¡°í•©ì— ì‚¬ìš©í•  êµ°ì‚¬ ì‹œì„¤ ì‚¬ì „ ê²½ë¡œ\n",
    "        self.military_facility_file = '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_military.json'\n",
    "        \n",
    "        # ë‹¤ë¥¸ ì‹œì„¤ ì‚¬ì „ ê²½ë¡œ\n",
    "        self.other_facility_files = [\n",
    "            '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_military.json',\n",
    "            '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_edu_cul.json',\n",
    "            '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_landmark.json',\n",
    "            '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_politics.json',\n",
    "            '/home/ds4_sia_nolb/#FINAL_POLARIS/06_Geo_coding/Dictiionary_data/dprk_edu_cul.json'\n",
    "        ]\n",
    "\n",
    "        # JSON íŒŒì¼ì—ì„œ ì‚¬ì „ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "        self.nk_locations = self._load_json_to_set([self.location_file], \"ë¶í•œ ì§€ì—­ëª…\")\n",
    "        self.nk_military_facilities = self._load_json_to_set([self.military_facility_file], \"ë¶í•œ êµ°ì‚¬ ì‹œì„¤\")\n",
    "        all_facility_files = [self.military_facility_file] + self.other_facility_files\n",
    "        self.nk_facilities = self._load_json_to_set(all_facility_files, \"ë¶í•œ ì „ì²´ ì‹œì„¤/ê±´ë¬¼ëª…\")\n",
    "\n",
    "        # Flashtext í‚¤ì›Œë“œ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”\n",
    "        self.keyword_processor_locations = KeywordProcessor()\n",
    "        self.keyword_processor_locations.add_keywords_from_list(list(self.nk_locations))\n",
    "        self.keyword_processor_facilities = KeywordProcessor()\n",
    "        self.keyword_processor_facilities.add_keywords_from_list(list(self.nk_facilities))\n",
    "\n",
    "        # ë¶í•œ ê´€ë ¨ í‚¤ì›Œë“œ (ë§¥ë½ íŒë‹¨ìš©)\n",
    "        self.nk_keywords = {\n",
    "            'ë¶í•œ', 'ì¡°ì„ ë¯¼ì£¼ì£¼ì˜ì¸ë¯¼ê³µí™”êµ­', 'ì¡°ì„ ', 'DPRK',\n",
    "            'ê¹€ì •ì€', 'ê¹€ì •ì¼', 'ê¹€ì¼ì„±', 'ê¹€ì—¬ì •',\n",
    "            'ì¡°ì„ ë¡œë™ë‹¹', 'ë…¸ë™ë‹¹', 'ìµœê³ ì§€ë„ì', 'ì›ìˆ˜ë‹˜', 'ìœ„ì›ì¥',\n",
    "            'í•µì‹¤í—˜', 'ë¯¸ì‚¬ì¼', 'ë¡œì¼“', 'ì¸ê³µìœ„ì„±', 'íƒ„ë„ë¯¸ì‚¬ì¼',\n",
    "            'ëŒ€ë‚¨', 'ë‚¨ì¡°ì„ ', 'í†µì¼', '6ìíšŒë‹´'\n",
    "        }\n",
    "        \n",
    "        # ì œì™¸í•  ì¼ë°˜ ë‹¨ì–´ë“¤ (ì˜¤íƒ ë°©ì§€)\n",
    "        self.exclude_words = {\n",
    "            'í•´ì£¼', 'ìˆœì²œ', 'ê°œì²œ', 'ì„±ì²œ', 'ì‹ ì²œ', 'ì•ˆì£¼', \n",
    "            'ê°•ê³„', 'íšŒì°½', 'ì˜¨ì²œ', 'ì˜ì›', 'ì‹ ì›', 'ê³ ì›', \n",
    "            'ëŒ€í¥', 'ì‹ ì–‘', 'ë´‰ì²œ', 'ì†¡í™”', 'ê³¼ì¼', 'ì‹ í¥',\n",
    "            'ë•ì„±', 'ì˜ê´‘', 'ê³ ì„±', 'ì² ì›', 'í‰ê°•', 'ê¹€í™”'\n",
    "        }\n",
    "\n",
    "    def _load_json_to_set(self, file_paths, dict_name):\n",
    "        \"\"\"\n",
    "        JSON íŒŒì¼(ë“¤)ì„ ì½ì–´ ì„¸íŠ¸(set)ë¡œ ë³€í™˜\n",
    "        \"\"\"\n",
    "        data_set = set()\n",
    "        for path in file_paths:\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"âš ï¸ ê²½ê³ : {dict_name} ì‚¬ì „ íŒŒì¼ '{path}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•´ë‹¹ ì‚¬ì „ì€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n",
    "                continue\n",
    "            try:\n",
    "                with open(path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    if isinstance(data, list):\n",
    "                        data_set.update(data)\n",
    "                    else:\n",
    "                        print(f\"âŒ ì˜¤ë¥˜: '{path}' íŒŒì¼ì˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¦¬ìŠ¤íŠ¸ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"âŒ ì˜¤ë¥˜: '{path}' íŒŒì¼ì˜ JSON í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "        return data_set\n",
    "\n",
    "    def _is_valid_context(self, sentence, location):\n",
    "        \"\"\"\n",
    "        ë¬¸ë§¥ìƒ í•´ë‹¹ ì§€ì—­ì´ ë¶í•œê³¼ ê´€ë ¨ìˆëŠ”ì§€ íŒë‹¨\n",
    "        \"\"\"\n",
    "        sentence_lower = sentence.lower()\n",
    "        \n",
    "        for keyword in self.nk_keywords:\n",
    "            if keyword.lower() in sentence_lower:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def _extract_with_spacy_ner(self, doc):\n",
    "        \"\"\"\n",
    "        spaCy NERì„ ì‚¬ìš©í•œ ì§€ì—­ ì¶”ì¶œ\n",
    "        \"\"\"\n",
    "        locations = []\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in ['GPE', 'LOC']:\n",
    "                location = ent.text.strip()\n",
    "                sentence = ent.sent.text\n",
    "                if self._is_valid_context(sentence, location):\n",
    "                    locations.append({\n",
    "                        'location': location,\n",
    "                        'context': sentence,\n",
    "                        'confidence': 'high',\n",
    "                        'method': 'spacy_ner'\n",
    "                    })\n",
    "        return locations\n",
    "\n",
    "    def _extract_with_flashtext(self, doc):\n",
    "        \"\"\"\n",
    "        Flashtextë¥¼ ì‚¬ìš©í•œ ì‚¬ì „ ê¸°ë°˜ ì¶”ì¶œ\n",
    "        \"\"\"\n",
    "        found_locations = []\n",
    "        for sentence in doc.sents:\n",
    "            sentence_text = sentence.text\n",
    "            if len(sentence_text.strip()) < 10:\n",
    "                continue\n",
    "\n",
    "            # ì§€ì—­ëª… ê²€ìƒ‰\n",
    "            locations_in_sentence = self.keyword_processor_locations.extract_keywords(sentence_text)\n",
    "            for location in set(locations_in_sentence):\n",
    "                if location in self.exclude_words:\n",
    "                    if self._is_valid_context(sentence_text, location):\n",
    "                        found_locations.append({\n",
    "                            'location': location,\n",
    "                            'context': sentence_text,\n",
    "                            'confidence': 'medium',\n",
    "                            'type': 'nk_location',\n",
    "                            'method': 'flashtext'\n",
    "                        })\n",
    "                else:\n",
    "                    found_locations.append({\n",
    "                        'location': location,\n",
    "                        'context': sentence_text,\n",
    "                        'confidence': 'high',\n",
    "                        'type': 'nk_location',\n",
    "                        'method': 'flashtext'\n",
    "                    })\n",
    "\n",
    "            # ì‹œì„¤ëª… ê²€ìƒ‰\n",
    "            facilities_in_sentence = self.keyword_processor_facilities.extract_keywords(sentence_text)\n",
    "            for facility in set(facilities_in_sentence):\n",
    "                found_locations.append({\n",
    "                    'location': facility,\n",
    "                    'context': sentence_text,\n",
    "                    'confidence': 'high',\n",
    "                    'type': 'nk_facility',\n",
    "                    'method': 'flashtext'\n",
    "                })\n",
    "        return found_locations\n",
    "\n",
    "    def _extract_combined_locations_with_regex(self, text):\n",
    "        \"\"\"\n",
    "        ì§€ì—­ëª…ê³¼ êµ°ì‚¬ ì‹œì„¤ëª… ì¡°í•© íŒ¨í„´ ì¶”ì¶œ (ì •ê·œì‹ ì‚¬ìš©)\n",
    "        \"\"\"\n",
    "        found_combined = []\n",
    "        \n",
    "        # Flashtextì˜ í‚¤ì›Œë“œ ëª©ë¡ì„ ì‚¬ìš©í•´ ì •ê·œì‹ íŒ¨í„´ ìƒì„±\n",
    "        nk_locations_pattern = '|'.join(re.escape(loc) for loc in self.nk_locations if len(loc) > 1)\n",
    "        nk_facilities_pattern = '|'.join(re.escape(fac) for fac in self.nk_military_facilities if len(fac) > 1)\n",
    "\n",
    "        if not nk_locations_pattern or not nk_facilities_pattern:\n",
    "            return []\n",
    "\n",
    "        combined_pattern = fr'({nk_locations_pattern})\\s*({nk_facilities_pattern})'\n",
    "        \n",
    "        sentences = re.split(r'[.!?]\\s+', text)\n",
    "        for sentence in sentences:\n",
    "            for match in re.finditer(combined_pattern, sentence):\n",
    "                location = match.group(1)\n",
    "                facility = match.group(2)\n",
    "                combined_name = f\"{location}{' ' if ' ' in sentence[match.start():match.end()] else ''}{facility}\"\n",
    "                \n",
    "                if self._is_valid_context(sentence, location):\n",
    "                    found_combined.append({\n",
    "                        'location': combined_name,\n",
    "                        'context': sentence,\n",
    "                        'confidence': 'high',\n",
    "                        'type': 'combined_facility',\n",
    "                        'method': 'regex'\n",
    "                    })\n",
    "        return found_combined\n",
    "\n",
    "    def extract_nk_locations_from_doc(self, doc):\n",
    "        \"\"\"\n",
    "        spaCy Doc ê°ì²´ì—ì„œ ë¶í•œ ê´€ë ¨ ì§€ì—­/ì¥ì†Œ ì •ë³´ ì¶”ì¶œ\n",
    "        \"\"\"\n",
    "        # 1. spaCy NER ì‚¬ìš©\n",
    "        spacy_results = self._extract_with_spacy_ner(doc)\n",
    "        \n",
    "        # 2. Flashtext ê¸°ë°˜ ì‚¬ì „ ì¶”ì¶œ\n",
    "        dict_results = self._extract_with_flashtext(doc)\n",
    "        \n",
    "        # 3. ì¡°í•©ëœ ë‹¨ì–´ ì¶”ì¶œ (ì •ê·œì‹)\n",
    "        combined_results = self._extract_combined_locations_with_regex(doc.text)\n",
    "        \n",
    "        all_results = spacy_results + dict_results + combined_results\n",
    "        unique_locations = {}\n",
    "        \n",
    "        for result in all_results:\n",
    "            location = result['location']\n",
    "            if location not in unique_locations:\n",
    "                unique_locations[location] = result\n",
    "            else:\n",
    "                if result.get('confidence') == 'high' and unique_locations[location].get('confidence') != 'high':\n",
    "                    unique_locations[location] = result\n",
    "        \n",
    "        return list(unique_locations.values())\n",
    "    \n",
    "    def _calculate_relevance_score(self, text, locations):\n",
    "        \"\"\"\n",
    "        ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        keyword_count = sum(1 for keyword in self.nk_keywords if keyword in text)\n",
    "        score += min(keyword_count * 8, 40)\n",
    "        \n",
    "        high_conf_count = sum(1 for loc in locations if loc.get('confidence') == 'high')\n",
    "        score += min(high_conf_count * 10, 40)\n",
    "        \n",
    "        med_conf_count = sum(1 for loc in locations if loc.get('confidence') == 'medium')\n",
    "        score += min(med_conf_count * 5, 20)\n",
    "        \n",
    "        return min(score, 100)\n",
    "\n",
    "    def analyze_results(self, full_text, locations_found, title=\"\"):\n",
    "        \"\"\"\n",
    "        ë¶„ì„ ê²°ê³¼ë¥¼ ìš”ì•½\n",
    "        \"\"\"\n",
    "        relevance_score = self._calculate_relevance_score(full_text, locations_found)\n",
    "        is_nk_related = (len([kw for kw in self.nk_keywords if kw in full_text]) > 0 and relevance_score > 30) or len(locations_found) > 0\n",
    "        \n",
    "        return {\n",
    "            'title': title,\n",
    "            'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'relevance_score': relevance_score,\n",
    "            'is_nk_related': is_nk_related,\n",
    "            'locations_found': locations_found,\n",
    "        }\n",
    "\n",
    "def process_articles_in_batches(extractor, articles, output_filename=\"re_extracted_locations_ten_year_all.jsonl\", batch_size=1000):\n",
    "    \"\"\"\n",
    "    ê¸°ì‚¬ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ê³  JSONL í˜•ì‹ìœ¼ë¡œ ì‹¤ì‹œê°„ ì €ì¥\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ”„ ì´ {len(articles)}ê°œ ê¸°ì‚¬ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¶„ì„í•©ë‹ˆë‹¤...\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    processed_count = 0\n",
    "    \n",
    "    with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
    "        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë°°ì¹˜ ìƒì„±\n",
    "        texts = [f\"{article['metadata']['title']} {article['text']} {article.get('summary', '')}\" for article in articles]\n",
    "        \n",
    "        for i, doc in enumerate(extractor.nlp.pipe(texts, batch_size=batch_size), 1):\n",
    "            article_index = i - 1\n",
    "            article = articles[article_index]\n",
    "            \n",
    "            locations_found = extractor.extract_nk_locations_from_doc(doc)\n",
    "            \n",
    "            summary = extractor.analyze_results(doc.text, locations_found, article['metadata']['title'])\n",
    "            \n",
    "            if summary['locations_found'] and summary['is_nk_related']:\n",
    "                result_entry = {\n",
    "                    'id_': article['id_'],\n",
    "                    'locations': [loc['location'] for loc in summary['locations_found']],\n",
    "                    'confidence_info': [\n",
    "                        {\n",
    "                            'location': loc['location'],\n",
    "                            'confidence': loc['confidence'],\n",
    "                            'type': loc.get('type', 'unknown'),\n",
    "                            'method': loc.get('method', 'unknown')\n",
    "                        }\n",
    "                        for loc in summary['locations_found']\n",
    "                    ]\n",
    "                }\n",
    "                outfile.write(json.dumps(result_entry, ensure_ascii=False) + '\\n')\n",
    "            \n",
    "            processed_count += 1\n",
    "            if processed_count % batch_size == 0 or processed_count == len(articles):\n",
    "                elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "                print(f\"ğŸ“Š {processed_count}/{len(articles)}ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. (ê²½ê³¼ ì‹œê°„: {elapsed_time:.2f}ì´ˆ)\")\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    print(f\"\\nâœ… ì „ì²´ ë¶„ì„ ì™„ë£Œ! ì´ {len(articles)}ê°œ ê¸°ì‚¬ ë¶„ì„ì— { (end_time - start_time).total_seconds():.2f}ì´ˆ ì†Œìš”ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"ğŸ’¾ ê²°ê³¼ê°€ {output_filename} íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "def main():\n",
    "    \"\"\"\n",
    "    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    print(\"ğŸ´â€â˜ ï¸ ë¶í•œ ê´€ë ¨ ì§€ì—­Â·ì¥ì†Œ ì¶”ì¶œê¸° (ìµœì í™” ë²„ì „)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        extractor = NorthKoreaLocationExtractor(\"ko_core_news_lg\")\n",
    "    except:\n",
    "        print(\"\\nëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ëª¨ë¸ë¡œ ì‹œë„í•©ë‹ˆë‹¤...\")\n",
    "        try:\n",
    "            extractor = NorthKoreaLocationExtractor(\"ko_core_news_md\")\n",
    "        except:\n",
    "            print(\"\\në‹¤ë¥¸ ëª¨ë¸ ë¡œë“œì—ë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. spaCy ëª¨ë¸ì„ ì„¤ì¹˜í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "            return\n",
    "\n",
    "    input_filename = \"/home/ds4_sia_nolb/#FINAL_POLARIS/04_plus_preprocessing/preprocessing_final_data/re_final_preprocessing.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(input_filename, 'r', encoding='utf-8') as f:\n",
    "            articles = json.load(f)\n",
    "        print(f\"ğŸ“ {input_filename} íŒŒì¼ì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ë¥¼ ì½ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: {input_filename} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: {input_filename} íŒŒì¼ì˜ JSON í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    process_articles_in_batches(extractor, articles)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
