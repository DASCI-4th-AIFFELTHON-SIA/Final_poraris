{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6b41308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ds4_sia_nolb/miniconda3/envs/vllm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ì„ cudaì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
      "âœ… KoBERT ê¸°ë°˜ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: snunlp/KR-SBERT-V40K-klueNLI-augSTS\n",
      "ëª¨ë¸ ìµœëŒ€ ì…ë ¥ ê¸¸ì´: 512\n",
      "ğŸ“„ ì´ ê¸°ì‚¬ ìˆ˜: 10470ê±´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê¸°ì‚¬ ìš”ì•½ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10470/10470 [06:26<00:00, 27.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 10470ê±´ì˜ ìš”ì•½ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: /home/ds4_sia_nolb/code/confirmed/summarized_llama_articles/summarized_articles_ballon.json\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, random, math\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# ===== ê¸°ë³¸ ì„¤ì • =====\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"âœ… ëª¨ë¸ì„ {device}ì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# ğŸ’¡ sentence-transformers ì „ìš© ëª¨ë¸ë¡œ ë³€ê²½\n",
    "MODEL_NAME = \"snunlp/KR-SBERT-V40K-klueNLI-augSTS\"\n",
    "DATA_DIR = \"/home/ds4_sia_nolb/llama_index_json/2024\"\n",
    "OUTPUT_DIR = \"/home/ds4_sia_nolb/code/confirmed/summarized_llama_articles\"\n",
    "OUTPUT_FILENAME = \"summarized_articles_ballon.json\"\n",
    "\n",
    "# ===== ëª¨ë¸ ë¡œë“œ =====\n",
    "model = SentenceTransformer(MODEL_NAME, device=device)\n",
    "print(f\"âœ… KoBERT ê¸°ë°˜ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {MODEL_NAME}\")\n",
    "\n",
    "max_model_input_length = 512\n",
    "print(f\"ëª¨ë¸ ìµœëŒ€ ì…ë ¥ ê¸¸ì´: {max_model_input_length}\")\n",
    "\n",
    "# ===== ì „ì²˜ë¦¬ =====\n",
    "NOISE_PATTERNS = [\n",
    "    r\"\\(ì˜ìƒ[^\\)]*\\)\",\n",
    "    r\"<ì €ì‘ê¶Œì\\(c\\).*?>\",\n",
    "    r\"ë¬´ë‹¨ì „ì¬\\s*ë°\\s*ì¬ë°°í¬\\s*ê¸ˆì§€\",\n",
    "    r\"ì¬ë°°í¬\\s*ê¸ˆì§€\",\n",
    "    r\"ì¬íŒë§¤\\s*ë°\\s*DB\\s*ê¸ˆì§€\",\n",
    "    r\"ì¬íŒë§¤\\s*ê¸ˆì§€\",\n",
    "    r\"DB\\s*ê¸ˆì§€\",\n",
    "    r\"ì œë³´ëŠ”\\s*ì¹´ì¹´ì˜¤í†¡.*\",\n",
    "    r\"êµ¬ë…ì¤‘|êµ¬ë… í•´ì§€|êµ¬ë…|ì´ì „\\s*ë‹¤ìŒ\",\n",
    "    r\"ì—°í•©ë‰´ìŠ¤\\s*TV|ì—°í•©\\s*ë‰´ìŠ¤\",\n",
    "    r\"ë‰´ìŠ¤ë ˆí„°\\s*êµ¬ë….*\",\n",
    "    r\"â“’\\s*ì—°í•©ë‰´ìŠ¤\",\n",
    "    r\"ì†¡ê³ \\s*ì‹œê°„[:ï¼š]?\\s*\\d{2}:\\d{2}\",\n",
    "    r\"ê¸°ì‚¬\\s*ì…ë ¥\\s*[:ï¼š]?.*\",\n",
    "    r\"ê¸°ì\\s*=\\s*\",\n",
    "    r\"\\(ì„œìš¸=.*?\\)\\s*\",\n",
    "    r\"\\([^)]+ì—°í•©ë‰´ìŠ¤\\)\\s*\",\n",
    "    r\"\\([^)]+=.*?\\)\\s*\",\n",
    "    r\"\\[.*?\\]\",              # ëŒ€ê´„í˜¸ ì „ì²´ ì œê±°\n",
    "    r\"ì´ë¯¸ì§€\\s*í™•ëŒ€\",\n",
    "]\n",
    "\n",
    "VIDEO_CLOSE_SPAN_RE = re.compile(\n",
    "    r'(?:(?<=^)|(?<=[\\.\\?\\!ã€‚ï¼Ÿï¼â€¦\\n,ï¼Œ;:]))\\s*.*?ì˜ìƒ\\s*ë‹«ê¸°\\s*',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "EMAIL_RE = re.compile(r\"[A-Za-z0-9._%+\\-]+@[A-Za-z0-9.\\-]+\\.[A-Za-z]{2,}\")\n",
    "\n",
    "# ì´ë¦„ íŒ¨í„´(í•œê¸€/ì˜ë¬¸)\n",
    "NAME_PATTERN = r\"(?:[ê°€-í£][ê°€-í£Â·]{1,4}(?:\\s[ê°€-í£Â·]{2,4})?|[A-Za-z][A-Za-z .'\\-]{0,30}[A-Za-z])\"\n",
    "# ê¸°ì/ì§í•¨ì´ ë¶™ì€ ê²½ìš°\n",
    "REPORTER_WITH_SUFFIX_RE = re.compile(\n",
    "    rf\"({NAME_PATTERN})\\s*(?:ê¸°ì|íŠ¹íŒŒì›|í†µì‹ ì›|ë…¼ì„¤ìœ„ì›|í‰ë¡ ê°€)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "# ì´ë©”ì¼ ì•ì— ì´ë¦„ì´ ì˜¤ëŠ” ê²½ìš°\n",
    "REPORTER_BEFORE_EMAIL_RE = re.compile(\n",
    "    rf\"({NAME_PATTERN})(?=\\s*[<\\(\\[]?\\s*[A-Za-z0-9._%+\\-]+@[A-Za-z0-9.\\-]+\\.[A-Za-z]{{2,}}\\s*[>\\)\\]]?)\"\n",
    ")\n",
    "\n",
    "# ===== ì „ì²˜ë¦¬ í•¨ìˆ˜ =====\n",
    "def preprocess_text(text: str):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\", set()\n",
    "\n",
    "    t = text\n",
    "\n",
    "    # 0) \"â€¦ ì˜ìƒ ë‹«ê¸°\" ì ˆ ì œê±°\n",
    "    t = VIDEO_CLOSE_SPAN_RE.sub(\" \", t)\n",
    "\n",
    "    # 1) ë…¸ì´ì¦ˆ íŒ¨í„´ ì œê±°\n",
    "    for pat in NOISE_PATTERNS:\n",
    "        t = re.sub(pat, \" \", t, flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    # 2) ì´ë©”ì¼ ì œê±°\n",
    "    t = EMAIL_RE.sub(\" \", t)\n",
    "\n",
    "    # 3) ê¸°ì ì´ë¦„ ì¶”ì¶œ(ë‘ ê³„ì—´ ëª¨ë‘)\n",
    "    names_with_suffix = {m.group(1).strip() for m in REPORTER_WITH_SUFFIX_RE.finditer(text)}\n",
    "    names_before_email = {m.group(1).strip() for m in REPORTER_BEFORE_EMAIL_RE.finditer(text)}\n",
    "    reporter_names = (names_with_suffix | names_before_email)\n",
    "\n",
    "    # 4) ê³µë°± ì •ë¦¬\n",
    "    t = re.sub(r\"[\\u200b\\u200e\\u200f\\ufeff]\", \"\", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip(\" ,ï¼Œ;:\")\n",
    "\n",
    "    return t, reporter_names\n",
    "\n",
    "# ===== ìš”ì•½ í›„ summary í›„ì²˜ë¦¬ =====\n",
    "def clean_summary(summary: str, reporter_names: set) -> str:\n",
    "    if not isinstance(summary, str):\n",
    "        return \"\"\n",
    "    cleaned = summary\n",
    "\n",
    "    for name in reporter_names:\n",
    "        # ì´ë¦„ ê²½ê³„\n",
    "        name_boundary = rf\"(?<![ê°€-í£A-Za-zÂ·]){re.escape(name)}(?![ê°€-í£A-Za-zÂ·])\"\n",
    "        # ì´ë¦„+ì§í•¨ ì œê±°\n",
    "        cleaned = re.sub(\n",
    "            rf\"{name_boundary}\\s*(?:ê¸°ì|íŠ¹íŒŒì›|í†µì‹ ì›|ë…¼ì„¤ìœ„ì›|í‰ë¡ ê°€)\",\n",
    "            \"\",\n",
    "            cleaned,\n",
    "            flags=re.IGNORECASE\n",
    "        )\n",
    "        # ì´ë¦„ ë‹¨ë… ì œê±°\n",
    "        cleaned = re.sub(name_boundary, \"\", cleaned)\n",
    "\n",
    "    # ì´ë©”ì¼ ì œê±°\n",
    "    cleaned = EMAIL_RE.sub(\"\", cleaned)\n",
    "\n",
    "    # ê³µë°±/êµ¬ë‘ì  ì •ë¦¬\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip(\" ,ï¼Œ;:\")\n",
    "    return cleaned\n",
    "\n",
    "def split_sentences_ko(text: str) -> List[str]:\n",
    "    try:\n",
    "        import kss\n",
    "        return [s.strip() for s in kss.split_sentences(text) if s.strip()]\n",
    "    except Exception:\n",
    "        sents = re.split(r'(?<=[\\.!?])\\s+(?=[â€œ\"(\\[]?[ê°€-í£A-Z0-9])', text)\n",
    "        return [s.strip() for s in sents if s.strip()]\n",
    "\n",
    "# ===== ì¶”ì¶œ ìš”ì•½ê¸° (Sentence-Transformers í™œìš©) =====\n",
    "def summarize_extractive(clean_text: str, num_sentences: int = 3) -> str:\n",
    "    if not clean_text:\n",
    "        return \"\"\n",
    "    sentences = split_sentences_ko(clean_text)\n",
    "    if len(sentences) <= num_sentences:\n",
    "        return \" \".join(sentences)\n",
    "\n",
    "    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    document_embedding = torch.mean(sentence_embeddings, dim=0).unsqueeze(0)\n",
    "    similarities = cosine_similarity(\n",
    "        sentence_embeddings.cpu().numpy(),\n",
    "        document_embedding.cpu().numpy()\n",
    "    ).flatten()\n",
    "\n",
    "    top_sentence_indices = similarities.argsort()[-num_sentences:][::-1]\n",
    "    top_sentence_indices.sort()\n",
    "    summarized_sentences = [sentences[i] for i in top_sentence_indices]\n",
    "    return \" \".join(summarized_sentences)\n",
    "\n",
    "# ===== ì‹¤í–‰ë¶€ =====\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    all_data = []\n",
    "    for filename in os.listdir(DATA_DIR):\n",
    "        if filename.endswith(\".json\"):\n",
    "            path = os.path.join(DATA_DIR, filename)\n",
    "            try:\n",
    "                with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    all_data.extend(data)\n",
    "                else:\n",
    "                    all_data.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {path} -> {e}\")\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"âŒ JSON ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        raise SystemExit(0)\n",
    "\n",
    "    print(f\"ğŸ“„ ì´ ê¸°ì‚¬ ìˆ˜: {len(all_data)}ê±´\")\n",
    "\n",
    "    for i, item in enumerate(tqdm(all_data, desc=\"ê¸°ì‚¬ ìš”ì•½ ì¤‘\")):\n",
    "        text = item.get(\"text\", \"\")\n",
    "        try:\n",
    "            clean_text, reporter_names = preprocess_text(text)\n",
    "            summary = summarize_extractive(clean_text, num_sentences=4)\n",
    "            summary = clean_summary(summary, reporter_names)\n",
    "        except Exception as e:\n",
    "            summary = \"\"\n",
    "            print(f\"âš ï¸ ìš”ì•½ ì‹¤íŒ¨(index={i}): {e}\")\n",
    "        item[\"summary\"] = summary\n",
    "\n",
    "    output_path = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"âœ… {len(all_data)}ê±´ì˜ ìš”ì•½ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
