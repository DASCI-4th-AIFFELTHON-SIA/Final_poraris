{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6b41308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ds4_sia_nolb/miniconda3/envs/vllm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델을 cuda에서 실행합니다.\n",
      "✅ KoBERT 기반 모델 로드 완료: snunlp/KR-SBERT-V40K-klueNLI-augSTS\n",
      "모델 최대 입력 길이: 512\n",
      "📄 총 기사 수: 10470건\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "기사 요약 중: 100%|██████████| 10470/10470 [06:26<00:00, 27.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 10470건의 요약 결과 저장 완료: /home/ds4_sia_nolb/code/confirmed/summarized_llama_articles/summarized_articles_ballon.json\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, random, math\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# ===== 기본 설정 =====\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"✅ 모델을 {device}에서 실행합니다.\")\n",
    "\n",
    "# 💡 sentence-transformers 전용 모델로 변경\n",
    "MODEL_NAME = \"snunlp/KR-SBERT-V40K-klueNLI-augSTS\"\n",
    "DATA_DIR = \"/home/ds4_sia_nolb/llama_index_json/2024\"\n",
    "OUTPUT_DIR = \"/home/ds4_sia_nolb/code/confirmed/summarized_llama_articles\"\n",
    "OUTPUT_FILENAME = \"summarized_articles_ballon.json\"\n",
    "\n",
    "# ===== 모델 로드 =====\n",
    "model = SentenceTransformer(MODEL_NAME, device=device)\n",
    "print(f\"✅ KoBERT 기반 모델 로드 완료: {MODEL_NAME}\")\n",
    "\n",
    "max_model_input_length = 512\n",
    "print(f\"모델 최대 입력 길이: {max_model_input_length}\")\n",
    "\n",
    "# ===== 전처리 =====\n",
    "NOISE_PATTERNS = [\n",
    "    r\"\\(영상[^\\)]*\\)\",\n",
    "    r\"<저작권자\\(c\\).*?>\",\n",
    "    r\"무단전재\\s*및\\s*재배포\\s*금지\",\n",
    "    r\"재배포\\s*금지\",\n",
    "    r\"재판매\\s*및\\s*DB\\s*금지\",\n",
    "    r\"재판매\\s*금지\",\n",
    "    r\"DB\\s*금지\",\n",
    "    r\"제보는\\s*카카오톡.*\",\n",
    "    r\"구독중|구독 해지|구독|이전\\s*다음\",\n",
    "    r\"연합뉴스\\s*TV|연합\\s*뉴스\",\n",
    "    r\"뉴스레터\\s*구독.*\",\n",
    "    r\"ⓒ\\s*연합뉴스\",\n",
    "    r\"송고\\s*시간[:：]?\\s*\\d{2}:\\d{2}\",\n",
    "    r\"기사\\s*입력\\s*[:：]?.*\",\n",
    "    r\"기자\\s*=\\s*\",\n",
    "    r\"\\(서울=.*?\\)\\s*\",\n",
    "    r\"\\([^)]+연합뉴스\\)\\s*\",\n",
    "    r\"\\([^)]+=.*?\\)\\s*\",\n",
    "    r\"\\[.*?\\]\",              # 대괄호 전체 제거\n",
    "    r\"이미지\\s*확대\",\n",
    "]\n",
    "\n",
    "VIDEO_CLOSE_SPAN_RE = re.compile(\n",
    "    r'(?:(?<=^)|(?<=[\\.\\?\\!。？！…\\n,，;:]))\\s*.*?영상\\s*닫기\\s*',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "EMAIL_RE = re.compile(r\"[A-Za-z0-9._%+\\-]+@[A-Za-z0-9.\\-]+\\.[A-Za-z]{2,}\")\n",
    "\n",
    "# 이름 패턴(한글/영문)\n",
    "NAME_PATTERN = r\"(?:[가-힣][가-힣·]{1,4}(?:\\s[가-힣·]{2,4})?|[A-Za-z][A-Za-z .'\\-]{0,30}[A-Za-z])\"\n",
    "# 기자/직함이 붙은 경우\n",
    "REPORTER_WITH_SUFFIX_RE = re.compile(\n",
    "    rf\"({NAME_PATTERN})\\s*(?:기자|특파원|통신원|논설위원|평론가)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "# 이메일 앞에 이름이 오는 경우\n",
    "REPORTER_BEFORE_EMAIL_RE = re.compile(\n",
    "    rf\"({NAME_PATTERN})(?=\\s*[<\\(\\[]?\\s*[A-Za-z0-9._%+\\-]+@[A-Za-z0-9.\\-]+\\.[A-Za-z]{{2,}}\\s*[>\\)\\]]?)\"\n",
    ")\n",
    "\n",
    "# ===== 전처리 함수 =====\n",
    "def preprocess_text(text: str):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\", set()\n",
    "\n",
    "    t = text\n",
    "\n",
    "    # 0) \"… 영상 닫기\" 절 제거\n",
    "    t = VIDEO_CLOSE_SPAN_RE.sub(\" \", t)\n",
    "\n",
    "    # 1) 노이즈 패턴 제거\n",
    "    for pat in NOISE_PATTERNS:\n",
    "        t = re.sub(pat, \" \", t, flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    # 2) 이메일 제거\n",
    "    t = EMAIL_RE.sub(\" \", t)\n",
    "\n",
    "    # 3) 기자 이름 추출(두 계열 모두)\n",
    "    names_with_suffix = {m.group(1).strip() for m in REPORTER_WITH_SUFFIX_RE.finditer(text)}\n",
    "    names_before_email = {m.group(1).strip() for m in REPORTER_BEFORE_EMAIL_RE.finditer(text)}\n",
    "    reporter_names = (names_with_suffix | names_before_email)\n",
    "\n",
    "    # 4) 공백 정리\n",
    "    t = re.sub(r\"[\\u200b\\u200e\\u200f\\ufeff]\", \"\", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip(\" ,，;:\")\n",
    "\n",
    "    return t, reporter_names\n",
    "\n",
    "# ===== 요약 후 summary 후처리 =====\n",
    "def clean_summary(summary: str, reporter_names: set) -> str:\n",
    "    if not isinstance(summary, str):\n",
    "        return \"\"\n",
    "    cleaned = summary\n",
    "\n",
    "    for name in reporter_names:\n",
    "        # 이름 경계\n",
    "        name_boundary = rf\"(?<![가-힣A-Za-z·]){re.escape(name)}(?![가-힣A-Za-z·])\"\n",
    "        # 이름+직함 제거\n",
    "        cleaned = re.sub(\n",
    "            rf\"{name_boundary}\\s*(?:기자|특파원|통신원|논설위원|평론가)\",\n",
    "            \"\",\n",
    "            cleaned,\n",
    "            flags=re.IGNORECASE\n",
    "        )\n",
    "        # 이름 단독 제거\n",
    "        cleaned = re.sub(name_boundary, \"\", cleaned)\n",
    "\n",
    "    # 이메일 제거\n",
    "    cleaned = EMAIL_RE.sub(\"\", cleaned)\n",
    "\n",
    "    # 공백/구두점 정리\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip(\" ,，;:\")\n",
    "    return cleaned\n",
    "\n",
    "def split_sentences_ko(text: str) -> List[str]:\n",
    "    try:\n",
    "        import kss\n",
    "        return [s.strip() for s in kss.split_sentences(text) if s.strip()]\n",
    "    except Exception:\n",
    "        sents = re.split(r'(?<=[\\.!?])\\s+(?=[“\"(\\[]?[가-힣A-Z0-9])', text)\n",
    "        return [s.strip() for s in sents if s.strip()]\n",
    "\n",
    "# ===== 추출 요약기 (Sentence-Transformers 활용) =====\n",
    "def summarize_extractive(clean_text: str, num_sentences: int = 3) -> str:\n",
    "    if not clean_text:\n",
    "        return \"\"\n",
    "    sentences = split_sentences_ko(clean_text)\n",
    "    if len(sentences) <= num_sentences:\n",
    "        return \" \".join(sentences)\n",
    "\n",
    "    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    document_embedding = torch.mean(sentence_embeddings, dim=0).unsqueeze(0)\n",
    "    similarities = cosine_similarity(\n",
    "        sentence_embeddings.cpu().numpy(),\n",
    "        document_embedding.cpu().numpy()\n",
    "    ).flatten()\n",
    "\n",
    "    top_sentence_indices = similarities.argsort()[-num_sentences:][::-1]\n",
    "    top_sentence_indices.sort()\n",
    "    summarized_sentences = [sentences[i] for i in top_sentence_indices]\n",
    "    return \" \".join(summarized_sentences)\n",
    "\n",
    "# ===== 실행부 =====\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    all_data = []\n",
    "    for filename in os.listdir(DATA_DIR):\n",
    "        if filename.endswith(\".json\"):\n",
    "            path = os.path.join(DATA_DIR, filename)\n",
    "            try:\n",
    "                with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    all_data.extend(data)\n",
    "                else:\n",
    "                    all_data.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ 파일 로드 실패: {path} -> {e}\")\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"❌ JSON 데이터가 없습니다.\")\n",
    "        raise SystemExit(0)\n",
    "\n",
    "    print(f\"📄 총 기사 수: {len(all_data)}건\")\n",
    "\n",
    "    for i, item in enumerate(tqdm(all_data, desc=\"기사 요약 중\")):\n",
    "        text = item.get(\"text\", \"\")\n",
    "        try:\n",
    "            clean_text, reporter_names = preprocess_text(text)\n",
    "            summary = summarize_extractive(clean_text, num_sentences=4)\n",
    "            summary = clean_summary(summary, reporter_names)\n",
    "        except Exception as e:\n",
    "            summary = \"\"\n",
    "            print(f\"⚠️ 요약 실패(index={i}): {e}\")\n",
    "        item[\"summary\"] = summary\n",
    "\n",
    "    output_path = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"✅ {len(all_data)}건의 요약 결과 저장 완료: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
