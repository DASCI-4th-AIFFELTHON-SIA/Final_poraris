{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba147ae6",
   "metadata": {},
   "source": [
    "**LLM기반 정보 추출**     \n",
    "LLM을 활용하여 뉴스 기사 중에서 `중요한 기사만 골라내고`    \n",
    "몽고db 컬렉션 `yna+preprocessed_v2`에 있는 문서에서 아래의 정보를 추출하는것이 목표:\n",
    "| 항목             | 설명                                 |\n",
    "| -------------- | ---------------------------------- |\n",
    "| `기사 날짜`        | pubDate (기존 필드)                    |\n",
    "| `기사 카테고리`      | 기존 category 필드                     |\n",
    "| `사건 발생일자`      | 본문 내 상대 표현을 변환                     |\n",
    "| `사건 장소`        | ex. \"평양\", \"청진항\" 등                  |\n",
    "| `핵심 인물`        | 김정은, 라브로프 등                        |\n",
    "| `기사 요약`        | 요약된 2\\~3줄                          |\n",
    "| `기사 URL`       | 기존 url 필드                          |\n",
    "| `핵심 키워드`       | \"ICBM\", \"군사 협력\", \"라브로프\", \"무기 이전\" 등 |\n",
    "| ✅ **중요 기사 여부** | 중요하면 추출, 아니면 제외                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a58023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB에서 데이터 불러오기\n",
    "from pymongo import MongoClient, errors\n",
    "import datetime\n",
    "\n",
    "# --- MongoDB 연결 설정 ---\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "DB_NAME = \"polaris\"\n",
    "SOURCE_COLLECTION_NAME = \"yna_preprocessed_v3\" \n",
    "\n",
    "# --- 날짜 범위 설정 ---\n",
    "# 시작 날짜: 년 월 일 시 분 초 \n",
    "start_date = datetime.datetime(2024, 5, 1, 0, 0, 0)\n",
    "# 종료 날짜: 년 월 일 시 분 초 \n",
    "end_date = datetime.datetime(2024, 5, 2, 23, 59, 59)\n",
    "\n",
    "raw_mongo_docs = []\n",
    "client = None\n",
    "try:\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[DB_NAME]\n",
    "    source_collection = db[SOURCE_COLLECTION_NAME]\n",
    "\n",
    "    print(f\"MongoDB에 연결되었습니다. '{SOURCE_COLLECTION_NAME}' 컬렉션에서 특정 날짜 범위의 문서를 로드합니다...\")\n",
    "\n",
    "    # MongoDB 쿼리: pubDate 필드가 start_date 이상이고 end_date 이하인 문서\n",
    "    date_range_query = {\n",
    "        \"pubDate\": {\n",
    "            \"$gte\": start_date,\n",
    "            \"$lte\": end_date\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # pubDate를 기준으로 오름차순 정렬하여 문서 조회\n",
    "    cursor = source_collection.find(date_range_query).sort('pubDate', 1) \n",
    "    \n",
    "    for doc in cursor:\n",
    "        raw_mongo_docs.append(doc)\n",
    "    \n",
    "    print(f\"총 {len(raw_mongo_docs)}개의 문서를 MongoDB에서 로드했습니다.\")\n",
    "    print(f\"날짜 범위: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "except errors.ConnectionFailure as e:\n",
    "    print(f\"MongoDB 연결 오류: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"데이터 로드 중 오류 발생: {e}\")\n",
    "finally:\n",
    "    if client:\n",
    "        client.close()\n",
    "        print(\"MongoDB 연결을 닫았습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12e5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라마 인덱스 문서 변환하기\n",
    "from llama_index.core import Document\n",
    "import json\n",
    "\n",
    "all_llama_docs = []\n",
    "print(\"로드된 문서를 LlamaIndex Document로 변환 중...\")\n",
    "    \n",
    "for doc in raw_mongo_docs:\n",
    "    content_text = doc.get(\"content\", \"\")\n",
    "    if not isinstance(content_text, str):\n",
    "        content_text = str(content_text) if content_text is not None else \"\"\n",
    "\n",
    "    title_text = doc.get(\"title\", \"\")\n",
    "    if not isinstance(title_text, str):\n",
    "        title_text = str(title_text) if title_text is not None else \"\"\n",
    "\n",
    "    llama_doc = Document(\n",
    "        text=content_text, \n",
    "        metadata={\n",
    "            \"title\": title_text, \n",
    "            \"pubDate\": str(doc.get(\"pubDate\", \"\")), \n",
    "            \"url\": doc.get(\"url\", \"\"),\n",
    "            \"category\": doc.get(\"category\")\n",
    "        }\n",
    "    )\n",
    "    all_llama_docs.append(llama_doc)\n",
    "    \n",
    "print(f\"총 {len(all_llama_docs)}개의 문서를 LlamaIndex Document로 변환했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede1b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LlamaIndex Node로 청킹하기\n",
    "\n",
    "# from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# if 'all_llama_docs' not in locals() or not all_llama_docs:\n",
    "#     print(\"오류: 'all_llama_docs'가 비어 있거나 생성되지 않았습니다.\")\n",
    "# else:\n",
    "#     print(\"LlamaIndex Document들을 Node(청크)로 분할 중...\")\n",
    "#     # LLM 모델 없이 텍스트 기반 청킹만 수행\n",
    "#     node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20) # 청크 사이즈 조절 가능\n",
    "#     all_nodes = node_parser.get_nodes_from_documents(all_llama_docs)\n",
    "    \n",
    "#     print(f\"총 {len(all_nodes)}개의 노드(청크)를 생성했습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2d6c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성된 노드의 첫 텍스트 미리보기\n",
    "    # if all_nodes:\n",
    "    #    print(f\"첫 번째 Node 내용 미리보기: {all_nodes[0].text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fdba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node들을 JSON 파일로 저장하기\n",
    "\n",
    "import json\n",
    "\n",
    "if 'all_llama_docs' not in locals() or not all_llama_docs:\n",
    "    print(\"오류: 'all_llama_docs'가 비어 있거나 생성되지 않았습니다.\")\n",
    "else:\n",
    "    print(\"생성된 노드(청크)들을 직렬화하여 파일로 저장 중...\")\n",
    "\n",
    "    # 각 노드를 딕셔너리로 변환하여 JSON으로 저장\n",
    "    serializable_nodes = [node.to_dict() for node in all_llama_docs]\n",
    "    \n",
    "    output_filename = \"prepared_nodes_for_llm_vm.json\"\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(serializable_nodes, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"변환된 노드 데이터가 '{output_filename}' 파일로 성공적으로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ef1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Node들을 JSON 파일로 저장하기(청킹 할 떄)\n",
    "\n",
    "import json\n",
    "# LlamaIndex 0.10.x 이상에서 Node 객체의 to_dict()를 위해 필요할 수 있습니다.\n",
    "# from llama_index.core.schema import Node \n",
    "\n",
    "# LlamaIndex Node 객체의 to_dict() 결과에서 불필요한 필드를 정리하는 함수\n",
    "def clean_llama_node_dict(data: dict) -> dict:\n",
    "    cleaned_data = data.copy() # 원본 딕셔너리를 직접 수정하지 않기 위해 복사\n",
    "\n",
    "    # 1. 최상위 레벨의 불필요한 필드 제거\n",
    "    # 'embedding' 필드는 나중에 생성되므로 저장 시에는 제거\n",
    "    if 'embedding' in cleaned_data and cleaned_data['embedding'] is None:\n",
    "        del cleaned_data['embedding']\n",
    "    \n",
    "    # 텍스트 노드에서는 이미지, 오디오, 비디오 리소스가 없음\n",
    "    if 'image_resource' in cleaned_data and cleaned_data['image_resource'] is None:\n",
    "        del cleaned_data['image_resource']\n",
    "    if 'audio_resource' in cleaned_data and cleaned_data['audio_resource'] is None:\n",
    "        del cleaned_data['audio_resource']\n",
    "    if 'video_resource' in cleaned_data and cleaned_data['video_resource'] is None:\n",
    "        del cleaned_data['video_resource']\n",
    "\n",
    "    # 2. 'text_resource' 내부의 불필요한 필드 제거\n",
    "    if 'text_resource' in cleaned_data and isinstance(cleaned_data['text_resource'], dict):\n",
    "        tr = cleaned_data['text_resource']\n",
    "        if 'embeddings' in tr and tr['embeddings'] is None:\n",
    "            del tr['embeddings']\n",
    "        if 'path' in tr and tr['path'] is None:\n",
    "            del tr['path']\n",
    "        # text_resource.url은 원본 metadata.url과 다를 수 있으므로, null이면 제거\n",
    "        if 'url' in tr and tr['url'] is None: \n",
    "            del tr['url']\n",
    "        if 'mimetype' in tr and tr['mimetype'] is None:\n",
    "            del tr['mimetype']\n",
    "    return cleaned_data\n",
    "\n",
    "if 'all_nodes' not in locals() or not all_nodes:\n",
    "    print(\"오류: 'all_nodes'가 비어 있거나 생성되지 않았습니다. 이전 구문을 먼저 실행해주세요.\")\n",
    "else:\n",
    "    print(\"생성된 노드(청크)들을 직렬화하여 파일로 저장 중...\")\n",
    "\n",
    "    serializable_nodes = []\n",
    "    for node in all_nodes:\n",
    "        node_dict = node.to_dict() # Node 객체를 딕셔너리로 변환\n",
    "        cleaned_node_dict = clean_llama_node_dict(node_dict) # 불필요한 필드 제거\n",
    "        serializable_nodes.append(cleaned_node_dict)\n",
    "    \n",
    "    output_filename = \"prepared_nodes_for_llm_vm_6_30.json\"\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(serializable_nodes, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"변환된 노드 데이터가 '{output_filename}' 파일로 성공적으로 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
