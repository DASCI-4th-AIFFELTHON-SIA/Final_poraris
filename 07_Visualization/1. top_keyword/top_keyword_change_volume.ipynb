{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d10ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 2016ÎÖÑ Ïó∞Í∞Ñ ÌÇ§ÏõåÎìú Î≥ÄÌôî Î∂ÑÏÑù ÏãúÏûë ===\n",
      "\n",
      "[1Ïõî] Î∂ÑÏÑù Ï§ë... -> /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/monthly_results_cluster/2016_01_keyword_grouped.json\n",
      "ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/monthly_results_cluster/2015_12_keyword_grouped.json\n",
      "  ‚ÑπÔ∏è  Ï†ÑÎÖÑÎèÑ(#2015) 12Ïõî Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏñ¥ 1ÏõîÏùÄ 'Ïã†Í∑ú'Î°ú Ï≤òÎ¶¨Îê©ÎãàÎã§.\n",
      "  ‚úì 1Ïõî Î∂ÑÏÑù ÏôÑÎ£å (ÌÇ§ÏõåÎìú 10Í∞ú)\n",
      "\n",
      "[2Ïõî] Î∂ÑÏÑù Ï§ë... -> /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/monthly_results_cluster/2016_02_keyword_grouped.json\n",
      "  ‚úì 2Ïõî Î∂ÑÏÑù ÏôÑÎ£å (ÌÇ§ÏõåÎìú 10Í∞ú)\n",
      "\n",
      "[3Ïõî] Î∂ÑÏÑù Ï§ë... -> /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/monthly_results_cluster/2016_03_keyword_grouped.json\n",
      "  ‚úì 3Ïõî Î∂ÑÏÑù ÏôÑÎ£å (ÌÇ§ÏõåÎìú 10Í∞ú)\n",
      "\n",
      "[4Ïõî] Î∂ÑÏÑù Ï§ë... -> /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/monthly_results_cluster/2016_04_keyword_grouped.json\n",
      "  ‚úì 4Ïõî Î∂ÑÏÑù ÏôÑÎ£å (ÌÇ§ÏõåÎìú 10Í∞ú)\n",
      "\n",
      "[5Ïõî] Î∂ÑÏÑù Ï§ë... -> /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/monthly_results_cluster/2016_05_keyword_grouped.json\n",
      "  ‚úì 5Ïõî Î∂ÑÏÑù ÏôÑÎ£å (ÌÇ§ÏõåÎìú 10Í∞ú)\n",
      "\n",
      "[6Ïõî] Î∂ÑÏÑù Ï§ë... -> /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/monthly_results_cluster/2016_06_keyword_grouped.json\n",
      "  ‚úì 6Ïõî Î∂ÑÏÑù ÏôÑÎ£å (ÌÇ§ÏõåÎìú 10Í∞ú)\n",
      "\n",
      "[7Ïõî] Î∂ÑÏÑù Ï§ë... -> /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/monthly_results_cluster/2016_07_keyword_grouped.json\n",
      "  ‚úì 7Ïõî Î∂ÑÏÑù ÏôÑÎ£å (ÌÇ§ÏõåÎìú 10Í∞ú)\n",
      "\n",
      "[8Ïõî] Î∂ÑÏÑù Ï§ë... -> /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/monthly_results_cluster/2016_08_keyword_grouped.json\n",
      "  ‚úì 8Ïõî Î∂ÑÏÑù ÏôÑÎ£å (ÌÇ§ÏõåÎìú 10Í∞ú)\n",
      "\n",
      "[9Ïõî] Î∂ÑÏÑù Ï§ë... -> /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/monthly_results_cluster/2016_09_keyword_grouped.json\n",
      "  ‚úì 9Ïõî Î∂ÑÏÑù ÏôÑÎ£å (ÌÇ§ÏõåÎìú 10Í∞ú)\n",
      "\n",
      "[10Ïõî] Î∂ÑÏÑù Ï§ë... -> /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/monthly_results_cluster/2016_10_keyword_grouped.json\n",
      "  ‚úì 10Ïõî Î∂ÑÏÑù ÏôÑÎ£å (ÌÇ§ÏõåÎìú 10Í∞ú)\n",
      "\n",
      "[11Ïõî] Î∂ÑÏÑù Ï§ë... -> /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/monthly_results_cluster/2016_11_keyword_grouped.json\n",
      "  ‚úì 11Ïõî Î∂ÑÏÑù ÏôÑÎ£å (ÌÇ§ÏõåÎìú 10Í∞ú)\n",
      "\n",
      "[12Ïõî] Î∂ÑÏÑù Ï§ë... -> /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/monthly_results_cluster/2016_12_keyword_grouped.json\n",
      "  ‚úì 12Ïõî Î∂ÑÏÑù ÏôÑÎ£å (ÌÇ§ÏõåÎìú 10Í∞ú)\n",
      "\n",
      "Í≤∞Í≥ºÍ∞Ä Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§: /home/ds4_sia_nolb/#FINAL_POLARIS/07_Visualization/1.top_keyword/monthly_keyword_change_volume/yearly_analysis_2016.json\n",
      "\n",
      "==================================================\n",
      "  2016ÎÖÑ ÌÇ§ÏõåÎìú Î≥ÄÌôî Î∂ÑÏÑù ÏöîÏïΩ\n",
      "==================================================\n",
      "Î∂ÑÏÑù ÏôÑÎ£å ÏõîÏàò: 12Í∞úÏõî\n",
      "ÏõîÌèâÍ∑† Î∂ÑÏÑù ÌÇ§ÏõåÎìú: 10.0Í∞ú\n",
      "Ïó∞Í∞Ñ Ïã†Í∑ú ÌÇ§ÏõåÎìú Ï¥ùÌï©: 75Í∞ú\n",
      "ÏõîÌèâÍ∑† Ïã†Í∑ú ÌÇ§ÏõåÎìú: 6.2Í∞ú\n",
      "\n",
      "ÏõîÎ≥Ñ ÌèâÍ∑† Î¨∏ÏÑúÏàò Ï∂îÏù¥:\n",
      "   1Ïõî: 184.4\n",
      "   2Ïõî: 355.9\n",
      "   3Ïõî: 295.0\n",
      "   4Ïõî: 189.9\n",
      "   5Ïõî: 156.4\n",
      "   6Ïõî: 113.1\n",
      "   7Ïõî: 88.4\n",
      "   8Ïõî: 166.5\n",
      "   9Ïõî: 177.4\n",
      "  10Ïõî: 111.4\n",
      "  11Ïõî: 29.9\n",
      "  12Ïõî: 47.8\n",
      "\n",
      "Î∂ÑÏÑù ÏôÑÎ£å ÏãúÍ∞Ñ: 2025-09-08T07:26:07.160485\n",
      "\n",
      "üéâ 2016ÎÖÑ Ïó∞Í∞Ñ ÌÇ§ÏõåÎìú Î≥ÄÌôî Î∂ÑÏÑùÏù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§!\n",
      "Í≤∞Í≥º ÌååÏùº: /home/ds4_sia_nolb/#FINAL_POLARIS/07_Visualization/1.top_keyword/monthly_keyword_change_volume/yearly_analysis_2016.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class YearlyKeywordChangeAnalyzer:\n",
    "    \"\"\"1ÎÖÑÍ∞Ñ ÌÇ§ÏõåÎìú Î≥ÄÌôîÎ•º Î∂ÑÏÑùÌïòÎäî ÌÅ¥ÎûòÏä§\"\"\"\n",
    "    \n",
    "    def __init__(self, similarity_threshold: float = 0.3):\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "    \n",
    "    def find_similar_keyword(self, current_keyword: Dict, previous_keywords: List[Dict]) -> Optional[Dict]:\n",
    "        \"\"\"ÌòÑÏû¨ ÌÇ§ÏõåÎìúÏôÄ ÎπÑÏä∑Ìïú Ïù¥Ï†Ñ ÌÇ§ÏõåÎìúÎ•º Ï∞æÎäî Ìï®Ïàò\"\"\"\n",
    "        current_phrase = current_keyword['phrase'].lower()\n",
    "        current_merged = current_keyword.get('merged_keywords', [])\n",
    "        \n",
    "        # 1. Ï†ïÌôïÌûà Í∞ôÏùÄ ÌÇ§ÏõåÎìú Ï∞æÍ∏∞\n",
    "        for prev_keyword in previous_keywords:\n",
    "            if prev_keyword['phrase'].lower() == current_phrase:\n",
    "                return prev_keyword\n",
    "        \n",
    "        # 2. merged_keywordsÏóêÏÑú Îß§Ïπ≠ Ï∞æÍ∏∞\n",
    "        for prev_keyword in previous_keywords:\n",
    "            prev_merged = prev_keyword.get('merged_keywords', [])\n",
    "            \n",
    "            # ÌòÑÏû¨ ÌÇ§ÏõåÎìúÏùò merged_keywordsÏôÄ Ïù¥Ï†Ñ ÌÇ§ÏõåÎìúÏùò phrase ÎπÑÍµê\n",
    "            if any(merged.lower() == prev_keyword['phrase'].lower() for merged in current_merged):\n",
    "                return prev_keyword\n",
    "            \n",
    "            # ÌòÑÏû¨ ÌÇ§ÏõåÎìúÏùò phraseÏôÄ Ïù¥Ï†Ñ ÌÇ§ÏõåÎìúÏùò merged_keywords ÎπÑÍµê\n",
    "            if any(merged.lower() == current_phrase for merged in prev_merged):\n",
    "                return prev_keyword\n",
    "            \n",
    "            # merged_keywords Í∞Ñ ÍµêÏßëÌï© ÌôïÏù∏\n",
    "            intersection = [curr for curr in current_merged \n",
    "                          if any(prev.lower() == curr.lower() for prev in prev_merged)]\n",
    "            if intersection:\n",
    "                return prev_keyword\n",
    "        \n",
    "        # 3. Î∂ÄÎ∂Ñ Îß§Ïπ≠ (ÌÇ§ÏõåÎìú ÏùºÎ∂ÄÍ∞Ä Ìè¨Ìï®ÎêòÎäî Í≤ΩÏö∞)\n",
    "        for prev_keyword in previous_keywords:\n",
    "            prev_phrase = prev_keyword['phrase'].lower()\n",
    "            if current_phrase in prev_phrase or prev_phrase in current_phrase:\n",
    "                return prev_keyword\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_change_status(self, current: int, previous: int) -> str:\n",
    "        \"\"\"Î≥ÄÌôî ÏÉÅÌÉúÎ•º Í≤∞Ï†ïÌïòÎäî Ìï®Ïàò\"\"\"\n",
    "        if previous == 0:\n",
    "            return \"Ïã†Í∑ú\"\n",
    "        change = current - previous\n",
    "        if change > 0:\n",
    "            return \"Ï¶ùÍ∞Ä\"\n",
    "        elif change < 0:\n",
    "            return \"Í∞êÏÜå\"\n",
    "        else:\n",
    "            return \"ÎèôÏùº\"\n",
    "    \n",
    "    def analyze_monthly_changes(self, current_data: Dict, previous_data: Optional[Dict], top_n: int = 10) -> List[Dict]:\n",
    "        \"\"\"ÏõîÎ≥Ñ ÌÇ§ÏõåÎìú Î≥ÄÌôîÎ•º Î∂ÑÏÑùÌïòÎäî Ìï®Ïàò\"\"\"\n",
    "        # ÌòÑÏû¨ Îã¨ ÏÉÅÏúÑ NÍ∞ú ÌÇ§ÏõåÎìú\n",
    "        current_top = current_data['keywords'][:min(top_n, len(current_data['keywords']))]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for index, current_keyword in enumerate(current_top):\n",
    "            # Ïù¥Ï†Ñ Îã¨ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÎäî Í≤ΩÏö∞ÏóêÎßå ÎπÑÍµê\n",
    "            if previous_data:\n",
    "                matched_keyword = self.find_similar_keyword(current_keyword, previous_data['keywords'])\n",
    "                previous_doc_count = matched_keyword['doc_count'] if matched_keyword else 0\n",
    "                previous_score = matched_keyword['score'] if matched_keyword else 0\n",
    "                matched_phrase = matched_keyword['phrase'] if matched_keyword else \"ÏóÜÏùå\"\n",
    "            else:\n",
    "                # Ï≤´ Î≤àÏß∏ ÏõîÏù¥Í±∞ÎÇò Ïù¥Ï†Ñ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏùÑ Îïå\n",
    "                previous_doc_count = 0\n",
    "                previous_score = 0\n",
    "                matched_phrase = \"ÏóÜÏùå\"\n",
    "            \n",
    "            # Î≥ÄÌôîÎüâ Í≥ÑÏÇ∞\n",
    "            change = current_keyword['doc_count'] - previous_doc_count\n",
    "            \n",
    "            # Î≥ÄÌôîÏú® Í≥ÑÏÇ∞\n",
    "            if previous_doc_count > 0:\n",
    "                change_percent = round(((current_keyword['doc_count'] - previous_doc_count) / previous_doc_count) * 100, 1)\n",
    "            else:\n",
    "                change_percent = \"Ïã†Í∑ú\"\n",
    "            \n",
    "            result = {\n",
    "                'rank': index + 1,\n",
    "                'current_phrase': current_keyword['phrase'],\n",
    "                'current_doc_count': current_keyword['doc_count'],\n",
    "                'current_score': current_keyword['score'],\n",
    "                'previous_doc_count': previous_doc_count,\n",
    "                'previous_score': previous_score,\n",
    "                'change': change,\n",
    "                'change_percent': change_percent,\n",
    "                'matched_phrase': matched_phrase,\n",
    "                'status': self.get_change_status(current_keyword['doc_count'], previous_doc_count)\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def load_json_file(self, file_path: str) -> Optional[Dict]:\n",
    "        \"\"\"JSON ÌååÏùºÏùÑ Î°úÎìúÌïòÎäî Ìï®Ïàò\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                return json.load(file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {file_path}\")\n",
    "            return None\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"JSON ÌååÏùºÏùÑ ÏùΩÎäîÎç∞ Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {file_path}\")\n",
    "            return None\n",
    "    \n",
    "    def save_results_to_json(self, results: Dict, output_path: str):\n",
    "        \"\"\"Í≤∞Í≥ºÎ•º JSON ÌååÏùºÎ°ú Ï†ÄÏû•\"\"\"\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                json.dump(results, file, ensure_ascii=False, indent=2)\n",
    "            print(f\"Í≤∞Í≥ºÍ∞Ä Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§: {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ÌååÏùº Ï†ÄÏû• Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {e}\")\n",
    "    \n",
    "    def analyze_yearly_changes(self, year: int, input_dir: str, output_path: str, top_n: int = 10):\n",
    "        \"\"\"1ÎÖÑÍ∞ÑÏùò ÌÇ§ÏõåÎìú Î≥ÄÌôîÎ•º Î∂ÑÏÑùÌïòÍ≥† Ï†ÄÏû•\"\"\"\n",
    "        yearly_results = {\n",
    "            'year': year,\n",
    "            'analysis_date': datetime.now().isoformat(),\n",
    "            'monthly_changes': {},\n",
    "            'yearly_summary': {}\n",
    "        }\n",
    "        \n",
    "        previous_month_data = None\n",
    "        all_monthly_stats = []\n",
    "        \n",
    "        print(f\"=== {year}ÎÖÑ Ïó∞Í∞Ñ ÌÇ§ÏõåÎìú Î≥ÄÌôî Î∂ÑÏÑù ÏãúÏûë ===\\n\")\n",
    "        \n",
    "        # 1ÏõîÎ∂ÄÌÑ∞ 12ÏõîÍπåÏßÄ ÏàúÏ∞® Î∂ÑÏÑù\n",
    "        for month in range(1, 13):\n",
    "            month_str = f\"{month:02d}\"\n",
    "            file_path = os.path.join(input_dir, f\"{year}_{month_str}_keyword_grouped.json\")\n",
    "            \n",
    "            print(f\"[{month}Ïõî] Î∂ÑÏÑù Ï§ë... -> {file_path}\")\n",
    "            \n",
    "            # ÌòÑÏû¨ Ïõî Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "            current_data = self.load_json_file(file_path)\n",
    "            if current_data is None:\n",
    "                print(f\"  ‚ö†Ô∏è  {month}Ïõî Îç∞Ïù¥ÌÑ∞Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Í±¥ÎÑàÎúÅÎãàÎã§.\\n\")\n",
    "                # Ïù¥Ï†Ñ Îã¨ Îç∞Ïù¥ÌÑ∞Îäî Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ (Îã§Ïùå Îã¨ ÎπÑÍµêÎ•º ÏúÑÌï¥)\n",
    "                continue\n",
    "\n",
    "            # ‚úÖ Ï†ÑÏõî Îç∞Ïù¥ÌÑ∞ Í≤∞Ï†ï Î°úÏßÅ (Ïó∞ÎèÑ Í≤ΩÍ≥Ñ Ï≤òÎ¶¨ Ìè¨Ìï®)\n",
    "            if month == 1:\n",
    "                # Ï†ÑÎÖÑÎèÑ 12Ïõî ÌååÏùº ÏãúÎèÑ\n",
    "                prev_file = os.path.join(input_dir, f\"{year-1}_12_keyword_grouped.json\")\n",
    "                prev_data = self.load_json_file(prev_file)\n",
    "                if prev_data is None:\n",
    "                    print(f\"  ‚ÑπÔ∏è  Ï†ÑÎÖÑÎèÑ(#{year-1}) 12Ïõî Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏñ¥ 1ÏõîÏùÄ 'Ïã†Í∑ú'Î°ú Ï≤òÎ¶¨Îê©ÎãàÎã§.\")\n",
    "            else:\n",
    "                # ÏßÅÏ†Ñ Î∞òÎ≥µÏóêÏÑú Ï†ÄÏû•Ìï¥Îëî Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©\n",
    "                prev_data = previous_month_data\n",
    "            \n",
    "            # ÏõîÎ≥Ñ Î≥ÄÌôî Î∂ÑÏÑù\n",
    "            monthly_changes = self.analyze_monthly_changes(current_data, prev_data, top_n)\n",
    "            \n",
    "            # ÏõîÎ≥Ñ ÌÜµÍ≥Ñ Í≥ÑÏÇ∞\n",
    "            monthly_stats = self.calculate_monthly_stats(monthly_changes, month, prev_data is not None)\n",
    "            all_monthly_stats.append(monthly_stats)\n",
    "            \n",
    "            # Í≤∞Í≥º Ï†ÄÏû•\n",
    "            yearly_results['monthly_changes'][f\"{month:02d}\"] = {\n",
    "                'month': month,\n",
    "                'current_data_info': {\n",
    "                    'year': current_data.get('year', year),\n",
    "                    'month': current_data.get('month', month),\n",
    "                    'total_keywords': len(current_data['keywords'])\n",
    "                },\n",
    "                'changes': monthly_changes,\n",
    "                'stats': monthly_stats\n",
    "            }\n",
    "            \n",
    "            print(f\"  ‚úì {month}Ïõî Î∂ÑÏÑù ÏôÑÎ£å (ÌÇ§ÏõåÎìú {len(monthly_changes)}Í∞ú)\\n\")\n",
    "            \n",
    "            # Îã§Ïùå Î∞òÎ≥µÏùÑ ÏúÑÌï¥ ÌòÑÏû¨ Îç∞Ïù¥ÌÑ∞Î•º Ïù¥Ï†Ñ Îç∞Ïù¥ÌÑ∞Î°ú ÏÑ§Ï†ï\n",
    "            previous_month_data = current_data\n",
    "        \n",
    "        # Ïó∞Í∞Ñ ÏöîÏïΩ ÌÜµÍ≥Ñ Í≥ÑÏÇ∞\n",
    "        yearly_results['yearly_summary'] = self.calculate_yearly_summary(all_monthly_stats)\n",
    "        \n",
    "        # Í≤∞Í≥º Ï†ÄÏû•\n",
    "        self.save_results_to_json(yearly_results, output_path)\n",
    "        \n",
    "        # ÏöîÏïΩ Ï∂úÎ†•\n",
    "        self.print_yearly_summary(yearly_results)\n",
    "        \n",
    "        return yearly_results\n",
    "    \n",
    "    def calculate_monthly_stats(self, changes: List[Dict], month: int, has_previous: bool) -> Dict:\n",
    "        \"\"\"ÏõîÎ≥Ñ ÌÜµÍ≥Ñ Í≥ÑÏÇ∞\"\"\"\n",
    "        if not has_previous:\n",
    "            # Ï≤´ Î≤àÏß∏ Îã¨ÏùÄ (Ï†ÑÏõî Îç∞Ïù¥ÌÑ∞ ÏóÜÏúºÎ©¥) Î™®Îì† ÌÇ§ÏõåÎìúÍ∞Ä Ïã†Í∑ú\n",
    "            return {\n",
    "                'total_keywords': len(changes),\n",
    "                'new_keywords': len(changes),\n",
    "                'increased': 0,\n",
    "                'decreased': 0,\n",
    "                'unchanged': 0,\n",
    "                'biggest_increase': None,\n",
    "                'biggest_decrease': None,\n",
    "                'avg_doc_count': sum(c['current_doc_count'] for c in changes) / len(changes) if changes else 0\n",
    "            }\n",
    "        \n",
    "        new_keywords = len([c for c in changes if c['status'] == 'Ïã†Í∑ú'])\n",
    "        increased = len([c for c in changes if c['status'] == 'Ï¶ùÍ∞Ä'])\n",
    "        decreased = len([c for c in changes if c['status'] == 'Í∞êÏÜå'])\n",
    "        unchanged = len([c for c in changes if c['status'] == 'ÎèôÏùº'])\n",
    "        \n",
    "        # Í∞ÄÏû• ÌÅ∞ Î≥ÄÌôî\n",
    "        increases = [c for c in changes if c['change'] > 0]\n",
    "        decreases = [c for c in changes if c['change'] < 0]\n",
    "        \n",
    "        biggest_increase = max(increases, key=lambda x: x['change']) if increases else None\n",
    "        biggest_decrease = min(decreases, key=lambda x: x['change']) if decreases else None\n",
    "        \n",
    "        return {\n",
    "            'total_keywords': len(changes),\n",
    "            'new_keywords': new_keywords,\n",
    "            'increased': increased,\n",
    "            'decreased': decreased,\n",
    "            'unchanged': unchanged,\n",
    "            'biggest_increase': {\n",
    "                'phrase': biggest_increase['current_phrase'],\n",
    "                'change': biggest_increase['change'],\n",
    "                'change_percent': biggest_increase['change_percent']\n",
    "            } if biggest_increase else None,\n",
    "            'biggest_decrease': {\n",
    "                'phrase': biggest_decrease['current_phrase'],\n",
    "                'change': biggest_decrease['change'],\n",
    "                'change_percent': biggest_decrease['change_percent']\n",
    "            } if biggest_decrease else None,\n",
    "            'avg_doc_count': sum(c['current_doc_count'] for c in changes) / len(changes) if changes else 0\n",
    "        }\n",
    "    \n",
    "    def calculate_yearly_summary(self, monthly_stats: List[Dict]) -> Dict:\n",
    "        \"\"\"Ïó∞Í∞Ñ ÏöîÏïΩ ÌÜµÍ≥Ñ Í≥ÑÏÇ∞\"\"\"\n",
    "        if not monthly_stats:\n",
    "            return {}\n",
    "        \n",
    "        total_months = len(monthly_stats)\n",
    "        \n",
    "        return {\n",
    "            'analyzed_months': total_months,\n",
    "            'avg_keywords_per_month': sum(s['total_keywords'] for s in monthly_stats) / total_months,\n",
    "            'total_new_keywords': sum(s['new_keywords'] for s in monthly_stats),\n",
    "            'avg_new_keywords_per_month': sum(s['new_keywords'] for s in monthly_stats) / total_months,\n",
    "            'most_volatile_month': max(monthly_stats, key=lambda x: x['increased'] + x['decreased'])['total_keywords'] if monthly_stats else None,\n",
    "            'avg_doc_count_trend': [round(s['avg_doc_count'], 1) for s in monthly_stats]\n",
    "        }\n",
    "    \n",
    "    def print_yearly_summary(self, yearly_results: Dict):\n",
    "        \"\"\"Ïó∞Í∞Ñ ÏöîÏïΩ Í≤∞Í≥º Ï∂úÎ†•\"\"\"\n",
    "        year = yearly_results['year']\n",
    "        summary = yearly_results['yearly_summary']\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"  {year}ÎÖÑ ÌÇ§ÏõåÎìú Î≥ÄÌôî Î∂ÑÏÑù ÏöîÏïΩ\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Î∂ÑÏÑù ÏôÑÎ£å ÏõîÏàò: {summary['analyzed_months']}Í∞úÏõî\")\n",
    "        print(f\"ÏõîÌèâÍ∑† Î∂ÑÏÑù ÌÇ§ÏõåÎìú: {summary['avg_keywords_per_month']:.1f}Í∞ú\")\n",
    "        print(f\"Ïó∞Í∞Ñ Ïã†Í∑ú ÌÇ§ÏõåÎìú Ï¥ùÌï©: {summary['total_new_keywords']}Í∞ú\")\n",
    "        print(f\"ÏõîÌèâÍ∑† Ïã†Í∑ú ÌÇ§ÏõåÎìú: {summary['avg_new_keywords_per_month']:.1f}Í∞ú\")\n",
    "        \n",
    "        print(f\"\\nÏõîÎ≥Ñ ÌèâÍ∑† Î¨∏ÏÑúÏàò Ï∂îÏù¥:\")\n",
    "        for i, avg_count in enumerate(summary['avg_doc_count_trend'], 1):\n",
    "            print(f\"  {i:2d}Ïõî: {avg_count}\")\n",
    "        \n",
    "        print(f\"\\nÎ∂ÑÏÑù ÏôÑÎ£å ÏãúÍ∞Ñ: {yearly_results['analysis_date']}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò\"\"\"\n",
    "    # Î∂ÑÏÑùÍ∏∞ Ï¥àÍ∏∞Ìôî\n",
    "    analyzer = YearlyKeywordChangeAnalyzer(similarity_threshold=0.3)\n",
    "    \n",
    "    # ÏÑ§Ï†ïÍ∞íÎì§\n",
    "    year = 2016\n",
    "    input_directory = \"/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster\"\n",
    "    output_file = f\"/home/ds4_sia_nolb/#FINAL_POLARIS/07_Visualization/1.top_keyword/re_monthly_keyword_change_volume/yearly_analysis_{year}.json\"\n",
    "    top_keywords = 10\n",
    "    \n",
    "    # 1ÎÖÑÍ∞Ñ Î∂ÑÏÑù Ïã§Ìñâ\n",
    "    results = analyzer.analyze_yearly_changes(\n",
    "        year=year,\n",
    "        input_dir=input_directory,\n",
    "        output_path=output_file,\n",
    "        top_n=top_keywords\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüéâ {year}ÎÖÑ Ïó∞Í∞Ñ ÌÇ§ÏõåÎìú Î≥ÄÌôî Î∂ÑÏÑùÏù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§!\")\n",
    "    print(f\"Í≤∞Í≥º ÌååÏùº: {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
