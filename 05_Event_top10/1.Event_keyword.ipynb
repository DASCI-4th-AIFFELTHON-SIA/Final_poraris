{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e09688cd",
   "metadata": {},
   "source": [
    "# 핵심이슈 키워드 추출 후 월별 그룹화 파일 자동 추출\n",
    "\n",
    "- 핵심이슈 키워드를 TF-IDF로 추출\n",
    "- 추출한 데이터의 연도를 입력하면 해당 년도의 `이슈 키워드`를 1월부터 12월까지 `월별`로 추출을 시작함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3636ffa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 전체 기사 데이터를 로드하는 중...\n",
      "✔️ 기존 TF-IDF 벡터라이저 파일 '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_idf_vectorizer_for_all_corpus.pkl'이 이미 존재합니다. 학습을 건너뜁니다.\n",
      "\n",
      "🚀 2025년 월별 키워드 추출을 시작합니다...\n",
      "\n",
      "==================================================\n",
      "📅 2025년 1월 키워드 추출 중...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 80434개의 기사 중 지정 기간 내 기사 385개를 찾았습니다.\n",
      "지정 기간 내 기사 수: 385개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 결과: 행동동사 109개, 행위명사 99개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 코퍼스: 385개 문서\n",
      "TF-IDF 용어수: 359320\n",
      "\n",
      "=== 2025년 1월 '사건 구' TOP 30 (TF-IDF + DF 결합) ===\n",
      "1. 탄도미사일 발사   (점수=16.11, 문서수=44)\n",
      "2. 북한 탄도미사일 발사   (점수=10.80, 문서수=27)\n",
      "3. 시험 발사   (점수=7.70, 문서수=23)\n",
      "4. 북한 군인 생포   (점수=6.80, 문서수=17)\n",
      "5. 동해 발사   (점수=6.50, 문서수=20)\n",
      "6. 미사일 발사   (점수=6.50, 문서수=20)\n",
      "7. 영상 공개   (점수=6.50, 문서수=20)\n",
      "8. 군인 생포 밝히다   (점수=6.00, 문서수=15)\n",
      "9. 일대 동해 발사   (점수=5.60, 문서수=14)\n",
      "10. 단거리 탄도미사일 발사   (점수=5.60, 문서수=14)\n",
      "💾 결과가 '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_01_keywords.json'에 저장되었습니다.\n",
      "\n",
      "==================================================\n",
      "📅 2025년 2월 키워드 추출 중...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 80434개의 기사 중 지정 기간 내 기사 298개를 찾았습니다.\n",
      "지정 기간 내 기사 수: 298개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 결과: 행동동사 86개, 행위명사 91개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 코퍼스: 298개 문서\n",
      "TF-IDF 용어수: 359320\n",
      "\n",
      "=== 2025년 2월 '사건 구' TOP 30 (TF-IDF + DF 결합) ===\n",
      "1. 한미 협력   (점수=3.30, 문서수=12)\n",
      "2. 러시아 추가 파병   (점수=2.80, 문서수=7)\n",
      "3. 센터 목적 열리다   (점수=2.80, 문서수=7)\n",
      "4. 공업 공장 준공   (점수=2.80, 문서수=7)\n",
      "5. 현지 시간 밝히다   (점수=2.80, 문서수=7)\n",
      "6. 포로 한국 송환   (점수=2.40, 문서수=6)\n",
      "7. 김일 체육 단장   (점수=2.40, 문서수=6)\n",
      "8. 행정부 출범 처음   (점수=2.40, 문서수=6)\n",
      "9. 추가 파병   (점수=2.10, 문서수=9)\n",
      "10. 협력 확대   (점수=2.10, 문서수=9)\n",
      "💾 결과가 '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_02_keywords.json'에 저장되었습니다.\n",
      "\n",
      "==================================================\n",
      "📅 2025년 3월 키워드 추출 중...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 80434개의 기사 중 지정 기간 내 기사 297개를 찾았습니다.\n",
      "지정 기간 내 기사 수: 297개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 결과: 행동동사 83개, 행위명사 80개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 코퍼스: 297개 문서\n",
      "TF-IDF 용어수: 359320\n",
      "\n",
      "=== 2025년 3월 '사건 구' TOP 30 (TF-IDF + DF 결합) ===\n",
      "1. 연합 훈련   (점수=8.50, 문서수=25)\n",
      "2. 한미 연합 훈련   (점수=8.00, 문서수=20)\n",
      "3. 서해 수호 기념   (점수=2.80, 문서수=7)\n",
      "4. 귀순 의사 밝히다   (점수=2.80, 문서수=7)\n",
      "5. 김정욱 국기 추다   (점수=2.80, 문서수=7)\n",
      "6. 연합 훈련 비난   (점수=2.40, 문서수=6)\n",
      "7. 북한 탄도미사일 발사   (점수=2.40, 문서수=6)\n",
      "8. 현지 시간 밝히다   (점수=2.40, 문서수=6)\n",
      "9. 포로 한국 싶다   (점수=2.40, 문서수=6)\n",
      "10. 추가 파병   (점수=2.10, 문서수=9)\n",
      "💾 결과가 '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_03_keywords.json'에 저장되었습니다.\n",
      "\n",
      "==================================================\n",
      "📅 2025년 4월 키워드 추출 중...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 80434개의 기사 중 지정 기간 내 기사 287개를 찾았습니다.\n",
      "지정 기간 내 기사 수: 287개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 결과: 행동동사 63개, 행위명사 73개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 코퍼스: 287개 문서\n",
      "TF-IDF 용어수: 359320\n",
      "\n",
      "=== 2025년 4월 '사건 구' TOP 30 (TF-IDF + DF 결합) ===\n",
      "1. 파병 공식   (점수=7.71, 문서수=23)\n",
      "2. 공식 인정   (점수=6.90, 문서수=21)\n",
      "3. 공식 확인   (점수=4.50, 문서수=15)\n",
      "4. 파병 공식 확인   (점수=4.40, 문서수=11)\n",
      "5. 파병 공식 인정   (점수=4.00, 문서수=10)\n",
      "6. 북한 파병 공식   (점수=4.00, 문서수=10)\n",
      "7. 영상 공개   (점수=3.30, 문서수=12)\n",
      "8. 러시아 파병 공식   (점수=3.20, 문서수=8)\n",
      "9. 북한 파병 인정   (점수=3.20, 문서수=8)\n",
      "10. 북한 국무위원 공개   (점수=3.20, 문서수=8)\n",
      "💾 결과가 '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_04_keywords.json'에 저장되었습니다.\n",
      "\n",
      "==================================================\n",
      "📅 2025년 5월 키워드 추출 중...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 80434개의 기사 중 지정 기간 내 기사 226개를 찾았습니다.\n",
      "지정 기간 내 기사 수: 226개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 결과: 행동동사 54개, 행위명사 69개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 코퍼스: 226개 문서\n",
      "TF-IDF 용어수: 359320\n",
      "\n",
      "=== 2025년 5월 '사건 구' TOP 30 (TF-IDF + DF 결합) ===\n",
      "1. 탄도미사일 발사   (점수=4.10, 문서수=14)\n",
      "2. 현지 시간 보도   (점수=3.20, 문서수=8)\n",
      "3. 지질 공원 지정   (점수=2.80, 문서수=7)\n",
      "4. 단거리 탄도미사일 발사   (점수=2.80, 문서수=7)\n",
      "5. 지난 평양 도착   (점수=2.40, 문서수=6)\n",
      "6. 동해 탄도미사일 발사   (점수=2.40, 문서수=6)\n",
      "7. 북한 탄도미사일 발사   (점수=2.40, 문서수=6)\n",
      "8. 동해 발사   (점수=2.10, 문서수=9)\n",
      "9. 모스크바 광장 열리다   (점수=2.00, 문서수=5)\n",
      "10. 일대 동해 발사   (점수=2.00, 문서수=5)\n",
      "💾 결과가 '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_05_keywords.json'에 저장되었습니다.\n",
      "\n",
      "==================================================\n",
      "📅 2025년 6월 키워드 추출 중...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 80434개의 기사 중 지정 기간 내 기사 277개를 찾았습니다.\n",
      "지정 기간 내 기사 수: 277개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 결과: 행동동사 74개, 행위명사 79개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 코퍼스: 277개 문서\n",
      "TF-IDF 용어수: 359320\n",
      "\n",
      "=== 2025년 6월 '사건 구' TOP 30 (TF-IDF + DF 결합) ===\n",
      "1. 대남 소음 방송   (점수=6.41, 문서수=16)\n",
      "2. 대북 확성기 방송   (점수=6.00, 문서수=15)\n",
      "3. 소음 방송   (점수=4.91, 문서수=16)\n",
      "4. 확성기 방송   (점수=4.90, 문서수=16)\n",
      "5. 진수식 도중 넘어지다   (점수=4.00, 문서수=10)\n",
      "6. 방송 중단   (점수=3.30, 문서수=12)\n",
      "7. 소음 방송 중단   (점수=3.20, 문서수=8)\n",
      "8. 확성기 방송 중단   (점수=3.20, 문서수=8)\n",
      "9. 현지 시간 보도   (점수=3.20, 문서수=8)\n",
      "10. 원산 갈다   (점수=2.90, 문서수=11)\n",
      "💾 결과가 '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_06_keywords.json'에 저장되었습니다.\n",
      "\n",
      "==================================================\n",
      "📅 2025년 7월 키워드 추출 중...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 80434개의 기사 중 지정 기간 내 기사 114개를 찾았습니다.\n",
      "지정 기간 내 기사 수: 114개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 결과: 행동동사 43개, 행위명사 24개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 코퍼스: 114개 문서\n",
      "TF-IDF 용어수: 359320\n",
      "\n",
      "=== 2025년 7월 '사건 구' TOP 30 (TF-IDF + DF 결합) ===\n",
      "1. 명의 신병 확보   (점수=4.01, 문서수=10)\n",
      "2. 관계 기관 조사   (점수=3.60, 문서수=9)\n",
      "3. 군사분계선 넘어오다   (점수=2.91, 문서수=11)\n",
      "4. 신병 확보   (점수=2.51, 문서수=10)\n",
      "5. 기관 조사   (점수=2.50, 문서수=10)\n",
      "6. 북한 원산 갈다   (점수=2.00, 문서수=5)\n",
      "7. 전선 군사분계선 넘어오다   (점수=2.00, 문서수=5)\n",
      "8. 주민 동해 송환   (점수=2.00, 문서수=5)\n",
      "9. 해당 인원 식별   (점수=2.00, 문서수=5)\n",
      "10. 신병 확보 밝히다   (점수=2.00, 문서수=5)\n",
      "💾 결과가 '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_07_keywords.json'에 저장되었습니다.\n",
      "\n",
      "==================================================\n",
      "📅 2025년 8월 키워드 추출 중...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 80434개의 기사 중 지정 기간 내 기사 0개를 찾았습니다.\n",
      "⚠️ 2025년 8월에 기사가 없습니다.\n",
      "\n",
      "==================================================\n",
      "📅 2025년 9월 키워드 추출 중...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 80434개의 기사 중 지정 기간 내 기사 0개를 찾았습니다.\n",
      "⚠️ 2025년 9월에 기사가 없습니다.\n",
      "\n",
      "==================================================\n",
      "📅 2025년 10월 키워드 추출 중...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 80434개의 기사 중 지정 기간 내 기사 0개를 찾았습니다.\n",
      "⚠️ 2025년 10월에 기사가 없습니다.\n",
      "\n",
      "==================================================\n",
      "📅 2025년 11월 키워드 추출 중...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 80434개의 기사 중 지정 기간 내 기사 0개를 찾았습니다.\n",
      "⚠️ 2025년 11월에 기사가 없습니다.\n",
      "\n",
      "==================================================\n",
      "📅 2025년 12월 키워드 추출 중...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 80434개의 기사 중 지정 기간 내 기사 0개를 찾았습니다.\n",
      "⚠️ 2025년 12월에 기사가 없습니다.\n",
      "\n",
      "🎉 2025년 월별 키워드 추출이 완료되었습니다!\n",
      "📁 결과 파일들이 '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results' 디렉토리에 저장되었습니다.\n",
      "📊 연간 종합 결과: '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_keywords_by_month_all.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from konlpy.tag import Okt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "import calendar\n",
    "\n",
    "# =========================\n",
    "# 설정\n",
    "# =========================\n",
    "file_path = '/home/ds4_sia_nolb/#FINAL_POLARIS/04_plus_preprocessing/preprocessing_final_data/re_final_preprocessing.json'\n",
    "output_dir = '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results'\n",
    "TFIDF_VECTORIZER_PATH = '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_idf_vectorizer_for_all_corpus.pkl'\n",
    "\n",
    "# 결과 저장 디렉토리 생성\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# 형태소 분석기\n",
    "# =========================\n",
    "okt = Okt()\n",
    "\n",
    "# =========================\n",
    "# 날짜 파서 (여러 포맷 허용)\n",
    "# =========================\n",
    "def parse_date_flexible(s: str):\n",
    "    if not s or not isinstance(s, str):\n",
    "        return None\n",
    "    s = s.strip()\n",
    "\n",
    "    candidates = [s]\n",
    "    if \"T\" in s:\n",
    "        candidates.append(s[:19])\n",
    "        candidates.append(s[:10])\n",
    "    if len(s) >= 10:\n",
    "        candidates.append(s[:10])\n",
    "    if \"-\" not in s and \".\" not in s and \"/\" not in s and len(s) == 8:\n",
    "        candidates.append(f\"{s[:4]}-{s[4:6]}-{s[6:8]}\")\n",
    "\n",
    "    fmts = [\n",
    "        \"%Y-%m-%d\",\n",
    "        \"%Y-%m-%d %H:%M:%S\",\n",
    "        \"%Y-%m-%d %H:%M\",\n",
    "        \"%Y/%m/%d\",\n",
    "        \"%Y/%m/%d %H:%M:%S\",\n",
    "        \"%Y.%m.%d\",\n",
    "        \"%Y.%m.%d %H:%M:%S\",\n",
    "        \"%Y.%m.%d %H:%M\",\n",
    "        \"%Y%m%d\",\n",
    "        \"%Y-%m-%dT%H:%M:%S\",\n",
    "    ]\n",
    "\n",
    "    for cand in candidates:\n",
    "        for fmt in fmts:\n",
    "            try:\n",
    "                return datetime.strptime(cand, fmt)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "def parse_date(date_str: str) -> datetime:\n",
    "    d = parse_date_flexible(date_str)\n",
    "    if d is None:\n",
    "        raise ValueError(f\"날짜 형식이 올바르지 않습니다: {date_str}\")\n",
    "    return d\n",
    "\n",
    "def extract_pubdate(article):\n",
    "    keys = [\"pubDate\", \"pubdate\", \"time\", \"date\", \"published\", \"pub_date\"]\n",
    "    for k in keys:\n",
    "        if k in article and article[k]:\n",
    "            dt = parse_date_flexible(str(article[k]))\n",
    "            if dt:\n",
    "                return dt\n",
    "    meta = article.get(\"metadata\", {}) or {}\n",
    "    for k in keys:\n",
    "        if k in meta and meta[k]:\n",
    "            dt = parse_date_flexible(str(meta[k]))\n",
    "            if dt:\n",
    "                return dt\n",
    "    return None\n",
    "\n",
    "# =========================\n",
    "# 기사 로드 및 필터링 유틸\n",
    "# =========================\n",
    "def load_all_articles(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(\"JSON 루트는 list 여야 합니다.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"파일을 찾을 수 없습니다: {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON 파싱 오류: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"알 수 없는 오류가 발생했습니다: {e}\")\n",
    "        return None\n",
    "\n",
    "def filter_articles_by_period(articles, start_date_str, end_date_str):\n",
    "    sdt = parse_date(start_date_str)\n",
    "    edt = parse_date(end_date_str)\n",
    "    \n",
    "    period_articles = []\n",
    "    \n",
    "    all_articles_count = len(articles)\n",
    "    \n",
    "    for article in tqdm(articles, desc=f\"기사 처리 중 ({sdt.date()}~{edt.date()})\", leave=False):\n",
    "        pub_date = extract_pubdate(article)\n",
    "        if pub_date and sdt <= pub_date <= edt:\n",
    "            period_articles.append(article)\n",
    "    \n",
    "    print(f\"총 {all_articles_count}개의 기사 중 지정 기간 내 기사 {len(period_articles)}개를 찾았습니다.\")\n",
    "    \n",
    "    return period_articles\n",
    "\n",
    "# =========================\n",
    "# 텍스트 정규화 (표기 통일 약간)\n",
    "# =========================\n",
    "def normalize_text(t: str) -> str:\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    t = t.replace(\"탄도 미사일\", \"탄도미사일\")\n",
    "    t = t.replace(\"순항 미사일\", \"순항미사일\")\n",
    "    t = t.replace(\"극초 음속\", \"극초음속\")\n",
    "    t = t.replace(\"초대형 방사포\", \"초대형방사포\")\n",
    "    return t\n",
    "\n",
    "# =========================\n",
    "# 불용어 & 뉴스 노이즈\n",
    "# =========================\n",
    "BASE_STOP = set([\n",
    "    '가','간','같은','같이','것','게다가','결국','곧','관하여','관련','관한','그','그것','그녀','그들',\n",
    "    '그리고','그때','그래','그래서','그러나','그러므로','그러한','그런','그렇게','그외','근거로','기타',\n",
    "    '까지도','까지','나','남들','너','누구','다','다가','다른','다만','다소','다수','다시','다음','단','단지',\n",
    "    '당신','대','대해서','더군다나','더구나','더라도','더욱이','도','도로','또','또는','또한','때','때문',\n",
    "    '라도','라면','라는','로','로부터','로써','를','마저','마치','만약','만일','만큼','모두','무엇','무슨',\n",
    "    '무척','물론','및','밖에','바로','보다','뿐이다','사람','사실은','상대적으로','생각','설령','소위','수',\n",
    "    '수준','쉽게','시대','시작하여','실로','실제','아니','아무','아무도','아무리','아마도','아울러','아직',\n",
    "    '앞에서','앞으로','어느','어떤','어떻게','어디','언제','얼마나','여기','여부','역시','예','오히려',\n",
    "    '와','왜','외에도','요','우리','우선','원래','위해서','으로','으로부터','으로써','을','의','의거하여',\n",
    "    '의지하여','의해','의해서','의하여','이','이것','이곳','이때','이라고','이러한','이런','이렇게','이제',\n",
    "    '이지만','이후','이상','이다','이전','인','일','일단','일반적으로','임시로','입장에서','자','자기','자신',\n",
    "    '잠시','저','저것','저기','저쪽','저희','전부','전혀','점에서','정도','제','조금','좀','주로','주제','즉',\n",
    "    '즉시','지금','진짜로','차라리','참','참으로','첫번째로','최고','최대','최소','최신','최초','통하여',\n",
    "    '통해서','평가','포함한','포함하여','하지만','하면서','하여','한','한때','한번','할','할것이다','할수있다',\n",
    "    '함께','해도', \n",
    "    # 아래 키워드는 idf_vectorizer_for_all_corpus.pkl파일 생성 이후 추가된 불용어임. BASE_STOP에 있으면 pkl파일로 인해 미적용 되기 때문에 ENTITY_NOISE에 추가하였음.\n",
    "    # '돼다', '서다', '대해', '나오다', '통해', '맞다', '대한', '위해', '기상청', '예보', '밝히다', '크다', '약간', '가다', '내리다', '받다', '기온'\n",
    "])\n",
    "\n",
    "NEWS_STOP = {\"기자\",\"연합뉴스\",\"사진\",\"속보\",\"종합\",\"자료\",\"영상\",\"단독\",\"전문\",\"인터뷰\",\"브리핑\"}\n",
    "\n",
    "# =========================\n",
    "# 엔터티 노이즈\n",
    "# =========================\n",
    "ENTITY_NOISE = {\n",
    "    \"북한\",\"한국\",\"대한민국\",\"남한\",\"미국\",\"중국\",\"일본\",\"러시아\",\"우크라이나\",\"유엔\",\"나토\",\"NATO\",\"EU\",\"유럽연합\",\n",
    "    \"푸틴\",\"블라디미르 푸틴\",\"바이든\",\"조 바이든\",\"시진핑\",\"김정은\",\"김여정\",\"문재인\",\"윤석열\",\"쇼이구\",\"젤렌스키\", \"중앙\", \"통신\", \"보도\", \n",
    "    # 아래 키워드는 BASE_STOP에 있어야하지만 잠시 옮겨옴.\n",
    "    '돼다', '서다', '대해', '나오다', '통해', '맞다', '대한', '위해', '기상청', '예보', '밝히다', '크다', '약간', '가다', '내리다', '받다', '기온',\n",
    "    '강수', '날씨', '소식통', '인용', '대체로', '이번', '들다', '들어', '올해', \n",
    "\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 토큰/텍스트\n",
    "# =========================\n",
    "def pos_tokens(text: str):\n",
    "    text = normalize_text(text or \"\")\n",
    "    return okt.pos(text, norm=True, stem=True)\n",
    "\n",
    "def doc_text(a) -> str:\n",
    "    # 수정: metadata의 title과 최상위 summary를 함께 사용\n",
    "    title = (a.get('metadata') or {}).get('title', '')\n",
    "    summary = a.get('summary', '')\n",
    "    return normalize_text(f\"{title} {summary}\")\n",
    "\n",
    "def tokenizer_for_vectorizer(s: str):\n",
    "    toks = []\n",
    "    for w, t in okt.pos(s, norm=True, stem=True):\n",
    "        if t not in (\"Noun\", \"Verb\"):\n",
    "            continue\n",
    "        if len(w) <= 1:\n",
    "            continue\n",
    "        if w in BASE_STOP or w in NEWS_STOP:\n",
    "            continue\n",
    "        if w.isdigit():\n",
    "            continue\n",
    "        toks.append(w)\n",
    "    return toks\n",
    "\n",
    "# =========================\n",
    "# 자동 학습: '행동 동사'와 '행위 명사'\n",
    "# =========================\n",
    "def learn_action_lexicons(articles, min_df_ratio_verbs=0.002, min_df_ratio_nouns=0.002):\n",
    "    verb_doc_df = Counter()\n",
    "    action_noun_df = Counter()\n",
    "    N_docs = len(articles)\n",
    "\n",
    "    for a in tqdm(articles, desc=\"행동 동사/행위명사 학습 중\", leave=False):\n",
    "        title = (a.get('metadata') or {}).get('title', '') # 수정\n",
    "        summary = a.get('summary','') or ''\n",
    "        p = pos_tokens(f\"{title} {summary}\")\n",
    "\n",
    "        verbs_in_doc = set()\n",
    "        action_nouns_in_doc = set()\n",
    "\n",
    "        for i, (w, t) in enumerate(p):\n",
    "            if t == \"Verb\":\n",
    "                verbs_in_doc.add(w)\n",
    "            if t == \"Noun\":\n",
    "                ahead = [p[j][0] for j in range(i+1, min(i+3, len(p)))]\n",
    "                if \"하다\" in ahead or \"되다\" in ahead:\n",
    "                    if w not in BASE_STOP and len(w) > 1:\n",
    "                        action_nouns_in_doc.add(w)\n",
    "\n",
    "        for v in verbs_in_doc:\n",
    "            verb_doc_df[v] += 1\n",
    "        for n in action_nouns_in_doc:\n",
    "            action_noun_df[n] += 1\n",
    "\n",
    "    min_df_verbs = max(5, int(N_docs * min_df_ratio_verbs))\n",
    "    min_df_nouns = max(5, int(N_docs * min_df_ratio_nouns))\n",
    "\n",
    "    drop_verbs = {\"하다\",\"되다\",\"이다\",\"있다\"}\n",
    "    verb_set = {v for v,df in verb_doc_df.items() if df >= min_df_verbs and v not in drop_verbs}\n",
    "    action_nouns = {n for n,df in action_noun_df.items() if df >= min_df_nouns}\n",
    "\n",
    "    print(f\"학습 결과: 행동동사 {len(verb_set)}개, 행위명사 {len(action_nouns)}개\")\n",
    "    return verb_set, action_nouns\n",
    "\n",
    "# =========================\n",
    "# 사건 구 후보 생성 + TF-IDF 결합 랭킹\n",
    "# =========================\n",
    "def nominalize_verb(v: str) -> str:\n",
    "    if v.endswith(\"하다\"):\n",
    "        return v[:-2]\n",
    "    if v.endswith(\"되다\"):\n",
    "        return v[:-2]\n",
    "    return v\n",
    "\n",
    "def extract_event_phrases_auto(articles, top_k=30, vectorizer=None):\n",
    "    N = len(articles)\n",
    "    if N == 0:\n",
    "        print(\"⚠ 지정 기간에 기사가 없습니다.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"지정 기간 내 기사 수: {N}개\")\n",
    "    \n",
    "    verb_set, action_nouns = learn_action_lexicons(articles)\n",
    "\n",
    "    phrase_df = Counter()\n",
    "    phrase_examples = defaultdict(list)\n",
    "\n",
    "    for a in tqdm(articles, desc=\"사건 구 자동 추출 중\", leave=False):\n",
    "        # 수정: metadata에서 title을 가져옴\n",
    "        title = (a.get('metadata') or {}).get('title', '')\n",
    "        summary = a.get('summary','') or ''\n",
    "        p = pos_tokens(f\"{title} {summary}\")\n",
    "        phrases_in_doc = set()\n",
    "\n",
    "        prev_nouns = []\n",
    "        L = len(p)\n",
    "        for i, (w, t) in enumerate(p):\n",
    "            if t == \"Noun\":\n",
    "                if w not in BASE_STOP and len(w) > 1:\n",
    "                    prev_nouns.append(w)\n",
    "                    if len(prev_nouns) > 5:\n",
    "                        prev_nouns = prev_nouns[-5:]\n",
    "\n",
    "            if t == \"Verb\" and w in verb_set:\n",
    "                vnom = nominalize_verb(w)\n",
    "                nn = [n for n in reversed(prev_nouns)][:2]\n",
    "                if nn:\n",
    "                    phrases_in_doc.add(f\"{nn[0]} {vnom}\".strip())\n",
    "                    if len(nn) >= 2:\n",
    "                        phrases_in_doc.add(f\"{nn[1]} {nn[0]} {vnom}\".strip())\n",
    "                else:\n",
    "                    phrases_in_doc.add(vnom.strip())\n",
    "\n",
    "            if t == \"Noun\" and w in action_nouns:\n",
    "                nn = [n for n in reversed(prev_nouns) if n != w][:2]\n",
    "                base = w\n",
    "                if nn:\n",
    "                    phrases_in_doc.add(f\"{nn[0]} {base}\".strip())\n",
    "                    if len(nn) >= 2:\n",
    "                        phrases_in_doc.add(f\"{nn[1]} {nn[0]} {base}\".strip())\n",
    "                else:\n",
    "                    phrases_in_doc.add(base.strip())\n",
    "\n",
    "                if i+1 < L and p[i+1][1] == \"Noun\" and p[i+1][0] in action_nouns:\n",
    "                    tail = p[i+1][0]\n",
    "                    if nn:\n",
    "                        phrases_in_doc.add(f\"{nn[0]} {base} {tail}\".strip())\n",
    "                        if len(nn) >= 2:\n",
    "                            phrases_in_doc.add(f\"{nn[1]} {nn[0]} {base} {tail}\".strip())\n",
    "                    else:\n",
    "                        phrases_in_doc.add(f\"{base} {tail}\".strip())\n",
    "\n",
    "        cleaned = set()\n",
    "        for ph in phrases_in_doc:\n",
    "            ph = re.sub(r\"\\s+\", \" \", ph).strip()\n",
    "            if len(ph.split()) == 1 and len(ph) <= 2:\n",
    "                continue\n",
    "            cleaned.add(ph)\n",
    "\n",
    "        for ph in cleaned:\n",
    "            phrase_df[ph] += 1\n",
    "            # 수정: title이 존재할 때만 examples에 추가\n",
    "            if len(phrase_examples[ph]) < 3 and title:\n",
    "                phrase_examples[ph].append(title)\n",
    "\n",
    "    if vectorizer is None:\n",
    "        print(\"[오류] TfidfVectorizer 객체가 전달되지 않았습니다.\")\n",
    "        return []\n",
    "\n",
    "    corpus_period = [doc_text(a) for a in articles]\n",
    "    Xp = vectorizer.transform(corpus_period)\n",
    "    tfidf_avg = np.asarray(Xp.mean(axis=0)).ravel()\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    tfidf_dict = {terms[i]: float(tfidf_avg[i]) for i in np.where(tfidf_avg > 0)[0]}\n",
    "\n",
    "    print(f\"TF-IDF 코퍼스: {len(corpus_period)}개 문서\")\n",
    "    print(f\"TF-IDF 용어수: {len(terms)}\")\n",
    "\n",
    "    def is_entity_only(ph: str) -> bool:\n",
    "        toks = ph.split()\n",
    "        if len(toks) <= 2 and any(ent in ph for ent in ENTITY_NOISE):\n",
    "            return True\n",
    "        ent_hits = sum(1 for t in toks if any(ent in t for ent in ENTITY_NOISE))\n",
    "        return (ent_hits >= max(1, len(toks) - 1))\n",
    "\n",
    "    def generic_penalty(ph: str) -> int:\n",
    "        generic = {\"대통령\",\"위원장\",\"정부\",\"당국\",\"관계자\",\"대변인\",\"회의\",\"논의\",\"강조\"}\n",
    "        return -sum(1 for t in ph.split() if t in generic)\n",
    "\n",
    "    def phrase_score(ph: str, df_cnt: int) -> float:\n",
    "        tfidf = tfidf_dict.get(ph, 0.0)\n",
    "        score = 0.6 * tfidf + 0.4 * float(df_cnt)\n",
    "\n",
    "        if is_entity_only(ph):\n",
    "            score -= 6.0\n",
    "        score += generic_penalty(ph)\n",
    "        if len(ph.split()) <= 2:\n",
    "            score -= 1.5\n",
    "        return score\n",
    "\n",
    "    scored = []\n",
    "    for ph, cnt in phrase_df.items():\n",
    "        if is_entity_only(ph):\n",
    "            continue\n",
    "        scored.append( (ph, cnt, phrase_score(ph, cnt)) )\n",
    "\n",
    "    scored.sort(key=lambda x: (x[2], x[1]), reverse=True)\n",
    "    ranked = [(ph, cnt, score, phrase_examples.get(ph, [])) for ph, cnt, score in scored[:top_k]]\n",
    "    return ranked\n",
    "\n",
    "# =========================\n",
    "# 전체 코퍼스용 TF-IDF 벡터라이저 사전 학습\n",
    "# =========================\n",
    "def pre_train_vectorizer(articles, save_path):\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"✔️ 기존 TF-IDF 벡터라이저 파일 '{save_path}'이 이미 존재합니다. 학습을 건너뜁니다.\")\n",
    "        return joblib.load(save_path)\n",
    "    \n",
    "    print(f\"🔍 전체 코퍼스용 TF-IDF 벡터라이저를 새로 학습합니다.\")\n",
    "    \n",
    "    full_corpus = [doc_text(a) for a in articles]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(\n",
    "        tokenizer=tokenizer_for_vectorizer,\n",
    "        ngram_range=(1, 3),\n",
    "        min_df=5,\n",
    "        max_df=0.85,\n",
    "        sublinear_tf=True,\n",
    "        norm='l2'\n",
    "    )\n",
    "    vectorizer.fit(full_corpus)\n",
    "    joblib.dump(vectorizer, save_path)\n",
    "    print(f\"✅ 전체 코퍼스 기반 TF-IDF 벡터라이저를 '{save_path}'에 저장했습니다.\")\n",
    "    return vectorizer\n",
    "\n",
    "# =========================\n",
    "# 월별 자동 처리 함수\n",
    "# =========================\n",
    "def process_monthly_keywords(year, all_articles, vectorizer):\n",
    "    \"\"\"지정된 연도의 모든 월에 대해 키워드를 추출하고 결과를 저장합니다.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for month in range(1, 13):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"📅 {year}년 {month}월 키워드 추출 중...\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        start_date = f\"{year}-{month:02d}-01\"\n",
    "        last_day = calendar.monthrange(year, month)[1]\n",
    "        end_date = f\"{year}-{month:02d}-{last_day:02d}\"\n",
    "        \n",
    "        monthly_articles = filter_articles_by_period(all_articles, start_date, end_date)\n",
    "        \n",
    "        if not monthly_articles:\n",
    "            print(f\"⚠️ {year}년 {month}월에 기사가 없습니다.\")\n",
    "            results[f\"{year}_{month:02d}\"] = []\n",
    "            continue\n",
    "        \n",
    "        events = extract_event_phrases_auto(\n",
    "            monthly_articles,\n",
    "            top_k=30,\n",
    "            vectorizer=vectorizer\n",
    "        )\n",
    "        \n",
    "        results[f\"{year}_{month:02d}\"] = events\n",
    "        \n",
    "        print(f\"\\n=== {year}년 {month}월 '사건 구' TOP 30 (TF-IDF + DF 결합) ===\")\n",
    "        if not events:\n",
    "            print(f\"{year}년 {month}월에 추출된 사건 구가 없습니다.\")\n",
    "        else:\n",
    "            for i, (ph, cnt, score, examples) in enumerate(events[:10], 1):\n",
    "                print(f\"{i}. {ph}   (점수={score:.2f}, 문서수={cnt})\")\n",
    "        \n",
    "        output_file = os.path.join(output_dir, f\"{year}_{month:02d}_keywords.json\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'year': year,\n",
    "                'month': month,\n",
    "                'period': f\"{start_date} ~ {end_date}\",\n",
    "                'total_articles': len(monthly_articles),\n",
    "                'keywords': [{'phrase': ph, 'doc_count': cnt, 'score': round(score, 2), 'examples': examples} \n",
    "                             for ph, cnt, score, examples in events]\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"💾 결과가 '{output_file}'에 저장되었습니다.\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# =========================\n",
    "# 실행부\n",
    "# =========================\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"📖 전체 기사 데이터를 로드하는 중...\")\n",
    "        all_articles = load_all_articles(file_path)\n",
    "        if not all_articles:\n",
    "            print(\"전체 코퍼스를 로드할 수 없습니다. 프로그램을 종료합니다.\")\n",
    "            exit()\n",
    "\n",
    "        vectorizer = pre_train_vectorizer(all_articles, TFIDF_VECTORIZER_PATH)\n",
    "        \n",
    "        year = int(input(\"분석할 연도를 입력하세요 (예: 2024): \").strip())\n",
    "        \n",
    "        print(f\"\\n🚀 {year}년 월별 키워드 추출을 시작합니다...\")\n",
    "        \n",
    "        monthly_results = process_monthly_keywords(year, all_articles, vectorizer)\n",
    "        \n",
    "        summary_file = os.path.join(output_dir, f\"{year}_keywords_by_month_all.json\")\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(monthly_results, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"\\n🎉 {year}년 월별 키워드 추출이 완료되었습니다!\")\n",
    "        print(f\"📁 결과 파일들이 '{output_dir}' 디렉토리에 저장되었습니다.\")\n",
    "        print(f\"📊 연간 종합 결과: '{summary_file}'\")\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"오류: {e}. 올바른 연도를 입력해주세요.\")\n",
    "    except Exception as e:\n",
    "        print(f\"예기치 않은 오류가 발생했습니다: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd798cb",
   "metadata": {},
   "source": [
    "# 키워드 클러스터\n",
    "\n",
    "- 월별 키워드 추출했을 때 의미가 중복되는 키워드가 다수 보임.\n",
    "- 해당 문제는 키워드별 언급된 기사 수(doc_count)와 그에 따른 순위 점수(score)가 분산되어 해당 키워드의 중요도가 낮아질 문제가 있음.\n",
    "- 그래서 비슷한 의미의 키워드를 병합하는 코드를 추가로 만듬.\n",
    "- 코드 하단에 연도, 연도별 특정 월 등 원하는 방법에 따라 약간의 변경 후 실행하면 추출되어 있던 키워드들을 병합해서 json 파일로 저장하게 됨.\n",
    "- similarity_threshold 에 따라서 키워드의 유사도를 결정할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c8e7ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 자동화된 키워드 그룹화 시스템 ===\n",
      "📝 변경 사항:\n",
      "- ❌ important_keywords 제거\n",
      "- ❌ keyword_bonus 제거\n",
      "- ✅ 자카드(70%) + 편집거리(30%) 유사도만 사용\n",
      "- 🚀 자동화된 배치 처리 기능 추가\n",
      "\n",
      "🚀 2025년 키워드 그룹화 배치 처리 시작\n",
      "📂 입력 디렉토리: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results\n",
      "📁 출력 디렉토리: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster\n",
      "📅 처리 월: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "⚙️ 옵션: {'similarity_threshold': 0.4, 'min_group_size': 2, 'max_groups': 12}\n",
      "\n",
      "============================================================\n",
      "처리 중: 2025년 1월\n",
      "입력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_01_keywords.json\n",
      "출력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_01_keyword_grouped.json\n",
      "============================================================\n",
      "=== 자동 키워드 그룹화 시작 (키워드 보너스 제거 버전) ===\n",
      "원본 키워드 수: 30\n",
      "유사도 임계값: 0.4\n",
      "최소 그룹 크기: 2\n",
      "유사도 계산: 자카드(70%) + 편집거리(30%)\n",
      "\n",
      "=== 그룹화 완료 ===\n",
      "최종 키워드 그룹 수: 12\n",
      "압축률: 60.0%\n",
      "\n",
      "=== 상위 그룹들 (순수 문자열 유사도 기반) ===\n",
      "1. 탄도미사일 발사 (점수: 60.3)\n",
      "   - 통합된 키워드 수: 9\n",
      "   - 평균 유사도: 0.483\n",
      "   - 통합 키워드: 탄도미사일 발사, 북한 탄도미사일 발사, 미사일 발사, 단거리 탄도미사일 발사, 탄도미사일 시험 발사, 거리 탄도미사일 시험 발사, 탄도미사일 동해 발사, 탄도미사일 시험, 올해 탄도미사일 발사\n",
      "\n",
      "2. 군인 생포 (점수: 18.1)\n",
      "   - 통합된 키워드 수: 3\n",
      "   - 평균 유사도: 0.568\n",
      "   - 통합 키워드: 북한 군인 생포, 군인 생포 밝히다, 군인 생포\n",
      "\n",
      "3. 시험 발사 (점수: 14.2)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.413\n",
      "   - 통합 키워드: 시험 발사, 동해 발사\n",
      "\n",
      "4. 영상 공개 (점수: 10.2)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.413\n",
      "   - 통합 키워드: 영상 공개, 사진 공개\n",
      "\n",
      "5. 전쟁 파병 (점수: 7.7)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.603\n",
      "   - 통합 키워드: 우크라이나 전쟁 파병, 전쟁 파병\n",
      "\n",
      "6. 초음속 미사일 (점수: 7.6)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.59\n",
      "   - 통합 키워드: 초음속 미사일 발사, 초음속 미사일 추정\n",
      "\n",
      "7. 일대 동해 발사 (점수: 5.6)\n",
      "   - 통합된 키워드 수: 1\n",
      "   - 평균 유사도: 1.0\n",
      "   - 통합 키워드: 일대 동해 발사\n",
      "\n",
      "8. 생포된 북한 포로 (점수: 5.2)\n",
      "   - 통합된 키워드 수: 1\n",
      "   - 평균 유사도: 1.0\n",
      "   - 통합 키워드: 생포된 북한 포로\n",
      "\n",
      "결과가 저장되었습니다: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_01_keyword_grouped.json\n",
      "✅ 성공적으로 처리되었습니다: 2025년 1월\n",
      "\n",
      "============================================================\n",
      "처리 중: 2025년 2월\n",
      "입력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_02_keywords.json\n",
      "출력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_02_keyword_grouped.json\n",
      "============================================================\n",
      "=== 자동 키워드 그룹화 시작 (키워드 보너스 제거 버전) ===\n",
      "원본 키워드 수: 30\n",
      "유사도 임계값: 0.4\n",
      "최소 그룹 크기: 2\n",
      "유사도 계산: 자카드(70%) + 편집거리(30%)\n",
      "\n",
      "=== 그룹화 완료 ===\n",
      "최종 키워드 그룹 수: 12\n",
      "압축률: 60.0%\n",
      "\n",
      "=== 상위 그룹들 (순수 문자열 유사도 기반) ===\n",
      "1. 포로 한국 송환 (점수: 6.0)\n",
      "   - 통합된 키워드 수: 3\n",
      "   - 평균 유사도: 0.43\n",
      "   - 통합 키워드: 포로 한국 송환, 포로 한국 요청, 북한 포로 송환\n",
      "\n",
      "2. 추가 파병 (점수: 4.9)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.633\n",
      "   - 통합 키워드: 러시아 추가 파병, 추가 파병\n",
      "\n",
      "3. 출범 처음 (점수: 4.1)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.633\n",
      "   - 통합 키워드: 행정부 출범 처음, 출범 처음\n",
      "\n",
      "4. 군인 생포 (점수: 4.0)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.417\n",
      "   - 통합 키워드: 북한 군인 생포, 군인 생포 밝히다\n",
      "\n",
      "5. 의지 확인 (점수: 3.7)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.633\n",
      "   - 통합 키워드: 의지 확인, 비핵화 의지 확인\n",
      "\n",
      "6. 한미 협력 (점수: 3.3)\n",
      "   - 통합된 키워드 수: 1\n",
      "   - 평균 유사도: 1.0\n",
      "   - 통합 키워드: 한미 협력\n",
      "\n",
      "7. 비서 준공 (점수: 3.2)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.417\n",
      "   - 통합 키워드: 비서 준공 사르다, 책임 비서 준공\n",
      "\n",
      "8. 센터 목적 열리다 (점수: 2.8)\n",
      "   - 통합된 키워드 수: 1\n",
      "   - 평균 유사도: 1.0\n",
      "   - 통합 키워드: 센터 목적 열리다\n",
      "\n",
      "결과가 저장되었습니다: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_02_keyword_grouped.json\n",
      "✅ 성공적으로 처리되었습니다: 2025년 2월\n",
      "\n",
      "============================================================\n",
      "처리 중: 2025년 3월\n",
      "입력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_03_keywords.json\n",
      "출력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_03_keyword_grouped.json\n",
      "============================================================\n",
      "=== 자동 키워드 그룹화 시작 (키워드 보너스 제거 버전) ===\n",
      "원본 키워드 수: 30\n",
      "유사도 임계값: 0.4\n",
      "최소 그룹 크기: 2\n",
      "유사도 계산: 자카드(70%) + 편집거리(30%)\n",
      "\n",
      "=== 그룹화 완료 ===\n",
      "최종 키워드 그룹 수: 12\n",
      "압축률: 60.0%\n",
      "\n",
      "=== 상위 그룹들 (순수 문자열 유사도 기반) ===\n",
      "1. 연합 훈련 (점수: 20.5)\n",
      "   - 통합된 키워드 수: 4\n",
      "   - 평균 유사도: 0.565\n",
      "   - 통합 키워드: 연합 훈련, 한미 연합 훈련, 연합 훈련 비난, 연합 훈련 반발\n",
      "\n",
      "2. 탄도미사일 발사 (점수: 5.7)\n",
      "   - 통합된 키워드 수: 3\n",
      "   - 평균 유사도: 0.619\n",
      "   - 통합 키워드: 북한 탄도미사일 발사, 탄도미사일 발사, 탄도미사일 수발 발사\n",
      "\n",
      "3. 북한 포로 송환 (점수: 5.7)\n",
      "   - 통합된 키워드 수: 3\n",
      "   - 평균 유사도: 0.506\n",
      "   - 통합 키워드: 북한 포로 송환, 북한 포로 면담, 포로 송환\n",
      "\n",
      "4. 수호 기념 (점수: 4.8)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.425\n",
      "   - 통합 키워드: 서해 수호 기념, 수호 기념 참석\n",
      "\n",
      "5. 현지 시간 (점수: 4.4)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.55\n",
      "   - 통합 키워드: 현지 시간 밝히다, 현지 시간 보도\n",
      "\n",
      "6. 추가 파병 (점수: 3.7)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.633\n",
      "   - 통합 키워드: 추가 파병, 러시아 추가 파병\n",
      "\n",
      "7. 귀순 의사 밝히다 (점수: 2.8)\n",
      "   - 통합된 키워드 수: 1\n",
      "   - 평균 유사도: 1.0\n",
      "   - 통합 키워드: 귀순 의사 밝히다\n",
      "\n",
      "8. 김정욱 국기 추다 (점수: 2.8)\n",
      "   - 통합된 키워드 수: 1\n",
      "   - 평균 유사도: 1.0\n",
      "   - 통합 키워드: 김정욱 국기 추다\n",
      "\n",
      "결과가 저장되었습니다: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_03_keyword_grouped.json\n",
      "✅ 성공적으로 처리되었습니다: 2025년 3월\n",
      "\n",
      "============================================================\n",
      "처리 중: 2025년 4월\n",
      "입력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_04_keywords.json\n",
      "출력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_04_keyword_grouped.json\n",
      "============================================================\n",
      "=== 자동 키워드 그룹화 시작 (키워드 보너스 제거 버전) ===\n",
      "원본 키워드 수: 30\n",
      "유사도 임계값: 0.4\n",
      "최소 그룹 크기: 2\n",
      "유사도 계산: 자카드(70%) + 편집거리(30%)\n",
      "\n",
      "=== 그룹화 완료 ===\n",
      "최종 키워드 그룹 수: 12\n",
      "압축률: 60.0%\n",
      "\n",
      "=== 상위 그룹들 (순수 문자열 유사도 기반) ===\n",
      "1. 파병 공식 (점수: 35.5)\n",
      "   - 통합된 키워드 수: 11\n",
      "   - 평균 유사도: 0.455\n",
      "   - 통합 키워드: 파병 공식, 파병 공식 확인, 파병 공식 인정, 북한 파병 공식, 러시아 파병 공식, 북한 파병 공식 인정, 파병 인정, 파병 사실 공식, 파병 처음 공식, 러시아 파병 공식 확인, 처음 공식\n",
      "\n",
      "2. 공식 인정 (점수: 13.4)\n",
      "   - 통합된 키워드 수: 3\n",
      "   - 평균 유사도: 0.452\n",
      "   - 통합 키워드: 공식 인정, 공식 확인, 처음 공식 인정\n",
      "\n",
      "3. 영상 공개 (점수: 7.7)\n",
      "   - 통합된 키워드 수: 3\n",
      "   - 평균 유사도: 0.603\n",
      "   - 통합 키워드: 영상 공개, 훈련 영상 공개, 영상 처음 공개\n",
      "\n",
      "4. 국무위원 공개 (점수: 4.9)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.677\n",
      "   - 통합 키워드: 북한 국무위원 공개, 국무위원 공개\n",
      "\n",
      "5. 조선소 진행 (점수: 4.8)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.45\n",
      "   - 통합 키워드: 남포 조선소 진행, 조선소 진행 돼다\n",
      "\n",
      "6. 적극 역할 (점수: 4.5)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.654\n",
      "   - 통합 키워드: 전투 적극 역할, 적극 역할\n",
      "\n",
      "7. 북한 파병 인정 (점수: 3.2)\n",
      "   - 통합된 키워드 수: 1\n",
      "   - 평균 유사도: 1.0\n",
      "   - 통합 키워드: 북한 파병 인정\n",
      "\n",
      "8. 우크라이나 전쟁 파병 (점수: 2.8)\n",
      "   - 통합된 키워드 수: 1\n",
      "   - 평균 유사도: 1.0\n",
      "   - 통합 키워드: 우크라이나 전쟁 파병\n",
      "\n",
      "결과가 저장되었습니다: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_04_keyword_grouped.json\n",
      "✅ 성공적으로 처리되었습니다: 2025년 4월\n",
      "\n",
      "============================================================\n",
      "처리 중: 2025년 5월\n",
      "입력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_05_keywords.json\n",
      "출력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_05_keyword_grouped.json\n",
      "============================================================\n",
      "=== 자동 키워드 그룹화 시작 (키워드 보너스 제거 버전) ===\n",
      "원본 키워드 수: 30\n",
      "유사도 임계값: 0.4\n",
      "최소 그룹 크기: 2\n",
      "유사도 계산: 자카드(70%) + 편집거리(30%)\n",
      "\n",
      "=== 그룹화 완료 ===\n",
      "최종 키워드 그룹 수: 12\n",
      "압축률: 60.0%\n",
      "\n",
      "=== 상위 그룹들 (순수 문자열 유사도 기반) ===\n",
      "1. 탄도미사일 발사 (점수: 14.2)\n",
      "   - 통합된 키워드 수: 6\n",
      "   - 평균 유사도: 0.514\n",
      "   - 통합 키워드: 탄도미사일 발사, 단거리 탄도미사일 발사, 동해 탄도미사일 발사, 북한 탄도미사일 발사, 미사일 발사, 탄도미사일 수발 발사\n",
      "\n",
      "2. 공원 지정 (점수: 4.1)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.654\n",
      "   - 통합 키워드: 지질 공원 지정, 공원 지정\n",
      "\n",
      "3. 평양 도착 (점수: 4.1)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.654\n",
      "   - 통합 키워드: 지난 평양 도착, 평양 도착\n",
      "\n",
      "4. 동해 발사 (점수: 4.1)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.654\n",
      "   - 통합 키워드: 동해 발사, 일대 동해 발사\n",
      "\n",
      "5. 현지 시간 보도 (점수: 3.2)\n",
      "   - 통합된 키워드 수: 1\n",
      "   - 평균 유사도: 1.0\n",
      "   - 통합 키워드: 현지 시간 보도\n",
      "\n",
      "6. 권고 판단 (점수: 3.2)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.417\n",
      "   - 통합 키워드: 등재 권고 판단, 권고 판단 내리다\n",
      "\n",
      "7. 모스크바 광장 열리다 (점수: 2.0)\n",
      "   - 통합된 키워드 수: 1\n",
      "   - 평균 유사도: 1.0\n",
      "   - 통합 키워드: 모스크바 광장 열리다\n",
      "\n",
      "8. 대북 제재 위반 (점수: 2.0)\n",
      "   - 통합된 키워드 수: 1\n",
      "   - 평균 유사도: 1.0\n",
      "   - 통합 키워드: 대북 제재 위반\n",
      "\n",
      "결과가 저장되었습니다: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_05_keyword_grouped.json\n",
      "✅ 성공적으로 처리되었습니다: 2025년 5월\n",
      "\n",
      "============================================================\n",
      "처리 중: 2025년 6월\n",
      "입력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_06_keywords.json\n",
      "출력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_06_keyword_grouped.json\n",
      "============================================================\n",
      "=== 자동 키워드 그룹화 시작 (키워드 보너스 제거 버전) ===\n",
      "원본 키워드 수: 30\n",
      "유사도 임계값: 0.4\n",
      "최소 그룹 크기: 2\n",
      "유사도 계산: 자카드(70%) + 편집거리(30%)\n",
      "\n",
      "=== 그룹화 완료 ===\n",
      "최종 키워드 그룹 수: 12\n",
      "압축률: 60.0%\n",
      "\n",
      "=== 상위 그룹들 (순수 문자열 유사도 기반) ===\n",
      "1. 소음 방송 (점수: 16.9)\n",
      "   - 통합된 키워드 수: 4\n",
      "   - 평균 유사도: 0.556\n",
      "   - 통합 키워드: 대남 소음 방송, 소음 방송, 소음 방송 중단, 소음 방송 멈추다\n",
      "\n",
      "2. 확성기 방송 (점수: 14.1)\n",
      "   - 통합된 키워드 수: 3\n",
      "   - 평균 유사도: 0.594\n",
      "   - 통합 키워드: 대북 확성기 방송, 확성기 방송, 확성기 방송 중단\n",
      "\n",
      "3. 도중 넘어지다 (점수: 6.5)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.658\n",
      "   - 통합 키워드: 진수식 도중 넘어지다, 도중 넘어지다\n",
      "\n",
      "4. 원산 갈다 (점수: 5.3)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.654\n",
      "   - 통합 키워드: 원산 갈다, 북한 원산 갈다\n",
      "\n",
      "5. 추가 파병 (점수: 4.9)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.633\n",
      "   - 통합 키워드: 추가 파병, 러시아 추가 파병\n",
      "\n",
      "6. 군사 건설 (점수: 4.9)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.654\n",
      "   - 통합 키워드: 군사 건설, 병력 군사 건설\n",
      "\n",
      "7. 방송 중단 (점수: 3.3)\n",
      "   - 통합된 키워드 수: 1\n",
      "   - 평균 유사도: 1.0\n",
      "   - 통합 키워드: 방송 중단\n",
      "\n",
      "8. 현지 시간 보도 (점수: 3.2)\n",
      "   - 통합된 키워드 수: 1\n",
      "   - 평균 유사도: 1.0\n",
      "   - 통합 키워드: 현지 시간 보도\n",
      "\n",
      "결과가 저장되었습니다: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_06_keyword_grouped.json\n",
      "✅ 성공적으로 처리되었습니다: 2025년 6월\n",
      "\n",
      "============================================================\n",
      "처리 중: 2025년 7월\n",
      "입력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_07_keywords.json\n",
      "출력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_07_keyword_grouped.json\n",
      "============================================================\n",
      "=== 자동 키워드 그룹화 시작 (키워드 보너스 제거 버전) ===\n",
      "원본 키워드 수: 30\n",
      "유사도 임계값: 0.4\n",
      "최소 그룹 크기: 2\n",
      "유사도 계산: 자카드(70%) + 편집거리(30%)\n",
      "\n",
      "=== 그룹화 완료 ===\n",
      "최종 키워드 그룹 수: 12\n",
      "압축률: 60.0%\n",
      "\n",
      "=== 상위 그룹들 (순수 문자열 유사도 기반) ===\n",
      "1. 신병 확보 (점수: 10.1)\n",
      "   - 통합된 키워드 수: 4\n",
      "   - 평균 유사도: 0.558\n",
      "   - 통합 키워드: 명의 신병 확보, 신병 확보, 신병 확보 밝히다, 실시 신병 확보\n",
      "\n",
      "2. 기관 조사 (점수: 6.1)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.654\n",
      "   - 통합 키워드: 관계 기관 조사, 기관 조사\n",
      "\n",
      "3. 군사분계선 넘어오다 (점수: 4.9)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.697\n",
      "   - 통합 키워드: 군사분계선 넘어오다, 전선 군사분계선 넘어오다\n",
      "\n",
      "4. 원산 갈다 (점수: 4.8)\n",
      "   - 통합된 키워드 수: 3\n",
      "   - 평균 유사도: 0.55\n",
      "   - 통합 키워드: 북한 원산 갈다, 강원도 원산 갈다, 리조트 원산 갈다\n",
      "\n",
      "5. 표류 구조 (점수: 3.7)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.654\n",
      "   - 통합 키워드: 해상 표류 구조, 표류 구조\n",
      "\n",
      "6. 평양 방어 (점수: 3.2)\n",
      "   - 통합된 키워드 수: 2\n",
      "   - 평균 유사도: 0.613\n",
      "   - 통합 키워드: 평양 방어 활용, 평양 방어 사용\n",
      "\n",
      "7. 주민 동해 송환 (점수: 2.0)\n",
      "   - 통합된 키워드 수: 1\n",
      "   - 평균 유사도: 1.0\n",
      "   - 통합 키워드: 주민 동해 송환\n",
      "\n",
      "8. 해당 인원 식별 (점수: 2.0)\n",
      "   - 통합된 키워드 수: 1\n",
      "   - 평균 유사도: 1.0\n",
      "   - 통합 키워드: 해당 인원 식별\n",
      "\n",
      "결과가 저장되었습니다: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_07_keyword_grouped.json\n",
      "✅ 성공적으로 처리되었습니다: 2025년 7월\n",
      "\n",
      "============================================================\n",
      "처리 중: 2025년 8월\n",
      "입력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_08_keywords.json\n",
      "출력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_08_keyword_grouped.json\n",
      "============================================================\n",
      "❌ 입력 파일이 존재하지 않습니다: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_08_keywords.json\n",
      "\n",
      "============================================================\n",
      "처리 중: 2025년 9월\n",
      "입력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_09_keywords.json\n",
      "출력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_09_keyword_grouped.json\n",
      "============================================================\n",
      "❌ 입력 파일이 존재하지 않습니다: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_09_keywords.json\n",
      "\n",
      "============================================================\n",
      "처리 중: 2025년 10월\n",
      "입력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_10_keywords.json\n",
      "출력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_10_keyword_grouped.json\n",
      "============================================================\n",
      "❌ 입력 파일이 존재하지 않습니다: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_10_keywords.json\n",
      "\n",
      "============================================================\n",
      "처리 중: 2025년 11월\n",
      "입력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_11_keywords.json\n",
      "출력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_11_keyword_grouped.json\n",
      "============================================================\n",
      "❌ 입력 파일이 존재하지 않습니다: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_11_keywords.json\n",
      "\n",
      "============================================================\n",
      "처리 중: 2025년 12월\n",
      "입력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_12_keywords.json\n",
      "출력 파일: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_12_keyword_grouped.json\n",
      "============================================================\n",
      "❌ 입력 파일이 존재하지 않습니다: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_12_keywords.json\n",
      "\n",
      "============================================================\n",
      "🎉 2025년 배치 처리 완료!\n",
      "✅ 성공: 7개 파일\n",
      "❌ 실패: 5개 파일\n",
      "📊 성공률: 58.3%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "class KeywordGrouper:\n",
    "    \"\"\"유사도 기반 키워드 자동 그룹화 클래스 - 키워드 보너스 제거 버전\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # important_keywords 제거 - 순수 문자열 유사도만 사용\n",
    "        pass\n",
    "    \n",
    "    def jaccard_similarity(self, str1: str, str2: str) -> float:\n",
    "        \"\"\"자카드 유사도 계산 (단어 집합 기반)\"\"\"\n",
    "        set1 = set(str1.split())\n",
    "        set2 = set(str2.split())\n",
    "        \n",
    "        intersection = set1.intersection(set2)\n",
    "        union = set1.union(set2)\n",
    "        \n",
    "        return len(intersection) / len(union) if len(union) > 0 else 0\n",
    "    \n",
    "    def levenshtein_distance(self, str1: str, str2: str) -> float:\n",
    "        \"\"\"편집 거리 기반 유사도 (레벤슈타인 거리)\"\"\"\n",
    "        if len(str1) == 0:\n",
    "            return len(str2)\n",
    "        if len(str2) == 0:\n",
    "            return len(str1)\n",
    "        \n",
    "        # 동적 프로그래밍 매트릭스 생성\n",
    "        matrix = [[0] * (len(str1) + 1) for _ in range(len(str2) + 1)]\n",
    "        \n",
    "        # 첫 번째 행과 열 초기화\n",
    "        for i in range(len(str2) + 1):\n",
    "            matrix[i][0] = i\n",
    "        for j in range(len(str1) + 1):\n",
    "            matrix[0][j] = j\n",
    "        \n",
    "        # 매트릭스 채우기\n",
    "        for i in range(1, len(str2) + 1):\n",
    "            for j in range(1, len(str1) + 1):\n",
    "                if str2[i-1] == str1[j-1]:\n",
    "                    matrix[i][j] = matrix[i-1][j-1]\n",
    "                else:\n",
    "                    matrix[i][j] = min(\n",
    "                        matrix[i-1][j-1] + 1,  # substitution\n",
    "                        matrix[i][j-1] + 1,    # insertion\n",
    "                        matrix[i-1][j] + 1     # deletion\n",
    "                    )\n",
    "        \n",
    "        max_len = max(len(str1), len(str2))\n",
    "        return 1 - (matrix[len(str2)][len(str1)] / max_len) if max_len > 0 else 1\n",
    "    \n",
    "    def calculate_similarity(self, phrase1: str, phrase2: str) -> float:\n",
    "        \"\"\"순수 문자열 유사도 계산 - 키워드 보너스 제거\"\"\"\n",
    "        # 자카드 유사도 (단어 겹침)\n",
    "        jaccard_score = self.jaccard_similarity(phrase1, phrase2)\n",
    "        \n",
    "        # 편집 거리 유사도 (문자열 유사성)\n",
    "        levenshtein_score = self.levenshtein_distance(phrase1, phrase2)\n",
    "        \n",
    "        # 가중 평균 (자카드 70%, 편집거리 30%)\n",
    "        return jaccard_score * 0.7 + levenshtein_score * 0.3\n",
    "    \n",
    "    def auto_group_keywords(self, keywords: List[Dict], similarity_threshold: float = 0.4, \n",
    "                           min_group_size: int = 2) -> List[Dict]:\n",
    "        \"\"\"클러스터링을 통한 자동 그룹화\"\"\"\n",
    "        groups = []\n",
    "        processed = set()\n",
    "        \n",
    "        for i in range(len(keywords)):\n",
    "            if i in processed:\n",
    "                continue\n",
    "            \n",
    "            current_group = [keywords[i]]\n",
    "            current_group_indices = [i]\n",
    "            processed.add(i)\n",
    "            \n",
    "            # 현재 키워드와 유사한 키워드들 찾기\n",
    "            for j in range(i + 1, len(keywords)):\n",
    "                if j in processed:\n",
    "                    continue\n",
    "                \n",
    "                similarity = self.calculate_similarity(\n",
    "                    keywords[i]['phrase'], \n",
    "                    keywords[j]['phrase']\n",
    "                )\n",
    "                \n",
    "                if similarity >= similarity_threshold:\n",
    "                    current_group.append(keywords[j])\n",
    "                    current_group_indices.append(j)\n",
    "                    processed.add(j)\n",
    "            \n",
    "            # 그룹 정보 저장\n",
    "            groups.append({\n",
    "                'keywords': current_group,\n",
    "                'indices': current_group_indices,\n",
    "                'avg_similarity': self.calculate_group_average_similarity(current_group)\n",
    "            })\n",
    "        \n",
    "        return groups\n",
    "    \n",
    "    def calculate_group_average_similarity(self, group_keywords: List[Dict]) -> float:\n",
    "        \"\"\"그룹 내 평균 유사도 계산\"\"\"\n",
    "        if len(group_keywords) < 2:\n",
    "            return 1.0\n",
    "        \n",
    "        total_similarity = 0\n",
    "        pair_count = 0\n",
    "        \n",
    "        for i in range(len(group_keywords)):\n",
    "            for j in range(i + 1, len(group_keywords)):\n",
    "                total_similarity += self.calculate_similarity(\n",
    "                    group_keywords[i]['phrase'],\n",
    "                    group_keywords[j]['phrase']\n",
    "                )\n",
    "                pair_count += 1\n",
    "        \n",
    "        return total_similarity / pair_count if pair_count > 0 else 1.0\n",
    "    \n",
    "    def select_representative_keyword(self, group_keywords: List[Dict]) -> Dict:\n",
    "        \"\"\"그룹 대표 키워드 선정 (가장 높은 점수)\"\"\"\n",
    "        return max(group_keywords, key=lambda k: k['score'])\n",
    "    \n",
    "    def find_common_words(self, phrases: List[str]) -> List[str]:\n",
    "        \"\"\"공통 단어 찾기\"\"\"\n",
    "        word_counts = Counter()\n",
    "        min_occurrence = math.ceil(len(phrases) * 0.6)  # 60% 이상 출현\n",
    "        \n",
    "        for phrase in phrases:\n",
    "            words = phrase.split()\n",
    "            for word in words:\n",
    "                word_counts[word] += 1\n",
    "        \n",
    "        common_words = [word for word, count in word_counts.items() \n",
    "                       if count >= min_occurrence]\n",
    "        \n",
    "        # 빈도순으로 정렬\n",
    "        return sorted(common_words, key=lambda w: word_counts[w], reverse=True)\n",
    "    \n",
    "    def generate_group_name(self, group_keywords: List[Dict]) -> str:\n",
    "        \"\"\"그룹명 생성 - 원래 단어 순서 유지\"\"\"\n",
    "        representative = self.select_representative_keyword(group_keywords)\n",
    "        phrases = [k['phrase'] for k in group_keywords]\n",
    "        \n",
    "        # 공통 단어 추출\n",
    "        common_words = self.find_common_words(phrases)\n",
    "        \n",
    "        if common_words and len(common_words) > 1:\n",
    "            # 대표 키워드에서 공통 단어들의 순서 찾기\n",
    "            rep_words = representative['phrase'].split()\n",
    "            ordered_common = []\n",
    "            \n",
    "            # 대표 키워드의 단어 순서대로 공통 단어 배열\n",
    "            for word in rep_words:\n",
    "                if word in common_words:\n",
    "                    ordered_common.append(word)\n",
    "            \n",
    "            # 빠진 공통 단어들 추가\n",
    "            for word in common_words:\n",
    "                if word not in ordered_common:\n",
    "                    ordered_common.append(word)\n",
    "            \n",
    "            if ordered_common:\n",
    "                return ' '.join(ordered_common)\n",
    "        \n",
    "        # 공통 단어가 없거나 1개면 대표 키워드 사용\n",
    "        return representative['phrase']\n",
    "    \n",
    "    def smart_group_keywords(self, keyword_data: Dict, **options) -> Dict:\n",
    "        \"\"\"메인 그룹화 함수\"\"\"\n",
    "        # 기본 옵션 설정\n",
    "        similarity_threshold = options.get('similarity_threshold', 0.4)\n",
    "        min_group_size = options.get('min_group_size', 2)\n",
    "        max_groups = options.get('max_groups', 15)\n",
    "        \n",
    "        print(\"=== 자동 키워드 그룹화 시작 (키워드 보너스 제거 버전) ===\")\n",
    "        print(f\"원본 키워드 수: {len(keyword_data['keywords'])}\")\n",
    "        print(f\"유사도 임계값: {similarity_threshold}\")\n",
    "        print(f\"최소 그룹 크기: {min_group_size}\")\n",
    "        print(f\"유사도 계산: 자카드(70%) + 편집거리(30%)\")\n",
    "        \n",
    "        # 자동 그룹화 수행\n",
    "        groups = self.auto_group_keywords(\n",
    "            keyword_data['keywords'], \n",
    "            similarity_threshold, \n",
    "            min_group_size\n",
    "        )\n",
    "        \n",
    "        # 그룹 정보를 최종 형태로 변환\n",
    "        grouped_keywords = []\n",
    "        \n",
    "        for group in groups:\n",
    "            representative = self.select_representative_keyword(group['keywords'])\n",
    "            group_name = self.generate_group_name(group['keywords'])\n",
    "            \n",
    "            # 예시 문장 중복 제거\n",
    "            all_examples = []\n",
    "            for keyword in group['keywords']:\n",
    "                all_examples.extend(keyword['examples'])\n",
    "            unique_examples = list(dict.fromkeys(all_examples))[:3]\n",
    "            \n",
    "            grouped_keyword = {\n",
    "                'phrase': group_name,\n",
    "                'doc_count': sum(k['doc_count'] for k in group['keywords']),\n",
    "                'score': round(sum(k['score'] for k in group['keywords']), 1),\n",
    "                'examples': unique_examples,\n",
    "                'merged_keywords': [k['phrase'] for k in group['keywords']],\n",
    "                'keyword_count': len(group['keywords']),\n",
    "                'avg_similarity': round(group['avg_similarity'], 3),\n",
    "                'representative_keyword': representative['phrase']\n",
    "            }\n",
    "            \n",
    "            grouped_keywords.append(grouped_keyword)\n",
    "        \n",
    "        # 점수순 정렬\n",
    "        grouped_keywords.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        # 최대 그룹 수 제한\n",
    "        final_keywords = grouped_keywords[:max_groups]\n",
    "        \n",
    "        # 결과 출력\n",
    "        print(f\"\\n=== 그룹화 완료 ===\")\n",
    "        print(f\"최종 키워드 그룹 수: {len(final_keywords)}\")\n",
    "        compression_rate = round((1 - len(final_keywords) / len(keyword_data['keywords'])) * 100, 1)\n",
    "        print(f\"압축률: {compression_rate}%\")\n",
    "        \n",
    "        # 상위 그룹들 출력\n",
    "        print(\"\\n=== 상위 그룹들 (순수 문자열 유사도 기반) ===\")\n",
    "        for i, group in enumerate(final_keywords[:8], 1):\n",
    "            print(f\"{i}. {group['phrase']} (점수: {group['score']})\")\n",
    "            print(f\"   - 통합된 키워드 수: {group['keyword_count']}\")\n",
    "            print(f\"   - 평균 유사도: {group['avg_similarity']}\")\n",
    "            print(f\"   - 통합 키워드: {', '.join(group['merged_keywords'])}\")\n",
    "            print()\n",
    "        \n",
    "        return {\n",
    "            'year': keyword_data['year'],\n",
    "            'month': keyword_data['month'],\n",
    "            'period': keyword_data['period'],\n",
    "            'total_articles': keyword_data['total_articles'],\n",
    "            'original_keyword_count': len(keyword_data['keywords']),\n",
    "            'grouped_keyword_count': len(final_keywords),\n",
    "            'compression_rate': compression_rate,\n",
    "            'similarity_threshold': similarity_threshold,\n",
    "            'similarity_method': 'jaccard(70%) + levenshtein(30%)',\n",
    "            'keywords': final_keywords\n",
    "        }\n",
    "    \n",
    "    def test_different_thresholds(self, keyword_data: Dict) -> None:\n",
    "        \"\"\"다양한 임계값으로 테스트\"\"\"\n",
    "        thresholds = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "        \n",
    "        print(\"=== 다양한 유사도 임계값 테스트 (키워드 보너스 제거 버전) ===\")\n",
    "        for threshold in thresholds:\n",
    "            result = self.smart_group_keywords(\n",
    "                keyword_data, \n",
    "                similarity_threshold=threshold,\n",
    "                min_group_size=2\n",
    "            )\n",
    "            \n",
    "            print(f\"임계값 {threshold}: {result['original_keyword_count']} → \"\n",
    "                  f\"{result['grouped_keyword_count']} ({result['compression_rate']}% 압축)\")\n",
    "        print()\n",
    "    \n",
    "    def load_json_file(self, file_path: str) -> Dict:\n",
    "        \"\"\"JSON 파일 로드\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"파일을 찾을 수 없습니다: {file_path}\")\n",
    "            return None\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"JSON 파싱 오류: {file_path}\")\n",
    "            return None\n",
    "    \n",
    "    def save_grouped_results(self, result: Dict, output_path: str) -> None:\n",
    "        \"\"\"그룹화 결과를 JSON 파일로 저장\"\"\"\n",
    "        try:\n",
    "            # 출력 디렉토리가 없으면 생성\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            \n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"결과가 저장되었습니다: {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"저장 중 오류 발생: {e}\")\n",
    "\n",
    "\n",
    "# 자동화된 배치 처리 함수들\n",
    "def process_single_file(grouper: KeywordGrouper, year: int, month: int, base_input_dir: str, base_output_dir: str, **options):\n",
    "    \"\"\"단일 파일 처리\"\"\"\n",
    "    # 입력 파일 경로 생성\n",
    "    input_file = os.path.join(base_input_dir, f\"{year}_{month:02d}_keywords.json\")\n",
    "    \n",
    "    # 출력 파일 경로 생성\n",
    "    output_file = os.path.join(base_output_dir, f\"{year}_{month:02d}_keyword_grouped.json\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"처리 중: {year}년 {month}월\")\n",
    "    print(f\"입력 파일: {input_file}\")\n",
    "    print(f\"출력 파일: {output_file}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # 파일 존재 여부 확인\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"❌ 입력 파일이 존재하지 않습니다: {input_file}\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # 파일 로드\n",
    "        data = grouper.load_json_file(input_file)\n",
    "        if data is None:\n",
    "            return False\n",
    "        \n",
    "        # 그룹화 수행\n",
    "        result = grouper.smart_group_keywords(data, **options)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grouper.save_grouped_results(result, output_file)\n",
    "        \n",
    "        print(f\"✅ 성공적으로 처리되었습니다: {year}년 {month}월\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 처리 중 오류 발생: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def process_year_batch(year: int, months: List[int] = None, **options):\n",
    "    \"\"\"연간 배치 처리\"\"\"\n",
    "    if months is None:\n",
    "        months = list(range(1, 13))  # 1월부터 12월까지\n",
    "    \n",
    "    # 기본 경로 설정\n",
    "    base_input_dir = f'/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results'\n",
    "    base_output_dir = f'/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster'\n",
    "    \n",
    "    # 사용자 정의 경로가 있으면 적용\n",
    "    base_input_dir = options.pop('input_dir', base_input_dir)\n",
    "    base_output_dir = options.pop('output_dir', base_output_dir)\n",
    "    \n",
    "    grouper = KeywordGrouper()\n",
    "    \n",
    "    print(f\"🚀 {year}년 키워드 그룹화 배치 처리 시작\")\n",
    "    print(f\"📂 입력 디렉토리: {base_input_dir}\")\n",
    "    print(f\"📁 출력 디렉토리: {base_output_dir}\")\n",
    "    print(f\"📅 처리 월: {months}\")\n",
    "    print(f\"⚙️ 옵션: {options}\")\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for month in months:\n",
    "        success = process_single_file(\n",
    "            grouper, year, month, base_input_dir, base_output_dir, **options\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    \n",
    "    # 최종 결과 출력\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🎉 {year}년 배치 처리 완료!\")\n",
    "    print(f\"✅ 성공: {successful}개 파일\")\n",
    "    print(f\"❌ 실패: {failed}개 파일\")\n",
    "    print(f\"📊 성공률: {successful/(successful+failed)*100:.1f}%\" if (successful+failed) > 0 else \"0%\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "\n",
    "def process_multiple_years(years: List[int], months: List[int] = None, **options):\n",
    "    \"\"\"다년도 배치 처리\"\"\"\n",
    "    print(f\"🎯 다년도 배치 처리 시작: {years}\")\n",
    "    \n",
    "    for year in years:\n",
    "        try:\n",
    "            process_year_batch(year, months, **options)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {year}년 처리 중 전체 오류 발생: {e}\")\n",
    "    \n",
    "    print(f\"🏁 모든 년도 처리 완료: {years}\")\n",
    "\n",
    "\n",
    "# 사용 예시 및 메인 실행부\n",
    "def main():\n",
    "    \"\"\"메인 실행 함수\"\"\"\n",
    "    grouper = KeywordGrouper()\n",
    "    \n",
    "    # 샘플 데이터 생성 (테스트용)\n",
    "    sample_data = {\n",
    "        \"year\": 2024,\n",
    "        \"month\": 1,\n",
    "        \"period\": \"2024-01-01 ~ 2024-01-31\",\n",
    "        \"total_articles\": 487,\n",
    "        \"keywords\": [\n",
    "            {\n",
    "                \"phrase\": \"시험 발사\",\n",
    "                \"doc_count\": 51,\n",
    "                \"score\": 18.9,\n",
    "                \"examples\": [\"예시1\", \"예시2\", \"예시3\"]\n",
    "            },\n",
    "            {\n",
    "                \"phrase\": \"탄도미사일 발사\",\n",
    "                \"doc_count\": 40,\n",
    "                \"score\": 14.5,\n",
    "                \"examples\": [\"예시4\", \"예시5\", \"예시6\"]\n",
    "            },\n",
    "            {\n",
    "                \"phrase\": \"순항미사일 발사\",\n",
    "                \"doc_count\": 28,\n",
    "                \"score\": 9.7,\n",
    "                \"examples\": [\"예시7\", \"예시8\", \"예시9\"]\n",
    "            },\n",
    "            {\n",
    "                \"phrase\": \"군사 협력\",\n",
    "                \"doc_count\": 27,\n",
    "                \"score\": 9.3,\n",
    "                \"examples\": [\"예시10\", \"예시11\", \"예시12\"]\n",
    "            },\n",
    "            {\n",
    "                \"phrase\": \"해상 사격\",\n",
    "                \"doc_count\": 25,\n",
    "                \"score\": 8.5,\n",
    "                \"examples\": [\"예시13\", \"예시14\", \"예시15\"]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"=== 자동화된 키워드 그룹화 시스템 ===\")\n",
    "    print(\"📝 변경 사항:\")\n",
    "    print(\"- ❌ important_keywords 제거\")\n",
    "    print(\"- ❌ keyword_bonus 제거\") \n",
    "    print(\"- ✅ 자카드(70%) + 편집거리(30%) 유사도만 사용\")\n",
    "    print(\"- 🚀 자동화된 배치 처리 기능 추가\")\n",
    "    print()\n",
    "    \n",
    "    # print(\"🔧 사용법:\")\n",
    "    # print(\"1. 단일 년도 처리:\")\n",
    "    # print(\"   process_year_batch(2024)\")\n",
    "    # print()\n",
    "    # print(\"2. 특정 월만 처리:\")\n",
    "    # print(\"   process_year_batch(2024, [1, 2, 3])\")\n",
    "    # print()\n",
    "    # print(\"3. 다년도 처리:\")\n",
    "    # print(\"   process_multiple_years([2023, 2024])\")\n",
    "    # print()\n",
    "    # print(\"4. 사용자 정의 옵션:\")\n",
    "    # print(\"   process_year_batch(2024,\")\n",
    "    # print(\"       similarity_threshold=0.3,\")\n",
    "    # print(\"       min_group_size=2,\")\n",
    "    # print(\"       max_groups=12,\")\n",
    "    # print(\"       input_dir='/custom/input/path',\")\n",
    "    # print(\"       output_dir='/custom/output/path')\")\n",
    "    # print()\n",
    "    \n",
    "    return grouper, sample_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    grouper, sample_data = main()\n",
    "    \n",
    "    # 🎯 실제 처리 시작 - 아래 중 하나를 선택하여 사용하세요\n",
    "    \n",
    "    # 방법 1: 2024년 전체 처리 (1월~12월)\n",
    "    process_year_batch(\n",
    "        year=2025,\n",
    "        similarity_threshold=0.4,\n",
    "        min_group_size=2,\n",
    "        max_groups=12\n",
    "    )\n",
    "    \n",
    "    # 방법 2: 2023년 전체 처리 (1월~12월) - 주석 해제하여 사용\n",
    "    # process_year_batch(\n",
    "    #     year=2023,\n",
    "    #     similarity_threshold=0.4,\n",
    "    #     min_group_size=2,\n",
    "    #     max_groups=12\n",
    "    # )\n",
    "    \n",
    "    # 방법 3: 2023년과 2024년 모두 처리 - 주석 해제하여 사용\n",
    "    # process_multiple_years(\n",
    "    #     years=[2023, 2024],\n",
    "    #     similarity_threshold=0.4,\n",
    "    #     min_group_size=2,\n",
    "    #     max_groups=12\n",
    "    # )\n",
    "    \n",
    "    # 방법 4: 특정 월만 처리 - 주석 해제하여 사용\n",
    "    # process_year_batch(\n",
    "    #     year=2024,\n",
    "    #     months=[10, 11, 12],  # 10월, 11월, 12월만 처리\n",
    "    #     similarity_threshold=0.4,\n",
    "    #     min_group_size=2,\n",
    "    #     max_groups=12\n",
    "    # )\n",
    "    \n",
    "    # 방법 5: 사용자 정의 경로로 처리 - 주석 해제하여 사용\n",
    "    # process_year_batch(\n",
    "    #     year=2024,\n",
    "    #     input_dir='/custom/input/directory',\n",
    "    #     output_dir='/custom/output/directory',\n",
    "    #     similarity_threshold=0.4,\n",
    "    #     min_group_size=2,\n",
    "    #     max_groups=12\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c60f061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ 기존 TF-IDF 벡터라이저 파일 '/home/ds4_sia_nolb/#FINAL_POLARIS/04_Event_top10/idf_vectorizer_for_all_corpus.pkl'이 이미 존재합니다. 학습을 건너뜁니다.\n",
      "\n",
      "✔️ 날짜가 입력되었습니다. 지정 기간 내 모든 기사를 기반으로 분석을 시작합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "기사 처리 중 (2024-01-01~2024-02-01): 100%|██████████| 80810/80810 [00:01<00:00, 80230.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 80810개의 기사 중 지정 기간 내 기사 510개를 찾았습니다.\n",
      "\n",
      "지정 기간 내 기사 수: 510개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "행동 동사/행위명사 학습 중: 100%|██████████| 510/510 [00:17<00:00, 29.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 결과: 행동동사 121개, 행위명사 144개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "사건 구 자동 추출 중: 100%|██████████| 510/510 [00:14<00:00, 35.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 코퍼스: 510개 문서\n",
      "TF-IDF 용어수: 340845\n",
      "\n",
      "=== 기간별 '사건 구' TOP 10 (TF-IDF + DF 결합) ===\n",
      "1. 시험 발사   (문서수=51)\n",
      "2. 탄도미사일 발사   (문서수=38)\n",
      "3. 군사 협력   (문서수=27)\n",
      "4. 순항미사일 발사   (문서수=25)\n",
      "5. 해상 사격   (문서수=24)\n",
      "6. 결의 위반   (문서수=23)\n",
      "7. 미사일 발사   (문서수=22)\n",
      "8. 우크라이나 공격 사용   (문서수=18)\n",
      "9. 안보리 결의 위반   (문서수=18)\n",
      "10. 발사 밝히다   (문서수=21)\n"
     ]
    }
   ],
   "source": [
    "# # 이벤트 키워드 추출하는 핵심 로직\n",
    "\n",
    "# import json\n",
    "# import os\n",
    "# import re\n",
    "# from tqdm import tqdm\n",
    "# from collections import Counter, defaultdict\n",
    "# from datetime import datetime\n",
    "# from konlpy.tag import Okt\n",
    "# import numpy as np\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import joblib\n",
    "\n",
    "# # =========================\n",
    "# # 설정\n",
    "# # =========================\n",
    "# file_path = '/home/ds4_sia_nolb/#FINAL_POLARIS/03_Bert_summarization/final_preprocessing_data/final_preprocessing.json'\n",
    "# output_file_path = '/home/ds4_sia_nolb/#FINAL_POLARIS/04_Event_top10/top10_file.json'\n",
    "# # TF-IDF 벡터라이저 불러오기 및 저장 경로 (전체 코퍼스 기반으로 변경)\n",
    "# TFIDF_VECTORIZER_PATH = '/home/ds4_sia_nolb/#FINAL_POLARIS/04_Event_top10/idf_vectorizer_for_all_corpus.pkl'\n",
    "\n",
    "# # =========================\n",
    "# # 형태소 분석기\n",
    "# # =========================\n",
    "# okt = Okt()\n",
    "\n",
    "# # =========================\n",
    "# # 날짜 파서 (여러 포맷 허용)\n",
    "# # =========================\n",
    "# def parse_date_flexible(s: str):\n",
    "#     if not s or not isinstance(s, str):\n",
    "#         return None\n",
    "#     s = s.strip()\n",
    "\n",
    "#     candidates = [s]\n",
    "#     if \"T\" in s:\n",
    "#         candidates.append(s[:19])\n",
    "#         candidates.append(s[:10])\n",
    "#     if len(s) >= 10:\n",
    "#         candidates.append(s[:10])\n",
    "#     if \"-\" not in s and \".\" not in s and \"/\" not in s and len(s) == 8:\n",
    "#         candidates.append(f\"{s[:4]}-{s[4:6]}-{s[6:8]}\")\n",
    "\n",
    "#     fmts = [\n",
    "#         \"%Y-%m-%d\",\n",
    "#         \"%Y-%m-%d %H:%M:%S\",\n",
    "#         \"%Y-%m-%d %H:%M\",\n",
    "#         \"%Y/%m/%d\",\n",
    "#         \"%Y/%m/%d %H:%M:%S\",\n",
    "#         \"%Y.%m.%d\",\n",
    "#         \"%Y.%m.%d %H:%M:%S\",\n",
    "#         \"%Y.%m.%d %H:%M\",\n",
    "#         \"%Y%m%d\",\n",
    "#         \"%Y-%m-%dT%H:%M:%S\",\n",
    "#     ]\n",
    "\n",
    "#     for cand in candidates:\n",
    "#         for fmt in fmts:\n",
    "#             try:\n",
    "#                 return datetime.strptime(cand, fmt)\n",
    "#             except Exception:\n",
    "#                 pass\n",
    "#     return None\n",
    "\n",
    "# def parse_date(date_str: str) -> datetime:\n",
    "#     d = parse_date_flexible(date_str)\n",
    "#     if d is None:\n",
    "#         raise ValueError(f\"날짜 형식이 올바르지 않습니다: {date_str}\")\n",
    "#     return d\n",
    "\n",
    "# def extract_pubdate(article):\n",
    "#     keys = [\"pubDate\", \"pubdate\", \"time\", \"date\", \"published\", \"pub_date\"]\n",
    "#     for k in keys:\n",
    "#         if k in article and article[k]:\n",
    "#             dt = parse_date_flexible(str(article[k]))\n",
    "#             if dt:\n",
    "#                 return dt\n",
    "#     meta = article.get(\"metadata\", {}) or {}\n",
    "#     for k in keys:\n",
    "#         if k in meta and meta[k]:\n",
    "#             dt = parse_date_flexible(str(meta[k]))\n",
    "#             if dt:\n",
    "#                 return dt\n",
    "#     return None\n",
    "\n",
    "# # =========================\n",
    "# # 기사 로드 및 필터링 유틸\n",
    "# # =========================\n",
    "# def load_all_articles(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#             data = json.load(f)\n",
    "#         if not isinstance(data, list):\n",
    "#             raise ValueError(\"JSON 루트는 list 여야 합니다.\")\n",
    "#         return data\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"파일을 찾을 수 없습니다: {file_path}\")\n",
    "#         return None\n",
    "#     except json.JSONDecodeError as e:\n",
    "#         print(f\"JSON 파싱 오류: {e}\")\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"알 수 없는 오류 발생: {e}\")\n",
    "#         return None\n",
    "\n",
    "# def filter_articles_by_period(articles, start_date_str, end_date_str):\n",
    "#     sdt = parse_date(start_date_str)\n",
    "#     edt = parse_date(end_date_str)\n",
    "    \n",
    "#     period_articles = []\n",
    "    \n",
    "#     all_articles_count = len(articles)\n",
    "    \n",
    "#     for article in tqdm(articles, desc=f\"기사 처리 중 ({sdt.date()}~{edt.date()})\"):\n",
    "#         pub_date = extract_pubdate(article)\n",
    "#         if pub_date and sdt <= pub_date <= edt:\n",
    "#             period_articles.append(article)\n",
    "    \n",
    "#     print(f\"총 {all_articles_count}개의 기사 중 지정 기간 내 기사 {len(period_articles)}개를 찾았습니다.\")\n",
    "    \n",
    "#     return period_articles\n",
    "\n",
    "# # =========================\n",
    "# # 텍스트 정규화 (표기 통일 약간)\n",
    "# # =========================\n",
    "# def normalize_text(t: str) -> str:\n",
    "#     if not t:\n",
    "#         return \"\"\n",
    "#     t = t.replace(\"탄도 미사일\", \"탄도미사일\")\n",
    "#     t = t.replace(\"순항 미사일\", \"순항미사일\")\n",
    "#     t = t.replace(\"극초 음속\", \"극초음속\")\n",
    "#     t = t.replace(\"초대형 방사포\", \"초대형방사포\")\n",
    "#     return t\n",
    "\n",
    "# # =========================\n",
    "# # 불용어 & 뉴스 노이즈\n",
    "# # =========================\n",
    "# BASE_STOP = set([\n",
    "#     '가','간','같은','같이','것','게다가','결국','곧','관하여','관련','관한','그','그것','그녀','그들',\n",
    "#     '그리고','그때','그래','그래서','그러나','그러므로','그러한','그런','그렇게','그외','근거로','기타',\n",
    "#     '까지도','까지','나','남들','너','누구','다','다가','다른','다만','다소','다수','다시','다음','단','단지',\n",
    "#     '당신','대','대해서','더군다나','더구나','더라도','더욱이','도','도로','또','또는','또한','때','때문',\n",
    "#     '라도','라면','라는','로','로부터','로써','를','마저','마치','만약','만일','만큼','모두','무엇','무슨',\n",
    "#     '무척','물론','및','밖에','바로','보다','뿐이다','사람','사실은','상대적으로','생각','설령','소위','수',\n",
    "#     '수준','쉽게','시대','시작하여','실로','실제','아니','아무','아무도','아무리','아마도','아울러','아직',\n",
    "#     '앞에서','앞으로','어느','어떤','어떻게','어디','언제','얼마나','여기','여부','역시','예','오히려',\n",
    "#     '와','왜','외에도','요','우리','우선','원래','위해서','으로','으로부터','으로써','을','의','의거하여',\n",
    "#     '의지하여','의해','의해서','의하여','이','이것','이곳','이때','이라고','이러한','이런','이렇게','이제',\n",
    "#     '이지만','이후','이상','이다','이전','인','일','일단','일반적으로','임시로','입장에서','자','자기','자신',\n",
    "#     '잠시','저','저것','저기','저쪽','저희','전부','전혀','점에서','정도','제','조금','좀','주로','주제','즉',\n",
    "#     '즉시','지금','진짜로','차라리','참','참으로','첫번째로','최고','최대','최소','최신','최초','통하여',\n",
    "#     '통해서','평가','포함한','포함하여','하지만','하면서','하여','한','한때','한번','할','할것이다','할수있다',\n",
    "#     '함께','해도', '돼다', '서다', '대해', '나오다', '통해', '맞다', \n",
    "# ])\n",
    "\n",
    "# NEWS_STOP = {\"기자\",\"연합뉴스\",\"사진\",\"속보\",\"종합\",\"자료\",\"영상\",\"단독\",\"전문\",\"인터뷰\",\"브리핑\"}\n",
    "\n",
    "# # =========================\n",
    "# # 엔터티 노이즈\n",
    "# # =========================\n",
    "# ENTITY_NOISE = {\n",
    "#     \"북한\",\"한국\",\"대한민국\",\"남한\",\"미국\",\"중국\",\"일본\",\"러시아\",\"우크라이나\",\"유엔\",\"나토\",\"NATO\",\"EU\",\"유럽연합\",\n",
    "#     \"푸틴\",\"블라디미르 푸틴\",\"바이든\",\"조 바이든\",\"시진핑\",\"김정은\",\"김여정\",\"문재인\",\"윤석열\",\"쇼이구\",\"젤렌스키\",\"통신\",\"중앙\",\"보도\"\n",
    "# }\n",
    "\n",
    "# # =========================\n",
    "# # 토큰/텍스트\n",
    "# # =========================\n",
    "# def pos_tokens(text: str):\n",
    "#     text = normalize_text(text or \"\")\n",
    "#     return okt.pos(text, norm=True, stem=True)\n",
    "\n",
    "# def doc_text(a) -> str:\n",
    "#     return normalize_text(f\"{a.get('title','')} {a.get('summary','')}\")\n",
    "\n",
    "# def tokenizer_for_vectorizer(s: str):\n",
    "#     toks = []\n",
    "#     for w, t in okt.pos(s, norm=True, stem=True):\n",
    "#         if t not in (\"Noun\", \"Verb\"):\n",
    "#             continue\n",
    "#         if len(w) <= 1:\n",
    "#             continue\n",
    "#         if w in BASE_STOP or w in NEWS_STOP:\n",
    "#             continue\n",
    "#         if w.isdigit():\n",
    "#             continue\n",
    "#         toks.append(w)\n",
    "#     return toks\n",
    "\n",
    "# # =========================\n",
    "# # 자동 학습: '행동 동사'와 '행위 명사'\n",
    "# # =========================\n",
    "# def learn_action_lexicons(articles, min_df_ratio_verbs=0.002, min_df_ratio_nouns=0.002):\n",
    "#     verb_doc_df = Counter()\n",
    "#     action_noun_df = Counter()\n",
    "#     N_docs = len(articles)\n",
    "\n",
    "#     for a in tqdm(articles, desc=\"행동 동사/행위명사 학습 중\"):\n",
    "#         title = a.get('title','') or ''\n",
    "#         summary = a.get('summary','') or ''\n",
    "#         p = pos_tokens(f\"{title} {summary}\")\n",
    "\n",
    "#         verbs_in_doc = set()\n",
    "#         action_nouns_in_doc = set()\n",
    "\n",
    "#         for i, (w, t) in enumerate(p):\n",
    "#             if t == \"Verb\":\n",
    "#                 verbs_in_doc.add(w)\n",
    "#             if t == \"Noun\":\n",
    "#                 ahead = [p[j][0] for j in range(i+1, min(i+3, len(p)))]\n",
    "#                 if \"하다\" in ahead or \"되다\" in ahead:\n",
    "#                     if w not in BASE_STOP and len(w) > 1:\n",
    "#                         action_nouns_in_doc.add(w)\n",
    "\n",
    "#         for v in verbs_in_doc:\n",
    "#             verb_doc_df[v] += 1\n",
    "#         for n in action_nouns_in_doc:\n",
    "#             action_noun_df[n] += 1\n",
    "\n",
    "#     min_df_verbs = max(5, int(N_docs * min_df_ratio_verbs))\n",
    "#     min_df_nouns = max(5, int(N_docs * min_df_ratio_nouns))\n",
    "\n",
    "#     drop_verbs = {\"하다\",\"되다\",\"이다\",\"있다\"}\n",
    "#     verb_set = {v for v,df in verb_doc_df.items() if df >= min_df_verbs and v not in drop_verbs}\n",
    "#     action_nouns = {n for n,df in action_noun_df.items() if df >= min_df_nouns}\n",
    "\n",
    "#     print(f\"학습 결과: 행동동사 {len(verb_set)}개, 행위명사 {len(action_nouns)}개\")\n",
    "#     return verb_set, action_nouns\n",
    "\n",
    "# def nominalize_verb(v: str) -> str:\n",
    "#     if v.endswith(\"하다\"):\n",
    "#         return v[:-2]\n",
    "#     if v.endswith(\"되다\"):\n",
    "#         return v[:-2]\n",
    "#     return v\n",
    "\n",
    "# # =========================\n",
    "# # 사건 구 후보 생성 + TF-IDF 결합 랭킹\n",
    "# # =========================\n",
    "# def extract_event_phrases_auto(articles, top_k=20, vectorizer=None):\n",
    "#     N = len(articles)\n",
    "#     if N == 0:\n",
    "#         print(\"⚠ 지정 기간에 기사가 없습니다.\")\n",
    "#         return []\n",
    "\n",
    "#     print(f\"\\n지정 기간 내 기사 수: {N}개\")\n",
    "    \n",
    "#     verb_set, action_nouns = learn_action_lexicons(articles)\n",
    "\n",
    "#     phrase_df = Counter()\n",
    "#     phrase_examples = defaultdict(list)\n",
    "\n",
    "#     for a in tqdm(articles, desc=\"사건 구 자동 추출 중\"):\n",
    "#         title = a.get('title','') or ''\n",
    "#         summary = a.get('summary','') or ''\n",
    "#         p = pos_tokens(f\"{title} {summary}\")\n",
    "#         phrases_in_doc = set()\n",
    "\n",
    "#         prev_nouns = []\n",
    "#         L = len(p)\n",
    "#         for i, (w, t) in enumerate(p):\n",
    "#             if t == \"Noun\":\n",
    "#                 if w not in BASE_STOP and len(w) > 1:\n",
    "#                     prev_nouns.append(w)\n",
    "#                     if len(prev_nouns) > 5:\n",
    "#                         prev_nouns = prev_nouns[-5:]\n",
    "\n",
    "#             if t == \"Verb\" and w in verb_set:\n",
    "#                 vnom = nominalize_verb(w)\n",
    "#                 nn = [n for n in reversed(prev_nouns)][:2]\n",
    "#                 if nn:\n",
    "#                     phrases_in_doc.add(f\"{nn[0]} {vnom}\".strip())\n",
    "#                     if len(nn) >= 2:\n",
    "#                         phrases_in_doc.add(f\"{nn[1]} {nn[0]} {vnom}\".strip())\n",
    "#                 else:\n",
    "#                     phrases_in_doc.add(vnom.strip())\n",
    "\n",
    "#             if t == \"Noun\" and w in action_nouns:\n",
    "#                 nn = [n for n in reversed(prev_nouns) if n != w][:2]\n",
    "#                 base = w\n",
    "#                 if nn:\n",
    "#                     phrases_in_doc.add(f\"{nn[0]} {base}\".strip())\n",
    "#                     if len(nn) >= 2:\n",
    "#                         phrases_in_doc.add(f\"{nn[1]} {nn[0]} {base}\".strip())\n",
    "#                 else:\n",
    "#                     phrases_in_doc.add(base.strip())\n",
    "\n",
    "#                 if i+1 < L and p[i+1][1] == \"Noun\" and p[i+1][0] in action_nouns:\n",
    "#                     tail = p[i+1][0]\n",
    "#                     if nn:\n",
    "#                         phrases_in_doc.add(f\"{nn[0]} {base} {tail}\".strip())\n",
    "#                         if len(nn) >= 2:\n",
    "#                             phrases_in_doc.add(f\"{nn[1]} {nn[0]} {base} {tail}\".strip())\n",
    "#                     else:\n",
    "#                         phrases_in_doc.add(f\"{base} {tail}\".strip())\n",
    "\n",
    "#         cleaned = set()\n",
    "#         for ph in phrases_in_doc:\n",
    "#             ph = re.sub(r\"\\s+\", \" \", ph).strip()\n",
    "#             if len(ph.split()) == 1 and len(ph) <= 2:\n",
    "#                 continue\n",
    "#             cleaned.add(ph)\n",
    "\n",
    "#         for ph in cleaned:\n",
    "#             phrase_df[ph] += 1\n",
    "#             if len(phrase_examples[ph]) < 3 and title:\n",
    "#                 phrase_examples[ph].append(title)\n",
    "\n",
    "#     if vectorizer is None:\n",
    "#         print(\"[오류] TfidfVectorizer 객체가 전달되지 않았습니다.\")\n",
    "#         return []\n",
    "\n",
    "#     corpus_period = [doc_text(a) for a in articles]\n",
    "#     Xp = vectorizer.transform(corpus_period)\n",
    "#     tfidf_avg = np.asarray(Xp.mean(axis=0)).ravel()\n",
    "#     terms = vectorizer.get_feature_names_out()\n",
    "#     tfidf_dict = {terms[i]: float(tfidf_avg[i]) for i in np.where(tfidf_avg > 0)[0]}\n",
    "\n",
    "#     print(f\"TF-IDF 코퍼스: {len(corpus_period)}개 문서\")\n",
    "#     print(f\"TF-IDF 용어수: {len(terms)}\")\n",
    "\n",
    "#     def is_entity_only(ph: str) -> bool:\n",
    "#         toks = ph.split()\n",
    "#         if len(toks) <= 2 and any(ent in ph for ent in ENTITY_NOISE):\n",
    "#             return True\n",
    "#         ent_hits = sum(1 for t in toks if any(ent in t for ent in ENTITY_NOISE))\n",
    "#         return (ent_hits >= max(1, len(toks) - 1))\n",
    "\n",
    "#     def generic_penalty(ph: str) -> int:\n",
    "#         generic = {\"대통령\",\"위원장\",\"정부\",\"당국\",\"관계자\",\"대변인\",\"회의\",\"논의\",\"강조\"}\n",
    "#         return -sum(1 for t in ph.split() if t in generic)\n",
    "\n",
    "#     def phrase_score(ph: str, df_cnt: int) -> float:\n",
    "#         tfidf = tfidf_dict.get(ph, 0.0)\n",
    "#         score = 0.6 * tfidf + 0.4 * float(df_cnt)\n",
    "\n",
    "#         if is_entity_only(ph):\n",
    "#             score -= 6.0\n",
    "#         score += generic_penalty(ph)\n",
    "#         if len(ph.split()) <= 2:\n",
    "#             score -= 1.5\n",
    "#         return score\n",
    "\n",
    "#     scored = []\n",
    "#     for ph, cnt in phrase_df.items():\n",
    "#         if is_entity_only(ph):\n",
    "#             continue\n",
    "#         scored.append( (ph, cnt, phrase_score(ph, cnt)) )\n",
    "\n",
    "#     scored.sort(key=lambda x: (x[2], x[1]), reverse=True)\n",
    "#     ranked = [(ph, cnt, phrase_examples.get(ph, [])) for ph, cnt, _ in scored[:top_k]]\n",
    "#     return ranked\n",
    "\n",
    "# # =========================\n",
    "# # 전체 코퍼스용 TF-IDF 벡터라이저 사전 학습\n",
    "# # =========================\n",
    "# def pre_train_vectorizer(articles, save_path):\n",
    "#     if os.path.exists(save_path):\n",
    "#         print(f\"✔️ 기존 TF-IDF 벡터라이저 파일 '{save_path}'이 이미 존재합니다. 학습을 건너뜁니다.\")\n",
    "#         return joblib.load(save_path)\n",
    "    \n",
    "#     print(f\"🔍 전체 코퍼스용 TF-IDF 벡터라이저를 새로 학습합니다.\")\n",
    "    \n",
    "#     full_corpus = [doc_text(a) for a in articles]\n",
    "    \n",
    "#     vectorizer = TfidfVectorizer(\n",
    "#         tokenizer=tokenizer_for_vectorizer,\n",
    "#         ngram_range=(1, 3),\n",
    "#         min_df=5,\n",
    "#         max_df=0.85,\n",
    "#         sublinear_tf=True,\n",
    "#         norm='l2'\n",
    "#     )\n",
    "#     vectorizer.fit(full_corpus)\n",
    "#     joblib.dump(vectorizer, save_path)\n",
    "#     print(f\"✅ 전체 코퍼스 기반 TF-IDF 벡터라이저를 '{save_path}'에 저장했습니다.\")\n",
    "#     return vectorizer\n",
    "\n",
    "# # =========================\n",
    "# # 실행부\n",
    "# # =========================\n",
    "# if __name__ == '__main__':\n",
    "#     try:\n",
    "#         # 1. 전체 기사 데이터를 한 번만 로드합니다.\n",
    "#         all_articles = load_all_articles(file_path)\n",
    "#         if not all_articles:\n",
    "#             print(\"전체 코퍼스를 로드할 수 없습니다. 프로그램을 종료합니다.\")\n",
    "#             exit()\n",
    "\n",
    "#         # 2. 전체 코퍼스 데이터를 이용해 TF-IDF 벡터라이저를 학습/로드합니다.\n",
    "#         vectorizer = pre_train_vectorizer(all_articles, TFIDF_VECTORIZER_PATH)\n",
    "        \n",
    "#         # 3. 날짜를 사용자에게 입력받습니다.\n",
    "#         start = input(\"시작일 입력 (YYYY-MM-DD 또는 YYYYMMDD): \").strip()\n",
    "#         end = input(\"종료일 입력 (YYYY-MM-DD 또는 YYYYMMDD): \").strip()\n",
    "\n",
    "#         print(\"\\n✔️ 날짜가 입력되었습니다. 지정 기간 내 모든 기사를 기반으로 분석을 시작합니다.\")\n",
    "        \n",
    "#         # 4. 로드된 전체 데이터에서 지정 기간 기사만 필터링합니다.\n",
    "#         period_articles = filter_articles_by_period(all_articles, start, end)\n",
    "        \n",
    "#         if not period_articles:\n",
    "#             print(\"분석할 기사가 없습니다. 프로그램을 종료합니다.\")\n",
    "#         else:\n",
    "#             # 5. 사건구를 추출하고 랭킹을 매깁니다.\n",
    "#             events = extract_event_phrases_auto(\n",
    "#                 period_articles,\n",
    "#                 top_k=10,\n",
    "#                 vectorizer=vectorizer\n",
    "#             )\n",
    "\n",
    "#             print(\"\\n=== 기간별 '사건 구' TOP 10 (TF-IDF + DF 결합) ===\")\n",
    "#             if not events:\n",
    "#                 print(\"해당 기간에 추출된 사건 구가 없습니다.\")\n",
    "#             else:\n",
    "#                 for i, (ph, cnt, examples) in enumerate(events, 1):\n",
    "#                     ex_str = \" | 예시: \" + \" / \".join(examples) if examples else \"\"\n",
    "#                     print(f\"{i}. {ph}   (문서수={cnt}){ex_str}\")\n",
    "    \n",
    "#     except ValueError as e:\n",
    "#         print(f\"오류: {e}. 올바른 날짜 형식을 입력해주세요. 프로그램을 종료합니다.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"예기치 않은 오류가 발생했습니다: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
