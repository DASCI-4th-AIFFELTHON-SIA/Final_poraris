{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e09688cd",
   "metadata": {},
   "source": [
    "# í•µì‹¬ì´ìŠˆ í‚¤ì›Œë“œ ì¶”ì¶œ í›„ ì›”ë³„ ê·¸ë£¹í™” íŒŒì¼ ìë™ ì¶”ì¶œ\n",
    "\n",
    "- í•µì‹¬ì´ìŠˆ í‚¤ì›Œë“œë¥¼ TF-IDFë¡œ ì¶”ì¶œ\n",
    "- ì¶”ì¶œí•œ ë°ì´í„°ì˜ ì—°ë„ë¥¼ ì…ë ¥í•˜ë©´ í•´ë‹¹ ë…„ë„ì˜ `ì´ìŠˆ í‚¤ì›Œë“œ`ë¥¼ 1ì›”ë¶€í„° 12ì›”ê¹Œì§€ `ì›”ë³„`ë¡œ ì¶”ì¶œì„ ì‹œì‘í•¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3636ffa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– ì „ì²´ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...\n",
      "âœ”ï¸ ê¸°ì¡´ TF-IDF ë²¡í„°ë¼ì´ì € íŒŒì¼ '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_idf_vectorizer_for_all_corpus.pkl'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.\n",
      "\n",
      "ğŸš€ 2025ë…„ ì›”ë³„ í‚¤ì›Œë“œ ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "\n",
      "==================================================\n",
      "ğŸ“… 2025ë…„ 1ì›” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 80434ê°œì˜ ê¸°ì‚¬ ì¤‘ ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ 385ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ ìˆ˜: 385ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ê²°ê³¼: í–‰ë™ë™ì‚¬ 109ê°œ, í–‰ìœ„ëª…ì‚¬ 99ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF ì½”í¼ìŠ¤: 385ê°œ ë¬¸ì„œ\n",
      "TF-IDF ìš©ì–´ìˆ˜: 359320\n",
      "\n",
      "=== 2025ë…„ 1ì›” 'ì‚¬ê±´ êµ¬' TOP 30 (TF-IDF + DF ê²°í•©) ===\n",
      "1. íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬ Â  (ì ìˆ˜=16.11, ë¬¸ì„œìˆ˜=44)\n",
      "2. ë¶í•œ íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬ Â  (ì ìˆ˜=10.80, ë¬¸ì„œìˆ˜=27)\n",
      "3. ì‹œí—˜ ë°œì‚¬ Â  (ì ìˆ˜=7.70, ë¬¸ì„œìˆ˜=23)\n",
      "4. ë¶í•œ êµ°ì¸ ìƒí¬ Â  (ì ìˆ˜=6.80, ë¬¸ì„œìˆ˜=17)\n",
      "5. ë™í•´ ë°œì‚¬ Â  (ì ìˆ˜=6.50, ë¬¸ì„œìˆ˜=20)\n",
      "6. ë¯¸ì‚¬ì¼ ë°œì‚¬ Â  (ì ìˆ˜=6.50, ë¬¸ì„œìˆ˜=20)\n",
      "7. ì˜ìƒ ê³µê°œ Â  (ì ìˆ˜=6.50, ë¬¸ì„œìˆ˜=20)\n",
      "8. êµ°ì¸ ìƒí¬ ë°íˆë‹¤ Â  (ì ìˆ˜=6.00, ë¬¸ì„œìˆ˜=15)\n",
      "9. ì¼ëŒ€ ë™í•´ ë°œì‚¬ Â  (ì ìˆ˜=5.60, ë¬¸ì„œìˆ˜=14)\n",
      "10. ë‹¨ê±°ë¦¬ íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬ Â  (ì ìˆ˜=5.60, ë¬¸ì„œìˆ˜=14)\n",
      "ğŸ’¾ ê²°ê³¼ê°€ '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_01_keywords.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "==================================================\n",
      "ğŸ“… 2025ë…„ 2ì›” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 80434ê°œì˜ ê¸°ì‚¬ ì¤‘ ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ 298ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ ìˆ˜: 298ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ê²°ê³¼: í–‰ë™ë™ì‚¬ 86ê°œ, í–‰ìœ„ëª…ì‚¬ 91ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF ì½”í¼ìŠ¤: 298ê°œ ë¬¸ì„œ\n",
      "TF-IDF ìš©ì–´ìˆ˜: 359320\n",
      "\n",
      "=== 2025ë…„ 2ì›” 'ì‚¬ê±´ êµ¬' TOP 30 (TF-IDF + DF ê²°í•©) ===\n",
      "1. í•œë¯¸ í˜‘ë ¥ Â  (ì ìˆ˜=3.30, ë¬¸ì„œìˆ˜=12)\n",
      "2. ëŸ¬ì‹œì•„ ì¶”ê°€ íŒŒë³‘ Â  (ì ìˆ˜=2.80, ë¬¸ì„œìˆ˜=7)\n",
      "3. ì„¼í„° ëª©ì  ì—´ë¦¬ë‹¤ Â  (ì ìˆ˜=2.80, ë¬¸ì„œìˆ˜=7)\n",
      "4. ê³µì—… ê³µì¥ ì¤€ê³µ Â  (ì ìˆ˜=2.80, ë¬¸ì„œìˆ˜=7)\n",
      "5. í˜„ì§€ ì‹œê°„ ë°íˆë‹¤ Â  (ì ìˆ˜=2.80, ë¬¸ì„œìˆ˜=7)\n",
      "6. í¬ë¡œ í•œêµ­ ì†¡í™˜ Â  (ì ìˆ˜=2.40, ë¬¸ì„œìˆ˜=6)\n",
      "7. ê¹€ì¼ ì²´ìœ¡ ë‹¨ì¥ Â  (ì ìˆ˜=2.40, ë¬¸ì„œìˆ˜=6)\n",
      "8. í–‰ì •ë¶€ ì¶œë²” ì²˜ìŒ Â  (ì ìˆ˜=2.40, ë¬¸ì„œìˆ˜=6)\n",
      "9. ì¶”ê°€ íŒŒë³‘ Â  (ì ìˆ˜=2.10, ë¬¸ì„œìˆ˜=9)\n",
      "10. í˜‘ë ¥ í™•ëŒ€ Â  (ì ìˆ˜=2.10, ë¬¸ì„œìˆ˜=9)\n",
      "ğŸ’¾ ê²°ê³¼ê°€ '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_02_keywords.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "==================================================\n",
      "ğŸ“… 2025ë…„ 3ì›” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 80434ê°œì˜ ê¸°ì‚¬ ì¤‘ ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ 297ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ ìˆ˜: 297ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ê²°ê³¼: í–‰ë™ë™ì‚¬ 83ê°œ, í–‰ìœ„ëª…ì‚¬ 80ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF ì½”í¼ìŠ¤: 297ê°œ ë¬¸ì„œ\n",
      "TF-IDF ìš©ì–´ìˆ˜: 359320\n",
      "\n",
      "=== 2025ë…„ 3ì›” 'ì‚¬ê±´ êµ¬' TOP 30 (TF-IDF + DF ê²°í•©) ===\n",
      "1. ì—°í•© í›ˆë ¨ Â  (ì ìˆ˜=8.50, ë¬¸ì„œìˆ˜=25)\n",
      "2. í•œë¯¸ ì—°í•© í›ˆë ¨ Â  (ì ìˆ˜=8.00, ë¬¸ì„œìˆ˜=20)\n",
      "3. ì„œí•´ ìˆ˜í˜¸ ê¸°ë… Â  (ì ìˆ˜=2.80, ë¬¸ì„œìˆ˜=7)\n",
      "4. ê·€ìˆœ ì˜ì‚¬ ë°íˆë‹¤ Â  (ì ìˆ˜=2.80, ë¬¸ì„œìˆ˜=7)\n",
      "5. ê¹€ì •ìš± êµ­ê¸° ì¶”ë‹¤ Â  (ì ìˆ˜=2.80, ë¬¸ì„œìˆ˜=7)\n",
      "6. ì—°í•© í›ˆë ¨ ë¹„ë‚œ Â  (ì ìˆ˜=2.40, ë¬¸ì„œìˆ˜=6)\n",
      "7. ë¶í•œ íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬ Â  (ì ìˆ˜=2.40, ë¬¸ì„œìˆ˜=6)\n",
      "8. í˜„ì§€ ì‹œê°„ ë°íˆë‹¤ Â  (ì ìˆ˜=2.40, ë¬¸ì„œìˆ˜=6)\n",
      "9. í¬ë¡œ í•œêµ­ ì‹¶ë‹¤ Â  (ì ìˆ˜=2.40, ë¬¸ì„œìˆ˜=6)\n",
      "10. ì¶”ê°€ íŒŒë³‘ Â  (ì ìˆ˜=2.10, ë¬¸ì„œìˆ˜=9)\n",
      "ğŸ’¾ ê²°ê³¼ê°€ '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_03_keywords.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "==================================================\n",
      "ğŸ“… 2025ë…„ 4ì›” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 80434ê°œì˜ ê¸°ì‚¬ ì¤‘ ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ 287ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ ìˆ˜: 287ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ê²°ê³¼: í–‰ë™ë™ì‚¬ 63ê°œ, í–‰ìœ„ëª…ì‚¬ 73ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF ì½”í¼ìŠ¤: 287ê°œ ë¬¸ì„œ\n",
      "TF-IDF ìš©ì–´ìˆ˜: 359320\n",
      "\n",
      "=== 2025ë…„ 4ì›” 'ì‚¬ê±´ êµ¬' TOP 30 (TF-IDF + DF ê²°í•©) ===\n",
      "1. íŒŒë³‘ ê³µì‹ Â  (ì ìˆ˜=7.71, ë¬¸ì„œìˆ˜=23)\n",
      "2. ê³µì‹ ì¸ì • Â  (ì ìˆ˜=6.90, ë¬¸ì„œìˆ˜=21)\n",
      "3. ê³µì‹ í™•ì¸ Â  (ì ìˆ˜=4.50, ë¬¸ì„œìˆ˜=15)\n",
      "4. íŒŒë³‘ ê³µì‹ í™•ì¸ Â  (ì ìˆ˜=4.40, ë¬¸ì„œìˆ˜=11)\n",
      "5. íŒŒë³‘ ê³µì‹ ì¸ì • Â  (ì ìˆ˜=4.00, ë¬¸ì„œìˆ˜=10)\n",
      "6. ë¶í•œ íŒŒë³‘ ê³µì‹ Â  (ì ìˆ˜=4.00, ë¬¸ì„œìˆ˜=10)\n",
      "7. ì˜ìƒ ê³µê°œ Â  (ì ìˆ˜=3.30, ë¬¸ì„œìˆ˜=12)\n",
      "8. ëŸ¬ì‹œì•„ íŒŒë³‘ ê³µì‹ Â  (ì ìˆ˜=3.20, ë¬¸ì„œìˆ˜=8)\n",
      "9. ë¶í•œ íŒŒë³‘ ì¸ì • Â  (ì ìˆ˜=3.20, ë¬¸ì„œìˆ˜=8)\n",
      "10. ë¶í•œ êµ­ë¬´ìœ„ì› ê³µê°œ Â  (ì ìˆ˜=3.20, ë¬¸ì„œìˆ˜=8)\n",
      "ğŸ’¾ ê²°ê³¼ê°€ '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_04_keywords.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "==================================================\n",
      "ğŸ“… 2025ë…„ 5ì›” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 80434ê°œì˜ ê¸°ì‚¬ ì¤‘ ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ 226ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ ìˆ˜: 226ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ê²°ê³¼: í–‰ë™ë™ì‚¬ 54ê°œ, í–‰ìœ„ëª…ì‚¬ 69ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF ì½”í¼ìŠ¤: 226ê°œ ë¬¸ì„œ\n",
      "TF-IDF ìš©ì–´ìˆ˜: 359320\n",
      "\n",
      "=== 2025ë…„ 5ì›” 'ì‚¬ê±´ êµ¬' TOP 30 (TF-IDF + DF ê²°í•©) ===\n",
      "1. íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬ Â  (ì ìˆ˜=4.10, ë¬¸ì„œìˆ˜=14)\n",
      "2. í˜„ì§€ ì‹œê°„ ë³´ë„ Â  (ì ìˆ˜=3.20, ë¬¸ì„œìˆ˜=8)\n",
      "3. ì§€ì§ˆ ê³µì› ì§€ì • Â  (ì ìˆ˜=2.80, ë¬¸ì„œìˆ˜=7)\n",
      "4. ë‹¨ê±°ë¦¬ íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬ Â  (ì ìˆ˜=2.80, ë¬¸ì„œìˆ˜=7)\n",
      "5. ì§€ë‚œ í‰ì–‘ ë„ì°© Â  (ì ìˆ˜=2.40, ë¬¸ì„œìˆ˜=6)\n",
      "6. ë™í•´ íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬ Â  (ì ìˆ˜=2.40, ë¬¸ì„œìˆ˜=6)\n",
      "7. ë¶í•œ íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬ Â  (ì ìˆ˜=2.40, ë¬¸ì„œìˆ˜=6)\n",
      "8. ë™í•´ ë°œì‚¬ Â  (ì ìˆ˜=2.10, ë¬¸ì„œìˆ˜=9)\n",
      "9. ëª¨ìŠ¤í¬ë°” ê´‘ì¥ ì—´ë¦¬ë‹¤ Â  (ì ìˆ˜=2.00, ë¬¸ì„œìˆ˜=5)\n",
      "10. ì¼ëŒ€ ë™í•´ ë°œì‚¬ Â  (ì ìˆ˜=2.00, ë¬¸ì„œìˆ˜=5)\n",
      "ğŸ’¾ ê²°ê³¼ê°€ '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_05_keywords.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "==================================================\n",
      "ğŸ“… 2025ë…„ 6ì›” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 80434ê°œì˜ ê¸°ì‚¬ ì¤‘ ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ 277ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ ìˆ˜: 277ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ê²°ê³¼: í–‰ë™ë™ì‚¬ 74ê°œ, í–‰ìœ„ëª…ì‚¬ 79ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF ì½”í¼ìŠ¤: 277ê°œ ë¬¸ì„œ\n",
      "TF-IDF ìš©ì–´ìˆ˜: 359320\n",
      "\n",
      "=== 2025ë…„ 6ì›” 'ì‚¬ê±´ êµ¬' TOP 30 (TF-IDF + DF ê²°í•©) ===\n",
      "1. ëŒ€ë‚¨ ì†ŒìŒ ë°©ì†¡ Â  (ì ìˆ˜=6.41, ë¬¸ì„œìˆ˜=16)\n",
      "2. ëŒ€ë¶ í™•ì„±ê¸° ë°©ì†¡ Â  (ì ìˆ˜=6.00, ë¬¸ì„œìˆ˜=15)\n",
      "3. ì†ŒìŒ ë°©ì†¡ Â  (ì ìˆ˜=4.91, ë¬¸ì„œìˆ˜=16)\n",
      "4. í™•ì„±ê¸° ë°©ì†¡ Â  (ì ìˆ˜=4.90, ë¬¸ì„œìˆ˜=16)\n",
      "5. ì§„ìˆ˜ì‹ ë„ì¤‘ ë„˜ì–´ì§€ë‹¤ Â  (ì ìˆ˜=4.00, ë¬¸ì„œìˆ˜=10)\n",
      "6. ë°©ì†¡ ì¤‘ë‹¨ Â  (ì ìˆ˜=3.30, ë¬¸ì„œìˆ˜=12)\n",
      "7. ì†ŒìŒ ë°©ì†¡ ì¤‘ë‹¨ Â  (ì ìˆ˜=3.20, ë¬¸ì„œìˆ˜=8)\n",
      "8. í™•ì„±ê¸° ë°©ì†¡ ì¤‘ë‹¨ Â  (ì ìˆ˜=3.20, ë¬¸ì„œìˆ˜=8)\n",
      "9. í˜„ì§€ ì‹œê°„ ë³´ë„ Â  (ì ìˆ˜=3.20, ë¬¸ì„œìˆ˜=8)\n",
      "10. ì›ì‚° ê°ˆë‹¤ Â  (ì ìˆ˜=2.90, ë¬¸ì„œìˆ˜=11)\n",
      "ğŸ’¾ ê²°ê³¼ê°€ '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_06_keywords.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "==================================================\n",
      "ğŸ“… 2025ë…„ 7ì›” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 80434ê°œì˜ ê¸°ì‚¬ ì¤‘ ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ 114ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ ìˆ˜: 114ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ê²°ê³¼: í–‰ë™ë™ì‚¬ 43ê°œ, í–‰ìœ„ëª…ì‚¬ 24ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF ì½”í¼ìŠ¤: 114ê°œ ë¬¸ì„œ\n",
      "TF-IDF ìš©ì–´ìˆ˜: 359320\n",
      "\n",
      "=== 2025ë…„ 7ì›” 'ì‚¬ê±´ êµ¬' TOP 30 (TF-IDF + DF ê²°í•©) ===\n",
      "1. ëª…ì˜ ì‹ ë³‘ í™•ë³´ Â  (ì ìˆ˜=4.01, ë¬¸ì„œìˆ˜=10)\n",
      "2. ê´€ê³„ ê¸°ê´€ ì¡°ì‚¬ Â  (ì ìˆ˜=3.60, ë¬¸ì„œìˆ˜=9)\n",
      "3. êµ°ì‚¬ë¶„ê³„ì„  ë„˜ì–´ì˜¤ë‹¤ Â  (ì ìˆ˜=2.91, ë¬¸ì„œìˆ˜=11)\n",
      "4. ì‹ ë³‘ í™•ë³´ Â  (ì ìˆ˜=2.51, ë¬¸ì„œìˆ˜=10)\n",
      "5. ê¸°ê´€ ì¡°ì‚¬ Â  (ì ìˆ˜=2.50, ë¬¸ì„œìˆ˜=10)\n",
      "6. ë¶í•œ ì›ì‚° ê°ˆë‹¤ Â  (ì ìˆ˜=2.00, ë¬¸ì„œìˆ˜=5)\n",
      "7. ì „ì„  êµ°ì‚¬ë¶„ê³„ì„  ë„˜ì–´ì˜¤ë‹¤ Â  (ì ìˆ˜=2.00, ë¬¸ì„œìˆ˜=5)\n",
      "8. ì£¼ë¯¼ ë™í•´ ì†¡í™˜ Â  (ì ìˆ˜=2.00, ë¬¸ì„œìˆ˜=5)\n",
      "9. í•´ë‹¹ ì¸ì› ì‹ë³„ Â  (ì ìˆ˜=2.00, ë¬¸ì„œìˆ˜=5)\n",
      "10. ì‹ ë³‘ í™•ë³´ ë°íˆë‹¤ Â  (ì ìˆ˜=2.00, ë¬¸ì„œìˆ˜=5)\n",
      "ğŸ’¾ ê²°ê³¼ê°€ '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_07_keywords.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "==================================================\n",
      "ğŸ“… 2025ë…„ 8ì›” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 80434ê°œì˜ ê¸°ì‚¬ ì¤‘ ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ 0ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "âš ï¸ 2025ë…„ 8ì›”ì— ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
      "\n",
      "==================================================\n",
      "ğŸ“… 2025ë…„ 9ì›” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 80434ê°œì˜ ê¸°ì‚¬ ì¤‘ ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ 0ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "âš ï¸ 2025ë…„ 9ì›”ì— ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
      "\n",
      "==================================================\n",
      "ğŸ“… 2025ë…„ 10ì›” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 80434ê°œì˜ ê¸°ì‚¬ ì¤‘ ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ 0ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "âš ï¸ 2025ë…„ 10ì›”ì— ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
      "\n",
      "==================================================\n",
      "ğŸ“… 2025ë…„ 11ì›” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 80434ê°œì˜ ê¸°ì‚¬ ì¤‘ ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ 0ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "âš ï¸ 2025ë…„ 11ì›”ì— ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
      "\n",
      "==================================================\n",
      "ğŸ“… 2025ë…„ 12ì›” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 80434ê°œì˜ ê¸°ì‚¬ ì¤‘ ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ 0ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "âš ï¸ 2025ë…„ 12ì›”ì— ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ‰ 2025ë…„ ì›”ë³„ í‚¤ì›Œë“œ ì¶”ì¶œì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "ğŸ“ ê²°ê³¼ íŒŒì¼ë“¤ì´ '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ğŸ“Š ì—°ê°„ ì¢…í•© ê²°ê³¼: '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_keywords_by_month_all.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from konlpy.tag import Okt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "import calendar\n",
    "\n",
    "# =========================\n",
    "# ì„¤ì •\n",
    "# =========================\n",
    "file_path = '/home/ds4_sia_nolb/#FINAL_POLARIS/04_plus_preprocessing/preprocessing_final_data/re_final_preprocessing.json'\n",
    "output_dir = '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results'\n",
    "TFIDF_VECTORIZER_PATH = '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_idf_vectorizer_for_all_corpus.pkl'\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# í˜•íƒœì†Œ ë¶„ì„ê¸°\n",
    "# =========================\n",
    "okt = Okt()\n",
    "\n",
    "# =========================\n",
    "# ë‚ ì§œ íŒŒì„œ (ì—¬ëŸ¬ í¬ë§· í—ˆìš©)\n",
    "# =========================\n",
    "def parse_date_flexible(s: str):\n",
    "    if not s or not isinstance(s, str):\n",
    "        return None\n",
    "    s = s.strip()\n",
    "\n",
    "    candidates = [s]\n",
    "    if \"T\" in s:\n",
    "        candidates.append(s[:19])\n",
    "        candidates.append(s[:10])\n",
    "    if len(s) >= 10:\n",
    "        candidates.append(s[:10])\n",
    "    if \"-\" not in s and \".\" not in s and \"/\" not in s and len(s) == 8:\n",
    "        candidates.append(f\"{s[:4]}-{s[4:6]}-{s[6:8]}\")\n",
    "\n",
    "    fmts = [\n",
    "        \"%Y-%m-%d\",\n",
    "        \"%Y-%m-%d %H:%M:%S\",\n",
    "        \"%Y-%m-%d %H:%M\",\n",
    "        \"%Y/%m/%d\",\n",
    "        \"%Y/%m/%d %H:%M:%S\",\n",
    "        \"%Y.%m.%d\",\n",
    "        \"%Y.%m.%d %H:%M:%S\",\n",
    "        \"%Y.%m.%d %H:%M\",\n",
    "        \"%Y%m%d\",\n",
    "        \"%Y-%m-%dT%H:%M:%S\",\n",
    "    ]\n",
    "\n",
    "    for cand in candidates:\n",
    "        for fmt in fmts:\n",
    "            try:\n",
    "                return datetime.strptime(cand, fmt)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "def parse_date(date_str: str) -> datetime:\n",
    "    d = parse_date_flexible(date_str)\n",
    "    if d is None:\n",
    "        raise ValueError(f\"ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {date_str}\")\n",
    "    return d\n",
    "\n",
    "def extract_pubdate(article):\n",
    "    keys = [\"pubDate\", \"pubdate\", \"time\", \"date\", \"published\", \"pub_date\"]\n",
    "    for k in keys:\n",
    "        if k in article and article[k]:\n",
    "            dt = parse_date_flexible(str(article[k]))\n",
    "            if dt:\n",
    "                return dt\n",
    "    meta = article.get(\"metadata\", {}) or {}\n",
    "    for k in keys:\n",
    "        if k in meta and meta[k]:\n",
    "            dt = parse_date_flexible(str(meta[k]))\n",
    "            if dt:\n",
    "                return dt\n",
    "    return None\n",
    "\n",
    "# =========================\n",
    "# ê¸°ì‚¬ ë¡œë“œ ë° í•„í„°ë§ ìœ í‹¸\n",
    "# =========================\n",
    "def load_all_articles(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(\"JSON ë£¨íŠ¸ëŠ” list ì—¬ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "        return None\n",
    "\n",
    "def filter_articles_by_period(articles, start_date_str, end_date_str):\n",
    "    sdt = parse_date(start_date_str)\n",
    "    edt = parse_date(end_date_str)\n",
    "    \n",
    "    period_articles = []\n",
    "    \n",
    "    all_articles_count = len(articles)\n",
    "    \n",
    "    for article in tqdm(articles, desc=f\"ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ({sdt.date()}~{edt.date()})\", leave=False):\n",
    "        pub_date = extract_pubdate(article)\n",
    "        if pub_date and sdt <= pub_date <= edt:\n",
    "            period_articles.append(article)\n",
    "    \n",
    "    print(f\"ì´ {all_articles_count}ê°œì˜ ê¸°ì‚¬ ì¤‘ ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ {len(period_articles)}ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    return period_articles\n",
    "\n",
    "# =========================\n",
    "# í…ìŠ¤íŠ¸ ì •ê·œí™” (í‘œê¸° í†µì¼ ì•½ê°„)\n",
    "# =========================\n",
    "def normalize_text(t: str) -> str:\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    t = t.replace(\"íƒ„ë„ ë¯¸ì‚¬ì¼\", \"íƒ„ë„ë¯¸ì‚¬ì¼\")\n",
    "    t = t.replace(\"ìˆœí•­ ë¯¸ì‚¬ì¼\", \"ìˆœí•­ë¯¸ì‚¬ì¼\")\n",
    "    t = t.replace(\"ê·¹ì´ˆ ìŒì†\", \"ê·¹ì´ˆìŒì†\")\n",
    "    t = t.replace(\"ì´ˆëŒ€í˜• ë°©ì‚¬í¬\", \"ì´ˆëŒ€í˜•ë°©ì‚¬í¬\")\n",
    "    return t\n",
    "\n",
    "# =========================\n",
    "# ë¶ˆìš©ì–´ & ë‰´ìŠ¤ ë…¸ì´ì¦ˆ\n",
    "# =========================\n",
    "BASE_STOP = set([\n",
    "    'ê°€','ê°„','ê°™ì€','ê°™ì´','ê²ƒ','ê²Œë‹¤ê°€','ê²°êµ­','ê³§','ê´€í•˜ì—¬','ê´€ë ¨','ê´€í•œ','ê·¸','ê·¸ê²ƒ','ê·¸ë…€','ê·¸ë“¤',\n",
    "    'ê·¸ë¦¬ê³ ','ê·¸ë•Œ','ê·¸ë˜','ê·¸ë˜ì„œ','ê·¸ëŸ¬ë‚˜','ê·¸ëŸ¬ë¯€ë¡œ','ê·¸ëŸ¬í•œ','ê·¸ëŸ°','ê·¸ë ‡ê²Œ','ê·¸ì™¸','ê·¼ê±°ë¡œ','ê¸°íƒ€',\n",
    "    'ê¹Œì§€ë„','ê¹Œì§€','ë‚˜','ë‚¨ë“¤','ë„ˆ','ëˆ„êµ¬','ë‹¤','ë‹¤ê°€','ë‹¤ë¥¸','ë‹¤ë§Œ','ë‹¤ì†Œ','ë‹¤ìˆ˜','ë‹¤ì‹œ','ë‹¤ìŒ','ë‹¨','ë‹¨ì§€',\n",
    "    'ë‹¹ì‹ ','ëŒ€','ëŒ€í•´ì„œ','ë”êµ°ë‹¤ë‚˜','ë”êµ¬ë‚˜','ë”ë¼ë„','ë”ìš±ì´','ë„','ë„ë¡œ','ë˜','ë˜ëŠ”','ë˜í•œ','ë•Œ','ë•Œë¬¸',\n",
    "    'ë¼ë„','ë¼ë©´','ë¼ëŠ”','ë¡œ','ë¡œë¶€í„°','ë¡œì¨','ë¥¼','ë§ˆì €','ë§ˆì¹˜','ë§Œì•½','ë§Œì¼','ë§Œí¼','ëª¨ë‘','ë¬´ì—‡','ë¬´ìŠ¨',\n",
    "    'ë¬´ì²™','ë¬¼ë¡ ','ë°','ë°–ì—','ë°”ë¡œ','ë³´ë‹¤','ë¿ì´ë‹¤','ì‚¬ëŒ','ì‚¬ì‹¤ì€','ìƒëŒ€ì ìœ¼ë¡œ','ìƒê°','ì„¤ë ¹','ì†Œìœ„','ìˆ˜',\n",
    "    'ìˆ˜ì¤€','ì‰½ê²Œ','ì‹œëŒ€','ì‹œì‘í•˜ì—¬','ì‹¤ë¡œ','ì‹¤ì œ','ì•„ë‹ˆ','ì•„ë¬´','ì•„ë¬´ë„','ì•„ë¬´ë¦¬','ì•„ë§ˆë„','ì•„ìš¸ëŸ¬','ì•„ì§',\n",
    "    'ì•ì—ì„œ','ì•ìœ¼ë¡œ','ì–´ëŠ','ì–´ë–¤','ì–´ë–»ê²Œ','ì–´ë””','ì–¸ì œ','ì–¼ë§ˆë‚˜','ì—¬ê¸°','ì—¬ë¶€','ì—­ì‹œ','ì˜ˆ','ì˜¤íˆë ¤',\n",
    "    'ì™€','ì™œ','ì™¸ì—ë„','ìš”','ìš°ë¦¬','ìš°ì„ ','ì›ë˜','ìœ„í•´ì„œ','ìœ¼ë¡œ','ìœ¼ë¡œë¶€í„°','ìœ¼ë¡œì¨','ì„','ì˜','ì˜ê±°í•˜ì—¬',\n",
    "    'ì˜ì§€í•˜ì—¬','ì˜í•´','ì˜í•´ì„œ','ì˜í•˜ì—¬','ì´','ì´ê²ƒ','ì´ê³³','ì´ë•Œ','ì´ë¼ê³ ','ì´ëŸ¬í•œ','ì´ëŸ°','ì´ë ‡ê²Œ','ì´ì œ',\n",
    "    'ì´ì§€ë§Œ','ì´í›„','ì´ìƒ','ì´ë‹¤','ì´ì „','ì¸','ì¼','ì¼ë‹¨','ì¼ë°˜ì ìœ¼ë¡œ','ì„ì‹œë¡œ','ì…ì¥ì—ì„œ','ì','ìê¸°','ìì‹ ',\n",
    "    'ì ì‹œ','ì €','ì €ê²ƒ','ì €ê¸°','ì €ìª½','ì €í¬','ì „ë¶€','ì „í˜€','ì ì—ì„œ','ì •ë„','ì œ','ì¡°ê¸ˆ','ì¢€','ì£¼ë¡œ','ì£¼ì œ','ì¦‰',\n",
    "    'ì¦‰ì‹œ','ì§€ê¸ˆ','ì§„ì§œë¡œ','ì°¨ë¼ë¦¬','ì°¸','ì°¸ìœ¼ë¡œ','ì²«ë²ˆì§¸ë¡œ','ìµœê³ ','ìµœëŒ€','ìµœì†Œ','ìµœì‹ ','ìµœì´ˆ','í†µí•˜ì—¬',\n",
    "    'í†µí•´ì„œ','í‰ê°€','í¬í•¨í•œ','í¬í•¨í•˜ì—¬','í•˜ì§€ë§Œ','í•˜ë©´ì„œ','í•˜ì—¬','í•œ','í•œë•Œ','í•œë²ˆ','í• ','í• ê²ƒì´ë‹¤','í• ìˆ˜ìˆë‹¤',\n",
    "    'í•¨ê»˜','í•´ë„', \n",
    "    # ì•„ë˜ í‚¤ì›Œë“œëŠ” idf_vectorizer_for_all_corpus.pklíŒŒì¼ ìƒì„± ì´í›„ ì¶”ê°€ëœ ë¶ˆìš©ì–´ì„. BASE_STOPì— ìˆìœ¼ë©´ pklíŒŒì¼ë¡œ ì¸í•´ ë¯¸ì ìš© ë˜ê¸° ë•Œë¬¸ì— ENTITY_NOISEì— ì¶”ê°€í•˜ì˜€ìŒ.\n",
    "    # 'ë¼ë‹¤', 'ì„œë‹¤', 'ëŒ€í•´', 'ë‚˜ì˜¤ë‹¤', 'í†µí•´', 'ë§ë‹¤', 'ëŒ€í•œ', 'ìœ„í•´', 'ê¸°ìƒì²­', 'ì˜ˆë³´', 'ë°íˆë‹¤', 'í¬ë‹¤', 'ì•½ê°„', 'ê°€ë‹¤', 'ë‚´ë¦¬ë‹¤', 'ë°›ë‹¤', 'ê¸°ì˜¨'\n",
    "])\n",
    "\n",
    "NEWS_STOP = {\"ê¸°ì\",\"ì—°í•©ë‰´ìŠ¤\",\"ì‚¬ì§„\",\"ì†ë³´\",\"ì¢…í•©\",\"ìë£Œ\",\"ì˜ìƒ\",\"ë‹¨ë…\",\"ì „ë¬¸\",\"ì¸í„°ë·°\",\"ë¸Œë¦¬í•‘\"}\n",
    "\n",
    "# =========================\n",
    "# ì—”í„°í‹° ë…¸ì´ì¦ˆ\n",
    "# =========================\n",
    "ENTITY_NOISE = {\n",
    "    \"ë¶í•œ\",\"í•œêµ­\",\"ëŒ€í•œë¯¼êµ­\",\"ë‚¨í•œ\",\"ë¯¸êµ­\",\"ì¤‘êµ­\",\"ì¼ë³¸\",\"ëŸ¬ì‹œì•„\",\"ìš°í¬ë¼ì´ë‚˜\",\"ìœ ì—”\",\"ë‚˜í† \",\"NATO\",\"EU\",\"ìœ ëŸ½ì—°í•©\",\n",
    "    \"í‘¸í‹´\",\"ë¸”ë¼ë””ë¯¸ë¥´ í‘¸í‹´\",\"ë°”ì´ë“ \",\"ì¡° ë°”ì´ë“ \",\"ì‹œì§„í•‘\",\"ê¹€ì •ì€\",\"ê¹€ì—¬ì •\",\"ë¬¸ì¬ì¸\",\"ìœ¤ì„ì—´\",\"ì‡¼ì´êµ¬\",\"ì ¤ë ŒìŠ¤í‚¤\", \"ì¤‘ì•™\", \"í†µì‹ \", \"ë³´ë„\", \n",
    "    # ì•„ë˜ í‚¤ì›Œë“œëŠ” BASE_STOPì— ìˆì–´ì•¼í•˜ì§€ë§Œ ì ì‹œ ì˜®ê²¨ì˜´.\n",
    "    'ë¼ë‹¤', 'ì„œë‹¤', 'ëŒ€í•´', 'ë‚˜ì˜¤ë‹¤', 'í†µí•´', 'ë§ë‹¤', 'ëŒ€í•œ', 'ìœ„í•´', 'ê¸°ìƒì²­', 'ì˜ˆë³´', 'ë°íˆë‹¤', 'í¬ë‹¤', 'ì•½ê°„', 'ê°€ë‹¤', 'ë‚´ë¦¬ë‹¤', 'ë°›ë‹¤', 'ê¸°ì˜¨',\n",
    "    'ê°•ìˆ˜', 'ë‚ ì”¨', 'ì†Œì‹í†µ', 'ì¸ìš©', 'ëŒ€ì²´ë¡œ', 'ì´ë²ˆ', 'ë“¤ë‹¤', 'ë“¤ì–´', 'ì˜¬í•´', \n",
    "\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# í† í°/í…ìŠ¤íŠ¸\n",
    "# =========================\n",
    "def pos_tokens(text: str):\n",
    "    text = normalize_text(text or \"\")\n",
    "    return okt.pos(text, norm=True, stem=True)\n",
    "\n",
    "def doc_text(a) -> str:\n",
    "    # ìˆ˜ì •: metadataì˜ titleê³¼ ìµœìƒìœ„ summaryë¥¼ í•¨ê»˜ ì‚¬ìš©\n",
    "    title = (a.get('metadata') or {}).get('title', '')\n",
    "    summary = a.get('summary', '')\n",
    "    return normalize_text(f\"{title} {summary}\")\n",
    "\n",
    "def tokenizer_for_vectorizer(s: str):\n",
    "    toks = []\n",
    "    for w, t in okt.pos(s, norm=True, stem=True):\n",
    "        if t not in (\"Noun\", \"Verb\"):\n",
    "            continue\n",
    "        if len(w) <= 1:\n",
    "            continue\n",
    "        if w in BASE_STOP or w in NEWS_STOP:\n",
    "            continue\n",
    "        if w.isdigit():\n",
    "            continue\n",
    "        toks.append(w)\n",
    "    return toks\n",
    "\n",
    "# =========================\n",
    "# ìë™ í•™ìŠµ: 'í–‰ë™ ë™ì‚¬'ì™€ 'í–‰ìœ„ ëª…ì‚¬'\n",
    "# =========================\n",
    "def learn_action_lexicons(articles, min_df_ratio_verbs=0.002, min_df_ratio_nouns=0.002):\n",
    "    verb_doc_df = Counter()\n",
    "    action_noun_df = Counter()\n",
    "    N_docs = len(articles)\n",
    "\n",
    "    for a in tqdm(articles, desc=\"í–‰ë™ ë™ì‚¬/í–‰ìœ„ëª…ì‚¬ í•™ìŠµ ì¤‘\", leave=False):\n",
    "        title = (a.get('metadata') or {}).get('title', '') # ìˆ˜ì •\n",
    "        summary = a.get('summary','') or ''\n",
    "        p = pos_tokens(f\"{title} {summary}\")\n",
    "\n",
    "        verbs_in_doc = set()\n",
    "        action_nouns_in_doc = set()\n",
    "\n",
    "        for i, (w, t) in enumerate(p):\n",
    "            if t == \"Verb\":\n",
    "                verbs_in_doc.add(w)\n",
    "            if t == \"Noun\":\n",
    "                ahead = [p[j][0] for j in range(i+1, min(i+3, len(p)))]\n",
    "                if \"í•˜ë‹¤\" in ahead or \"ë˜ë‹¤\" in ahead:\n",
    "                    if w not in BASE_STOP and len(w) > 1:\n",
    "                        action_nouns_in_doc.add(w)\n",
    "\n",
    "        for v in verbs_in_doc:\n",
    "            verb_doc_df[v] += 1\n",
    "        for n in action_nouns_in_doc:\n",
    "            action_noun_df[n] += 1\n",
    "\n",
    "    min_df_verbs = max(5, int(N_docs * min_df_ratio_verbs))\n",
    "    min_df_nouns = max(5, int(N_docs * min_df_ratio_nouns))\n",
    "\n",
    "    drop_verbs = {\"í•˜ë‹¤\",\"ë˜ë‹¤\",\"ì´ë‹¤\",\"ìˆë‹¤\"}\n",
    "    verb_set = {v for v,df in verb_doc_df.items() if df >= min_df_verbs and v not in drop_verbs}\n",
    "    action_nouns = {n for n,df in action_noun_df.items() if df >= min_df_nouns}\n",
    "\n",
    "    print(f\"í•™ìŠµ ê²°ê³¼: í–‰ë™ë™ì‚¬ {len(verb_set)}ê°œ, í–‰ìœ„ëª…ì‚¬ {len(action_nouns)}ê°œ\")\n",
    "    return verb_set, action_nouns\n",
    "\n",
    "# =========================\n",
    "# ì‚¬ê±´ êµ¬ í›„ë³´ ìƒì„± + TF-IDF ê²°í•© ë­í‚¹\n",
    "# =========================\n",
    "def nominalize_verb(v: str) -> str:\n",
    "    if v.endswith(\"í•˜ë‹¤\"):\n",
    "        return v[:-2]\n",
    "    if v.endswith(\"ë˜ë‹¤\"):\n",
    "        return v[:-2]\n",
    "    return v\n",
    "\n",
    "def extract_event_phrases_auto(articles, top_k=30, vectorizer=None):\n",
    "    N = len(articles)\n",
    "    if N == 0:\n",
    "        print(\"âš  ì§€ì • ê¸°ê°„ì— ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ ìˆ˜: {N}ê°œ\")\n",
    "    \n",
    "    verb_set, action_nouns = learn_action_lexicons(articles)\n",
    "\n",
    "    phrase_df = Counter()\n",
    "    phrase_examples = defaultdict(list)\n",
    "\n",
    "    for a in tqdm(articles, desc=\"ì‚¬ê±´ êµ¬ ìë™ ì¶”ì¶œ ì¤‘\", leave=False):\n",
    "        # ìˆ˜ì •: metadataì—ì„œ titleì„ ê°€ì ¸ì˜´\n",
    "        title = (a.get('metadata') or {}).get('title', '')\n",
    "        summary = a.get('summary','') or ''\n",
    "        p = pos_tokens(f\"{title} {summary}\")\n",
    "        phrases_in_doc = set()\n",
    "\n",
    "        prev_nouns = []\n",
    "        L = len(p)\n",
    "        for i, (w, t) in enumerate(p):\n",
    "            if t == \"Noun\":\n",
    "                if w not in BASE_STOP and len(w) > 1:\n",
    "                    prev_nouns.append(w)\n",
    "                    if len(prev_nouns) > 5:\n",
    "                        prev_nouns = prev_nouns[-5:]\n",
    "\n",
    "            if t == \"Verb\" and w in verb_set:\n",
    "                vnom = nominalize_verb(w)\n",
    "                nn = [n for n in reversed(prev_nouns)][:2]\n",
    "                if nn:\n",
    "                    phrases_in_doc.add(f\"{nn[0]} {vnom}\".strip())\n",
    "                    if len(nn) >= 2:\n",
    "                        phrases_in_doc.add(f\"{nn[1]} {nn[0]} {vnom}\".strip())\n",
    "                else:\n",
    "                    phrases_in_doc.add(vnom.strip())\n",
    "\n",
    "            if t == \"Noun\" and w in action_nouns:\n",
    "                nn = [n for n in reversed(prev_nouns) if n != w][:2]\n",
    "                base = w\n",
    "                if nn:\n",
    "                    phrases_in_doc.add(f\"{nn[0]} {base}\".strip())\n",
    "                    if len(nn) >= 2:\n",
    "                        phrases_in_doc.add(f\"{nn[1]} {nn[0]} {base}\".strip())\n",
    "                else:\n",
    "                    phrases_in_doc.add(base.strip())\n",
    "\n",
    "                if i+1 < L and p[i+1][1] == \"Noun\" and p[i+1][0] in action_nouns:\n",
    "                    tail = p[i+1][0]\n",
    "                    if nn:\n",
    "                        phrases_in_doc.add(f\"{nn[0]} {base} {tail}\".strip())\n",
    "                        if len(nn) >= 2:\n",
    "                            phrases_in_doc.add(f\"{nn[1]} {nn[0]} {base} {tail}\".strip())\n",
    "                    else:\n",
    "                        phrases_in_doc.add(f\"{base} {tail}\".strip())\n",
    "\n",
    "        cleaned = set()\n",
    "        for ph in phrases_in_doc:\n",
    "            ph = re.sub(r\"\\s+\", \" \", ph).strip()\n",
    "            if len(ph.split()) == 1 and len(ph) <= 2:\n",
    "                continue\n",
    "            cleaned.add(ph)\n",
    "\n",
    "        for ph in cleaned:\n",
    "            phrase_df[ph] += 1\n",
    "            # ìˆ˜ì •: titleì´ ì¡´ì¬í•  ë•Œë§Œ examplesì— ì¶”ê°€\n",
    "            if len(phrase_examples[ph]) < 3 and title:\n",
    "                phrase_examples[ph].append(title)\n",
    "\n",
    "    if vectorizer is None:\n",
    "        print(\"[ì˜¤ë¥˜] TfidfVectorizer ê°ì²´ê°€ ì „ë‹¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        return []\n",
    "\n",
    "    corpus_period = [doc_text(a) for a in articles]\n",
    "    Xp = vectorizer.transform(corpus_period)\n",
    "    tfidf_avg = np.asarray(Xp.mean(axis=0)).ravel()\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    tfidf_dict = {terms[i]: float(tfidf_avg[i]) for i in np.where(tfidf_avg > 0)[0]}\n",
    "\n",
    "    print(f\"TF-IDF ì½”í¼ìŠ¤: {len(corpus_period)}ê°œ ë¬¸ì„œ\")\n",
    "    print(f\"TF-IDF ìš©ì–´ìˆ˜: {len(terms)}\")\n",
    "\n",
    "    def is_entity_only(ph: str) -> bool:\n",
    "        toks = ph.split()\n",
    "        if len(toks) <= 2 and any(ent in ph for ent in ENTITY_NOISE):\n",
    "            return True\n",
    "        ent_hits = sum(1 for t in toks if any(ent in t for ent in ENTITY_NOISE))\n",
    "        return (ent_hits >= max(1, len(toks) - 1))\n",
    "\n",
    "    def generic_penalty(ph: str) -> int:\n",
    "        generic = {\"ëŒ€í†µë ¹\",\"ìœ„ì›ì¥\",\"ì •ë¶€\",\"ë‹¹êµ­\",\"ê´€ê³„ì\",\"ëŒ€ë³€ì¸\",\"íšŒì˜\",\"ë…¼ì˜\",\"ê°•ì¡°\"}\n",
    "        return -sum(1 for t in ph.split() if t in generic)\n",
    "\n",
    "    def phrase_score(ph: str, df_cnt: int) -> float:\n",
    "        tfidf = tfidf_dict.get(ph, 0.0)\n",
    "        score = 0.6 * tfidf + 0.4 * float(df_cnt)\n",
    "\n",
    "        if is_entity_only(ph):\n",
    "            score -= 6.0\n",
    "        score += generic_penalty(ph)\n",
    "        if len(ph.split()) <= 2:\n",
    "            score -= 1.5\n",
    "        return score\n",
    "\n",
    "    scored = []\n",
    "    for ph, cnt in phrase_df.items():\n",
    "        if is_entity_only(ph):\n",
    "            continue\n",
    "        scored.append( (ph, cnt, phrase_score(ph, cnt)) )\n",
    "\n",
    "    scored.sort(key=lambda x: (x[2], x[1]), reverse=True)\n",
    "    ranked = [(ph, cnt, score, phrase_examples.get(ph, [])) for ph, cnt, score in scored[:top_k]]\n",
    "    return ranked\n",
    "\n",
    "# =========================\n",
    "# ì „ì²´ ì½”í¼ìŠ¤ìš© TF-IDF ë²¡í„°ë¼ì´ì € ì‚¬ì „ í•™ìŠµ\n",
    "# =========================\n",
    "def pre_train_vectorizer(articles, save_path):\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"âœ”ï¸ ê¸°ì¡´ TF-IDF ë²¡í„°ë¼ì´ì € íŒŒì¼ '{save_path}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "        return joblib.load(save_path)\n",
    "    \n",
    "    print(f\"ğŸ” ì „ì²´ ì½”í¼ìŠ¤ìš© TF-IDF ë²¡í„°ë¼ì´ì €ë¥¼ ìƒˆë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    full_corpus = [doc_text(a) for a in articles]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(\n",
    "        tokenizer=tokenizer_for_vectorizer,\n",
    "        ngram_range=(1, 3),\n",
    "        min_df=5,\n",
    "        max_df=0.85,\n",
    "        sublinear_tf=True,\n",
    "        norm='l2'\n",
    "    )\n",
    "    vectorizer.fit(full_corpus)\n",
    "    joblib.dump(vectorizer, save_path)\n",
    "    print(f\"âœ… ì „ì²´ ì½”í¼ìŠ¤ ê¸°ë°˜ TF-IDF ë²¡í„°ë¼ì´ì €ë¥¼ '{save_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    return vectorizer\n",
    "\n",
    "# =========================\n",
    "# ì›”ë³„ ìë™ ì²˜ë¦¬ í•¨ìˆ˜\n",
    "# =========================\n",
    "def process_monthly_keywords(year, all_articles, vectorizer):\n",
    "    \"\"\"ì§€ì •ëœ ì—°ë„ì˜ ëª¨ë“  ì›”ì— ëŒ€í•´ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for month in range(1, 13):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"ğŸ“… {year}ë…„ {month}ì›” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        start_date = f\"{year}-{month:02d}-01\"\n",
    "        last_day = calendar.monthrange(year, month)[1]\n",
    "        end_date = f\"{year}-{month:02d}-{last_day:02d}\"\n",
    "        \n",
    "        monthly_articles = filter_articles_by_period(all_articles, start_date, end_date)\n",
    "        \n",
    "        if not monthly_articles:\n",
    "            print(f\"âš ï¸ {year}ë…„ {month}ì›”ì— ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            results[f\"{year}_{month:02d}\"] = []\n",
    "            continue\n",
    "        \n",
    "        events = extract_event_phrases_auto(\n",
    "            monthly_articles,\n",
    "            top_k=30,\n",
    "            vectorizer=vectorizer\n",
    "        )\n",
    "        \n",
    "        results[f\"{year}_{month:02d}\"] = events\n",
    "        \n",
    "        print(f\"\\n=== {year}ë…„ {month}ì›” 'ì‚¬ê±´ êµ¬' TOP 30 (TF-IDF + DF ê²°í•©) ===\")\n",
    "        if not events:\n",
    "            print(f\"{year}ë…„ {month}ì›”ì— ì¶”ì¶œëœ ì‚¬ê±´ êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            for i, (ph, cnt, score, examples) in enumerate(events[:10], 1):\n",
    "                print(f\"{i}. {ph} Â  (ì ìˆ˜={score:.2f}, ë¬¸ì„œìˆ˜={cnt})\")\n",
    "        \n",
    "        output_file = os.path.join(output_dir, f\"{year}_{month:02d}_keywords.json\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'year': year,\n",
    "                'month': month,\n",
    "                'period': f\"{start_date} ~ {end_date}\",\n",
    "                'total_articles': len(monthly_articles),\n",
    "                'keywords': [{'phrase': ph, 'doc_count': cnt, 'score': round(score, 2), 'examples': examples} \n",
    "                             for ph, cnt, score, examples in events]\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"ğŸ’¾ ê²°ê³¼ê°€ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# =========================\n",
    "# ì‹¤í–‰ë¶€\n",
    "# =========================\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"ğŸ“– ì „ì²´ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...\")\n",
    "        all_articles = load_all_articles(file_path)\n",
    "        if not all_articles:\n",
    "            print(\"ì „ì²´ ì½”í¼ìŠ¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "            exit()\n",
    "\n",
    "        vectorizer = pre_train_vectorizer(all_articles, TFIDF_VECTORIZER_PATH)\n",
    "        \n",
    "        year = int(input(\"ë¶„ì„í•  ì—°ë„ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 2024): \").strip())\n",
    "        \n",
    "        print(f\"\\nğŸš€ {year}ë…„ ì›”ë³„ í‚¤ì›Œë“œ ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "        \n",
    "        monthly_results = process_monthly_keywords(year, all_articles, vectorizer)\n",
    "        \n",
    "        summary_file = os.path.join(output_dir, f\"{year}_keywords_by_month_all.json\")\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(monthly_results, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"\\nğŸ‰ {year}ë…„ ì›”ë³„ í‚¤ì›Œë“œ ì¶”ì¶œì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "        print(f\"ğŸ“ ê²°ê³¼ íŒŒì¼ë“¤ì´ '{output_dir}' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        print(f\"ğŸ“Š ì—°ê°„ ì¢…í•© ê²°ê³¼: '{summary_file}'\")\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"ì˜¤ë¥˜: {e}. ì˜¬ë°”ë¥¸ ì—°ë„ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd798cb",
   "metadata": {},
   "source": [
    "# í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„°\n",
    "\n",
    "- ì›”ë³„ í‚¤ì›Œë“œ ì¶”ì¶œí–ˆì„ ë•Œ ì˜ë¯¸ê°€ ì¤‘ë³µë˜ëŠ” í‚¤ì›Œë“œê°€ ë‹¤ìˆ˜ ë³´ì„.\n",
    "- í•´ë‹¹ ë¬¸ì œëŠ” í‚¤ì›Œë“œë³„ ì–¸ê¸‰ëœ ê¸°ì‚¬ ìˆ˜(doc_count)ì™€ ê·¸ì— ë”°ë¥¸ ìˆœìœ„ ì ìˆ˜(score)ê°€ ë¶„ì‚°ë˜ì–´ í•´ë‹¹ í‚¤ì›Œë“œì˜ ì¤‘ìš”ë„ê°€ ë‚®ì•„ì§ˆ ë¬¸ì œê°€ ìˆìŒ.\n",
    "- ê·¸ë˜ì„œ ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ í‚¤ì›Œë“œë¥¼ ë³‘í•©í•˜ëŠ” ì½”ë“œë¥¼ ì¶”ê°€ë¡œ ë§Œë“¬.\n",
    "- ì½”ë“œ í•˜ë‹¨ì— ì—°ë„, ì—°ë„ë³„ íŠ¹ì • ì›” ë“± ì›í•˜ëŠ” ë°©ë²•ì— ë”°ë¼ ì•½ê°„ì˜ ë³€ê²½ í›„ ì‹¤í–‰í•˜ë©´ ì¶”ì¶œë˜ì–´ ìˆë˜ í‚¤ì›Œë“œë“¤ì„ ë³‘í•©í•´ì„œ json íŒŒì¼ë¡œ ì €ì¥í•˜ê²Œ ë¨.\n",
    "- similarity_threshold ì— ë”°ë¼ì„œ í‚¤ì›Œë“œì˜ ìœ ì‚¬ë„ë¥¼ ê²°ì •í•  ìˆ˜ ìˆìŒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c8e7ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ìë™í™”ëœ í‚¤ì›Œë“œ ê·¸ë£¹í™” ì‹œìŠ¤í…œ ===\n",
      "ğŸ“ ë³€ê²½ ì‚¬í•­:\n",
      "- âŒ important_keywords ì œê±°\n",
      "- âŒ keyword_bonus ì œê±°\n",
      "- âœ… ìì¹´ë“œ(70%) + í¸ì§‘ê±°ë¦¬(30%) ìœ ì‚¬ë„ë§Œ ì‚¬ìš©\n",
      "- ğŸš€ ìë™í™”ëœ ë°°ì¹˜ ì²˜ë¦¬ ê¸°ëŠ¥ ì¶”ê°€\n",
      "\n",
      "ğŸš€ 2025ë…„ í‚¤ì›Œë“œ ê·¸ë£¹í™” ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘\n",
      "ğŸ“‚ ì…ë ¥ ë””ë ‰í† ë¦¬: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results\n",
      "ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster\n",
      "ğŸ“… ì²˜ë¦¬ ì›”: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "âš™ï¸ ì˜µì…˜: {'similarity_threshold': 0.4, 'min_group_size': 2, 'max_groups': 12}\n",
      "\n",
      "============================================================\n",
      "ì²˜ë¦¬ ì¤‘: 2025ë…„ 1ì›”\n",
      "ì…ë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_01_keywords.json\n",
      "ì¶œë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_01_keyword_grouped.json\n",
      "============================================================\n",
      "=== ìë™ í‚¤ì›Œë“œ ê·¸ë£¹í™” ì‹œì‘ (í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ ì œê±° ë²„ì „) ===\n",
      "ì›ë³¸ í‚¤ì›Œë“œ ìˆ˜: 30\n",
      "ìœ ì‚¬ë„ ì„ê³„ê°’: 0.4\n",
      "ìµœì†Œ ê·¸ë£¹ í¬ê¸°: 2\n",
      "ìœ ì‚¬ë„ ê³„ì‚°: ìì¹´ë“œ(70%) + í¸ì§‘ê±°ë¦¬(30%)\n",
      "\n",
      "=== ê·¸ë£¹í™” ì™„ë£Œ ===\n",
      "ìµœì¢… í‚¤ì›Œë“œ ê·¸ë£¹ ìˆ˜: 12\n",
      "ì••ì¶•ë¥ : 60.0%\n",
      "\n",
      "=== ìƒìœ„ ê·¸ë£¹ë“¤ (ìˆœìˆ˜ ë¬¸ìì—´ ìœ ì‚¬ë„ ê¸°ë°˜) ===\n",
      "1. íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬ (ì ìˆ˜: 60.3)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 9\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.483\n",
      "   - í†µí•© í‚¤ì›Œë“œ: íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬, ë¶í•œ íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬, ë¯¸ì‚¬ì¼ ë°œì‚¬, ë‹¨ê±°ë¦¬ íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬, íƒ„ë„ë¯¸ì‚¬ì¼ ì‹œí—˜ ë°œì‚¬, ê±°ë¦¬ íƒ„ë„ë¯¸ì‚¬ì¼ ì‹œí—˜ ë°œì‚¬, íƒ„ë„ë¯¸ì‚¬ì¼ ë™í•´ ë°œì‚¬, íƒ„ë„ë¯¸ì‚¬ì¼ ì‹œí—˜, ì˜¬í•´ íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬\n",
      "\n",
      "2. êµ°ì¸ ìƒí¬ (ì ìˆ˜: 18.1)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 3\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.568\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ë¶í•œ êµ°ì¸ ìƒí¬, êµ°ì¸ ìƒí¬ ë°íˆë‹¤, êµ°ì¸ ìƒí¬\n",
      "\n",
      "3. ì‹œí—˜ ë°œì‚¬ (ì ìˆ˜: 14.2)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.413\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ì‹œí—˜ ë°œì‚¬, ë™í•´ ë°œì‚¬\n",
      "\n",
      "4. ì˜ìƒ ê³µê°œ (ì ìˆ˜: 10.2)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.413\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ì˜ìƒ ê³µê°œ, ì‚¬ì§„ ê³µê°œ\n",
      "\n",
      "5. ì „ìŸ íŒŒë³‘ (ì ìˆ˜: 7.7)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.603\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ìš°í¬ë¼ì´ë‚˜ ì „ìŸ íŒŒë³‘, ì „ìŸ íŒŒë³‘\n",
      "\n",
      "6. ì´ˆìŒì† ë¯¸ì‚¬ì¼ (ì ìˆ˜: 7.6)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.59\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ì´ˆìŒì† ë¯¸ì‚¬ì¼ ë°œì‚¬, ì´ˆìŒì† ë¯¸ì‚¬ì¼ ì¶”ì •\n",
      "\n",
      "7. ì¼ëŒ€ ë™í•´ ë°œì‚¬ (ì ìˆ˜: 5.6)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 1\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 1.0\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ì¼ëŒ€ ë™í•´ ë°œì‚¬\n",
      "\n",
      "8. ìƒí¬ëœ ë¶í•œ í¬ë¡œ (ì ìˆ˜: 5.2)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 1\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 1.0\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ìƒí¬ëœ ë¶í•œ í¬ë¡œ\n",
      "\n",
      "ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_01_keyword_grouped.json\n",
      "âœ… ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤: 2025ë…„ 1ì›”\n",
      "\n",
      "============================================================\n",
      "ì²˜ë¦¬ ì¤‘: 2025ë…„ 2ì›”\n",
      "ì…ë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_02_keywords.json\n",
      "ì¶œë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_02_keyword_grouped.json\n",
      "============================================================\n",
      "=== ìë™ í‚¤ì›Œë“œ ê·¸ë£¹í™” ì‹œì‘ (í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ ì œê±° ë²„ì „) ===\n",
      "ì›ë³¸ í‚¤ì›Œë“œ ìˆ˜: 30\n",
      "ìœ ì‚¬ë„ ì„ê³„ê°’: 0.4\n",
      "ìµœì†Œ ê·¸ë£¹ í¬ê¸°: 2\n",
      "ìœ ì‚¬ë„ ê³„ì‚°: ìì¹´ë“œ(70%) + í¸ì§‘ê±°ë¦¬(30%)\n",
      "\n",
      "=== ê·¸ë£¹í™” ì™„ë£Œ ===\n",
      "ìµœì¢… í‚¤ì›Œë“œ ê·¸ë£¹ ìˆ˜: 12\n",
      "ì••ì¶•ë¥ : 60.0%\n",
      "\n",
      "=== ìƒìœ„ ê·¸ë£¹ë“¤ (ìˆœìˆ˜ ë¬¸ìì—´ ìœ ì‚¬ë„ ê¸°ë°˜) ===\n",
      "1. í¬ë¡œ í•œêµ­ ì†¡í™˜ (ì ìˆ˜: 6.0)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 3\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.43\n",
      "   - í†µí•© í‚¤ì›Œë“œ: í¬ë¡œ í•œêµ­ ì†¡í™˜, í¬ë¡œ í•œêµ­ ìš”ì²­, ë¶í•œ í¬ë¡œ ì†¡í™˜\n",
      "\n",
      "2. ì¶”ê°€ íŒŒë³‘ (ì ìˆ˜: 4.9)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.633\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ëŸ¬ì‹œì•„ ì¶”ê°€ íŒŒë³‘, ì¶”ê°€ íŒŒë³‘\n",
      "\n",
      "3. ì¶œë²” ì²˜ìŒ (ì ìˆ˜: 4.1)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.633\n",
      "   - í†µí•© í‚¤ì›Œë“œ: í–‰ì •ë¶€ ì¶œë²” ì²˜ìŒ, ì¶œë²” ì²˜ìŒ\n",
      "\n",
      "4. êµ°ì¸ ìƒí¬ (ì ìˆ˜: 4.0)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.417\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ë¶í•œ êµ°ì¸ ìƒí¬, êµ°ì¸ ìƒí¬ ë°íˆë‹¤\n",
      "\n",
      "5. ì˜ì§€ í™•ì¸ (ì ìˆ˜: 3.7)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.633\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ì˜ì§€ í™•ì¸, ë¹„í•µí™” ì˜ì§€ í™•ì¸\n",
      "\n",
      "6. í•œë¯¸ í˜‘ë ¥ (ì ìˆ˜: 3.3)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 1\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 1.0\n",
      "   - í†µí•© í‚¤ì›Œë“œ: í•œë¯¸ í˜‘ë ¥\n",
      "\n",
      "7. ë¹„ì„œ ì¤€ê³µ (ì ìˆ˜: 3.2)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.417\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ë¹„ì„œ ì¤€ê³µ ì‚¬ë¥´ë‹¤, ì±…ì„ ë¹„ì„œ ì¤€ê³µ\n",
      "\n",
      "8. ì„¼í„° ëª©ì  ì—´ë¦¬ë‹¤ (ì ìˆ˜: 2.8)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 1\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 1.0\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ì„¼í„° ëª©ì  ì—´ë¦¬ë‹¤\n",
      "\n",
      "ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_02_keyword_grouped.json\n",
      "âœ… ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤: 2025ë…„ 2ì›”\n",
      "\n",
      "============================================================\n",
      "ì²˜ë¦¬ ì¤‘: 2025ë…„ 3ì›”\n",
      "ì…ë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_03_keywords.json\n",
      "ì¶œë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_03_keyword_grouped.json\n",
      "============================================================\n",
      "=== ìë™ í‚¤ì›Œë“œ ê·¸ë£¹í™” ì‹œì‘ (í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ ì œê±° ë²„ì „) ===\n",
      "ì›ë³¸ í‚¤ì›Œë“œ ìˆ˜: 30\n",
      "ìœ ì‚¬ë„ ì„ê³„ê°’: 0.4\n",
      "ìµœì†Œ ê·¸ë£¹ í¬ê¸°: 2\n",
      "ìœ ì‚¬ë„ ê³„ì‚°: ìì¹´ë“œ(70%) + í¸ì§‘ê±°ë¦¬(30%)\n",
      "\n",
      "=== ê·¸ë£¹í™” ì™„ë£Œ ===\n",
      "ìµœì¢… í‚¤ì›Œë“œ ê·¸ë£¹ ìˆ˜: 12\n",
      "ì••ì¶•ë¥ : 60.0%\n",
      "\n",
      "=== ìƒìœ„ ê·¸ë£¹ë“¤ (ìˆœìˆ˜ ë¬¸ìì—´ ìœ ì‚¬ë„ ê¸°ë°˜) ===\n",
      "1. ì—°í•© í›ˆë ¨ (ì ìˆ˜: 20.5)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 4\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.565\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ì—°í•© í›ˆë ¨, í•œë¯¸ ì—°í•© í›ˆë ¨, ì—°í•© í›ˆë ¨ ë¹„ë‚œ, ì—°í•© í›ˆë ¨ ë°˜ë°œ\n",
      "\n",
      "2. íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬ (ì ìˆ˜: 5.7)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 3\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.619\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ë¶í•œ íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬, íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬, íƒ„ë„ë¯¸ì‚¬ì¼ ìˆ˜ë°œ ë°œì‚¬\n",
      "\n",
      "3. ë¶í•œ í¬ë¡œ ì†¡í™˜ (ì ìˆ˜: 5.7)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 3\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.506\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ë¶í•œ í¬ë¡œ ì†¡í™˜, ë¶í•œ í¬ë¡œ ë©´ë‹´, í¬ë¡œ ì†¡í™˜\n",
      "\n",
      "4. ìˆ˜í˜¸ ê¸°ë… (ì ìˆ˜: 4.8)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.425\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ì„œí•´ ìˆ˜í˜¸ ê¸°ë…, ìˆ˜í˜¸ ê¸°ë… ì°¸ì„\n",
      "\n",
      "5. í˜„ì§€ ì‹œê°„ (ì ìˆ˜: 4.4)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.55\n",
      "   - í†µí•© í‚¤ì›Œë“œ: í˜„ì§€ ì‹œê°„ ë°íˆë‹¤, í˜„ì§€ ì‹œê°„ ë³´ë„\n",
      "\n",
      "6. ì¶”ê°€ íŒŒë³‘ (ì ìˆ˜: 3.7)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.633\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ì¶”ê°€ íŒŒë³‘, ëŸ¬ì‹œì•„ ì¶”ê°€ íŒŒë³‘\n",
      "\n",
      "7. ê·€ìˆœ ì˜ì‚¬ ë°íˆë‹¤ (ì ìˆ˜: 2.8)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 1\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 1.0\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ê·€ìˆœ ì˜ì‚¬ ë°íˆë‹¤\n",
      "\n",
      "8. ê¹€ì •ìš± êµ­ê¸° ì¶”ë‹¤ (ì ìˆ˜: 2.8)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 1\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 1.0\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ê¹€ì •ìš± êµ­ê¸° ì¶”ë‹¤\n",
      "\n",
      "ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_03_keyword_grouped.json\n",
      "âœ… ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤: 2025ë…„ 3ì›”\n",
      "\n",
      "============================================================\n",
      "ì²˜ë¦¬ ì¤‘: 2025ë…„ 4ì›”\n",
      "ì…ë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_04_keywords.json\n",
      "ì¶œë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_04_keyword_grouped.json\n",
      "============================================================\n",
      "=== ìë™ í‚¤ì›Œë“œ ê·¸ë£¹í™” ì‹œì‘ (í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ ì œê±° ë²„ì „) ===\n",
      "ì›ë³¸ í‚¤ì›Œë“œ ìˆ˜: 30\n",
      "ìœ ì‚¬ë„ ì„ê³„ê°’: 0.4\n",
      "ìµœì†Œ ê·¸ë£¹ í¬ê¸°: 2\n",
      "ìœ ì‚¬ë„ ê³„ì‚°: ìì¹´ë“œ(70%) + í¸ì§‘ê±°ë¦¬(30%)\n",
      "\n",
      "=== ê·¸ë£¹í™” ì™„ë£Œ ===\n",
      "ìµœì¢… í‚¤ì›Œë“œ ê·¸ë£¹ ìˆ˜: 12\n",
      "ì••ì¶•ë¥ : 60.0%\n",
      "\n",
      "=== ìƒìœ„ ê·¸ë£¹ë“¤ (ìˆœìˆ˜ ë¬¸ìì—´ ìœ ì‚¬ë„ ê¸°ë°˜) ===\n",
      "1. íŒŒë³‘ ê³µì‹ (ì ìˆ˜: 35.5)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 11\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.455\n",
      "   - í†µí•© í‚¤ì›Œë“œ: íŒŒë³‘ ê³µì‹, íŒŒë³‘ ê³µì‹ í™•ì¸, íŒŒë³‘ ê³µì‹ ì¸ì •, ë¶í•œ íŒŒë³‘ ê³µì‹, ëŸ¬ì‹œì•„ íŒŒë³‘ ê³µì‹, ë¶í•œ íŒŒë³‘ ê³µì‹ ì¸ì •, íŒŒë³‘ ì¸ì •, íŒŒë³‘ ì‚¬ì‹¤ ê³µì‹, íŒŒë³‘ ì²˜ìŒ ê³µì‹, ëŸ¬ì‹œì•„ íŒŒë³‘ ê³µì‹ í™•ì¸, ì²˜ìŒ ê³µì‹\n",
      "\n",
      "2. ê³µì‹ ì¸ì • (ì ìˆ˜: 13.4)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 3\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.452\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ê³µì‹ ì¸ì •, ê³µì‹ í™•ì¸, ì²˜ìŒ ê³µì‹ ì¸ì •\n",
      "\n",
      "3. ì˜ìƒ ê³µê°œ (ì ìˆ˜: 7.7)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 3\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.603\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ì˜ìƒ ê³µê°œ, í›ˆë ¨ ì˜ìƒ ê³µê°œ, ì˜ìƒ ì²˜ìŒ ê³µê°œ\n",
      "\n",
      "4. êµ­ë¬´ìœ„ì› ê³µê°œ (ì ìˆ˜: 4.9)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.677\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ë¶í•œ êµ­ë¬´ìœ„ì› ê³µê°œ, êµ­ë¬´ìœ„ì› ê³µê°œ\n",
      "\n",
      "5. ì¡°ì„ ì†Œ ì§„í–‰ (ì ìˆ˜: 4.8)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.45\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ë‚¨í¬ ì¡°ì„ ì†Œ ì§„í–‰, ì¡°ì„ ì†Œ ì§„í–‰ ë¼ë‹¤\n",
      "\n",
      "6. ì ê·¹ ì—­í•  (ì ìˆ˜: 4.5)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.654\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ì „íˆ¬ ì ê·¹ ì—­í• , ì ê·¹ ì—­í• \n",
      "\n",
      "7. ë¶í•œ íŒŒë³‘ ì¸ì • (ì ìˆ˜: 3.2)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 1\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 1.0\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ë¶í•œ íŒŒë³‘ ì¸ì •\n",
      "\n",
      "8. ìš°í¬ë¼ì´ë‚˜ ì „ìŸ íŒŒë³‘ (ì ìˆ˜: 2.8)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 1\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 1.0\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ìš°í¬ë¼ì´ë‚˜ ì „ìŸ íŒŒë³‘\n",
      "\n",
      "ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_04_keyword_grouped.json\n",
      "âœ… ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤: 2025ë…„ 4ì›”\n",
      "\n",
      "============================================================\n",
      "ì²˜ë¦¬ ì¤‘: 2025ë…„ 5ì›”\n",
      "ì…ë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_05_keywords.json\n",
      "ì¶œë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_05_keyword_grouped.json\n",
      "============================================================\n",
      "=== ìë™ í‚¤ì›Œë“œ ê·¸ë£¹í™” ì‹œì‘ (í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ ì œê±° ë²„ì „) ===\n",
      "ì›ë³¸ í‚¤ì›Œë“œ ìˆ˜: 30\n",
      "ìœ ì‚¬ë„ ì„ê³„ê°’: 0.4\n",
      "ìµœì†Œ ê·¸ë£¹ í¬ê¸°: 2\n",
      "ìœ ì‚¬ë„ ê³„ì‚°: ìì¹´ë“œ(70%) + í¸ì§‘ê±°ë¦¬(30%)\n",
      "\n",
      "=== ê·¸ë£¹í™” ì™„ë£Œ ===\n",
      "ìµœì¢… í‚¤ì›Œë“œ ê·¸ë£¹ ìˆ˜: 12\n",
      "ì••ì¶•ë¥ : 60.0%\n",
      "\n",
      "=== ìƒìœ„ ê·¸ë£¹ë“¤ (ìˆœìˆ˜ ë¬¸ìì—´ ìœ ì‚¬ë„ ê¸°ë°˜) ===\n",
      "1. íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬ (ì ìˆ˜: 14.2)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 6\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.514\n",
      "   - í†µí•© í‚¤ì›Œë“œ: íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬, ë‹¨ê±°ë¦¬ íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬, ë™í•´ íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬, ë¶í•œ íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬, ë¯¸ì‚¬ì¼ ë°œì‚¬, íƒ„ë„ë¯¸ì‚¬ì¼ ìˆ˜ë°œ ë°œì‚¬\n",
      "\n",
      "2. ê³µì› ì§€ì • (ì ìˆ˜: 4.1)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.654\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ì§€ì§ˆ ê³µì› ì§€ì •, ê³µì› ì§€ì •\n",
      "\n",
      "3. í‰ì–‘ ë„ì°© (ì ìˆ˜: 4.1)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.654\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ì§€ë‚œ í‰ì–‘ ë„ì°©, í‰ì–‘ ë„ì°©\n",
      "\n",
      "4. ë™í•´ ë°œì‚¬ (ì ìˆ˜: 4.1)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.654\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ë™í•´ ë°œì‚¬, ì¼ëŒ€ ë™í•´ ë°œì‚¬\n",
      "\n",
      "5. í˜„ì§€ ì‹œê°„ ë³´ë„ (ì ìˆ˜: 3.2)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 1\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 1.0\n",
      "   - í†µí•© í‚¤ì›Œë“œ: í˜„ì§€ ì‹œê°„ ë³´ë„\n",
      "\n",
      "6. ê¶Œê³  íŒë‹¨ (ì ìˆ˜: 3.2)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.417\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ë“±ì¬ ê¶Œê³  íŒë‹¨, ê¶Œê³  íŒë‹¨ ë‚´ë¦¬ë‹¤\n",
      "\n",
      "7. ëª¨ìŠ¤í¬ë°” ê´‘ì¥ ì—´ë¦¬ë‹¤ (ì ìˆ˜: 2.0)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 1\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 1.0\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ëª¨ìŠ¤í¬ë°” ê´‘ì¥ ì—´ë¦¬ë‹¤\n",
      "\n",
      "8. ëŒ€ë¶ ì œì¬ ìœ„ë°˜ (ì ìˆ˜: 2.0)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 1\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 1.0\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ëŒ€ë¶ ì œì¬ ìœ„ë°˜\n",
      "\n",
      "ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_05_keyword_grouped.json\n",
      "âœ… ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤: 2025ë…„ 5ì›”\n",
      "\n",
      "============================================================\n",
      "ì²˜ë¦¬ ì¤‘: 2025ë…„ 6ì›”\n",
      "ì…ë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_06_keywords.json\n",
      "ì¶œë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_06_keyword_grouped.json\n",
      "============================================================\n",
      "=== ìë™ í‚¤ì›Œë“œ ê·¸ë£¹í™” ì‹œì‘ (í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ ì œê±° ë²„ì „) ===\n",
      "ì›ë³¸ í‚¤ì›Œë“œ ìˆ˜: 30\n",
      "ìœ ì‚¬ë„ ì„ê³„ê°’: 0.4\n",
      "ìµœì†Œ ê·¸ë£¹ í¬ê¸°: 2\n",
      "ìœ ì‚¬ë„ ê³„ì‚°: ìì¹´ë“œ(70%) + í¸ì§‘ê±°ë¦¬(30%)\n",
      "\n",
      "=== ê·¸ë£¹í™” ì™„ë£Œ ===\n",
      "ìµœì¢… í‚¤ì›Œë“œ ê·¸ë£¹ ìˆ˜: 12\n",
      "ì••ì¶•ë¥ : 60.0%\n",
      "\n",
      "=== ìƒìœ„ ê·¸ë£¹ë“¤ (ìˆœìˆ˜ ë¬¸ìì—´ ìœ ì‚¬ë„ ê¸°ë°˜) ===\n",
      "1. ì†ŒìŒ ë°©ì†¡ (ì ìˆ˜: 16.9)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 4\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.556\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ëŒ€ë‚¨ ì†ŒìŒ ë°©ì†¡, ì†ŒìŒ ë°©ì†¡, ì†ŒìŒ ë°©ì†¡ ì¤‘ë‹¨, ì†ŒìŒ ë°©ì†¡ ë©ˆì¶”ë‹¤\n",
      "\n",
      "2. í™•ì„±ê¸° ë°©ì†¡ (ì ìˆ˜: 14.1)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 3\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.594\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ëŒ€ë¶ í™•ì„±ê¸° ë°©ì†¡, í™•ì„±ê¸° ë°©ì†¡, í™•ì„±ê¸° ë°©ì†¡ ì¤‘ë‹¨\n",
      "\n",
      "3. ë„ì¤‘ ë„˜ì–´ì§€ë‹¤ (ì ìˆ˜: 6.5)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.658\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ì§„ìˆ˜ì‹ ë„ì¤‘ ë„˜ì–´ì§€ë‹¤, ë„ì¤‘ ë„˜ì–´ì§€ë‹¤\n",
      "\n",
      "4. ì›ì‚° ê°ˆë‹¤ (ì ìˆ˜: 5.3)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.654\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ì›ì‚° ê°ˆë‹¤, ë¶í•œ ì›ì‚° ê°ˆë‹¤\n",
      "\n",
      "5. ì¶”ê°€ íŒŒë³‘ (ì ìˆ˜: 4.9)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.633\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ì¶”ê°€ íŒŒë³‘, ëŸ¬ì‹œì•„ ì¶”ê°€ íŒŒë³‘\n",
      "\n",
      "6. êµ°ì‚¬ ê±´ì„¤ (ì ìˆ˜: 4.9)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.654\n",
      "   - í†µí•© í‚¤ì›Œë“œ: êµ°ì‚¬ ê±´ì„¤, ë³‘ë ¥ êµ°ì‚¬ ê±´ì„¤\n",
      "\n",
      "7. ë°©ì†¡ ì¤‘ë‹¨ (ì ìˆ˜: 3.3)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 1\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 1.0\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ë°©ì†¡ ì¤‘ë‹¨\n",
      "\n",
      "8. í˜„ì§€ ì‹œê°„ ë³´ë„ (ì ìˆ˜: 3.2)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 1\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 1.0\n",
      "   - í†µí•© í‚¤ì›Œë“œ: í˜„ì§€ ì‹œê°„ ë³´ë„\n",
      "\n",
      "ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_06_keyword_grouped.json\n",
      "âœ… ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤: 2025ë…„ 6ì›”\n",
      "\n",
      "============================================================\n",
      "ì²˜ë¦¬ ì¤‘: 2025ë…„ 7ì›”\n",
      "ì…ë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_07_keywords.json\n",
      "ì¶œë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_07_keyword_grouped.json\n",
      "============================================================\n",
      "=== ìë™ í‚¤ì›Œë“œ ê·¸ë£¹í™” ì‹œì‘ (í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ ì œê±° ë²„ì „) ===\n",
      "ì›ë³¸ í‚¤ì›Œë“œ ìˆ˜: 30\n",
      "ìœ ì‚¬ë„ ì„ê³„ê°’: 0.4\n",
      "ìµœì†Œ ê·¸ë£¹ í¬ê¸°: 2\n",
      "ìœ ì‚¬ë„ ê³„ì‚°: ìì¹´ë“œ(70%) + í¸ì§‘ê±°ë¦¬(30%)\n",
      "\n",
      "=== ê·¸ë£¹í™” ì™„ë£Œ ===\n",
      "ìµœì¢… í‚¤ì›Œë“œ ê·¸ë£¹ ìˆ˜: 12\n",
      "ì••ì¶•ë¥ : 60.0%\n",
      "\n",
      "=== ìƒìœ„ ê·¸ë£¹ë“¤ (ìˆœìˆ˜ ë¬¸ìì—´ ìœ ì‚¬ë„ ê¸°ë°˜) ===\n",
      "1. ì‹ ë³‘ í™•ë³´ (ì ìˆ˜: 10.1)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 4\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.558\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ëª…ì˜ ì‹ ë³‘ í™•ë³´, ì‹ ë³‘ í™•ë³´, ì‹ ë³‘ í™•ë³´ ë°íˆë‹¤, ì‹¤ì‹œ ì‹ ë³‘ í™•ë³´\n",
      "\n",
      "2. ê¸°ê´€ ì¡°ì‚¬ (ì ìˆ˜: 6.1)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.654\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ê´€ê³„ ê¸°ê´€ ì¡°ì‚¬, ê¸°ê´€ ì¡°ì‚¬\n",
      "\n",
      "3. êµ°ì‚¬ë¶„ê³„ì„  ë„˜ì–´ì˜¤ë‹¤ (ì ìˆ˜: 4.9)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.697\n",
      "   - í†µí•© í‚¤ì›Œë“œ: êµ°ì‚¬ë¶„ê³„ì„  ë„˜ì–´ì˜¤ë‹¤, ì „ì„  êµ°ì‚¬ë¶„ê³„ì„  ë„˜ì–´ì˜¤ë‹¤\n",
      "\n",
      "4. ì›ì‚° ê°ˆë‹¤ (ì ìˆ˜: 4.8)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 3\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.55\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ë¶í•œ ì›ì‚° ê°ˆë‹¤, ê°•ì›ë„ ì›ì‚° ê°ˆë‹¤, ë¦¬ì¡°íŠ¸ ì›ì‚° ê°ˆë‹¤\n",
      "\n",
      "5. í‘œë¥˜ êµ¬ì¡° (ì ìˆ˜: 3.7)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.654\n",
      "   - í†µí•© í‚¤ì›Œë“œ: í•´ìƒ í‘œë¥˜ êµ¬ì¡°, í‘œë¥˜ êµ¬ì¡°\n",
      "\n",
      "6. í‰ì–‘ ë°©ì–´ (ì ìˆ˜: 3.2)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 2\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 0.613\n",
      "   - í†µí•© í‚¤ì›Œë“œ: í‰ì–‘ ë°©ì–´ í™œìš©, í‰ì–‘ ë°©ì–´ ì‚¬ìš©\n",
      "\n",
      "7. ì£¼ë¯¼ ë™í•´ ì†¡í™˜ (ì ìˆ˜: 2.0)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 1\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 1.0\n",
      "   - í†µí•© í‚¤ì›Œë“œ: ì£¼ë¯¼ ë™í•´ ì†¡í™˜\n",
      "\n",
      "8. í•´ë‹¹ ì¸ì› ì‹ë³„ (ì ìˆ˜: 2.0)\n",
      "   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: 1\n",
      "   - í‰ê·  ìœ ì‚¬ë„: 1.0\n",
      "   - í†µí•© í‚¤ì›Œë“œ: í•´ë‹¹ ì¸ì› ì‹ë³„\n",
      "\n",
      "ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_07_keyword_grouped.json\n",
      "âœ… ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤: 2025ë…„ 7ì›”\n",
      "\n",
      "============================================================\n",
      "ì²˜ë¦¬ ì¤‘: 2025ë…„ 8ì›”\n",
      "ì…ë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_08_keywords.json\n",
      "ì¶œë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_08_keyword_grouped.json\n",
      "============================================================\n",
      "âŒ ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_08_keywords.json\n",
      "\n",
      "============================================================\n",
      "ì²˜ë¦¬ ì¤‘: 2025ë…„ 9ì›”\n",
      "ì…ë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_09_keywords.json\n",
      "ì¶œë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_09_keyword_grouped.json\n",
      "============================================================\n",
      "âŒ ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_09_keywords.json\n",
      "\n",
      "============================================================\n",
      "ì²˜ë¦¬ ì¤‘: 2025ë…„ 10ì›”\n",
      "ì…ë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_10_keywords.json\n",
      "ì¶œë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_10_keyword_grouped.json\n",
      "============================================================\n",
      "âŒ ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_10_keywords.json\n",
      "\n",
      "============================================================\n",
      "ì²˜ë¦¬ ì¤‘: 2025ë…„ 11ì›”\n",
      "ì…ë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_11_keywords.json\n",
      "ì¶œë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_11_keyword_grouped.json\n",
      "============================================================\n",
      "âŒ ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_11_keywords.json\n",
      "\n",
      "============================================================\n",
      "ì²˜ë¦¬ ì¤‘: 2025ë…„ 12ì›”\n",
      "ì…ë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_12_keywords.json\n",
      "ì¶œë ¥ íŒŒì¼: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster/2025_12_keyword_grouped.json\n",
      "============================================================\n",
      "âŒ ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results/2025_12_keywords.json\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ 2025ë…„ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!\n",
      "âœ… ì„±ê³µ: 7ê°œ íŒŒì¼\n",
      "âŒ ì‹¤íŒ¨: 5ê°œ íŒŒì¼\n",
      "ğŸ“Š ì„±ê³µë¥ : 58.3%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "class KeywordGrouper:\n",
    "    \"\"\"ìœ ì‚¬ë„ ê¸°ë°˜ í‚¤ì›Œë“œ ìë™ ê·¸ë£¹í™” í´ë˜ìŠ¤ - í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ ì œê±° ë²„ì „\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # important_keywords ì œê±° - ìˆœìˆ˜ ë¬¸ìì—´ ìœ ì‚¬ë„ë§Œ ì‚¬ìš©\n",
    "        pass\n",
    "    \n",
    "    def jaccard_similarity(self, str1: str, str2: str) -> float:\n",
    "        \"\"\"ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚° (ë‹¨ì–´ ì§‘í•© ê¸°ë°˜)\"\"\"\n",
    "        set1 = set(str1.split())\n",
    "        set2 = set(str2.split())\n",
    "        \n",
    "        intersection = set1.intersection(set2)\n",
    "        union = set1.union(set2)\n",
    "        \n",
    "        return len(intersection) / len(union) if len(union) > 0 else 0\n",
    "    \n",
    "    def levenshtein_distance(self, str1: str, str2: str) -> float:\n",
    "        \"\"\"í¸ì§‘ ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ë„ (ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬)\"\"\"\n",
    "        if len(str1) == 0:\n",
    "            return len(str2)\n",
    "        if len(str2) == 0:\n",
    "            return len(str1)\n",
    "        \n",
    "        # ë™ì  í”„ë¡œê·¸ë˜ë° ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±\n",
    "        matrix = [[0] * (len(str1) + 1) for _ in range(len(str2) + 1)]\n",
    "        \n",
    "        # ì²« ë²ˆì§¸ í–‰ê³¼ ì—´ ì´ˆê¸°í™”\n",
    "        for i in range(len(str2) + 1):\n",
    "            matrix[i][0] = i\n",
    "        for j in range(len(str1) + 1):\n",
    "            matrix[0][j] = j\n",
    "        \n",
    "        # ë§¤íŠ¸ë¦­ìŠ¤ ì±„ìš°ê¸°\n",
    "        for i in range(1, len(str2) + 1):\n",
    "            for j in range(1, len(str1) + 1):\n",
    "                if str2[i-1] == str1[j-1]:\n",
    "                    matrix[i][j] = matrix[i-1][j-1]\n",
    "                else:\n",
    "                    matrix[i][j] = min(\n",
    "                        matrix[i-1][j-1] + 1,  # substitution\n",
    "                        matrix[i][j-1] + 1,    # insertion\n",
    "                        matrix[i-1][j] + 1     # deletion\n",
    "                    )\n",
    "        \n",
    "        max_len = max(len(str1), len(str2))\n",
    "        return 1 - (matrix[len(str2)][len(str1)] / max_len) if max_len > 0 else 1\n",
    "    \n",
    "    def calculate_similarity(self, phrase1: str, phrase2: str) -> float:\n",
    "        \"\"\"ìˆœìˆ˜ ë¬¸ìì—´ ìœ ì‚¬ë„ ê³„ì‚° - í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ ì œê±°\"\"\"\n",
    "        # ìì¹´ë“œ ìœ ì‚¬ë„ (ë‹¨ì–´ ê²¹ì¹¨)\n",
    "        jaccard_score = self.jaccard_similarity(phrase1, phrase2)\n",
    "        \n",
    "        # í¸ì§‘ ê±°ë¦¬ ìœ ì‚¬ë„ (ë¬¸ìì—´ ìœ ì‚¬ì„±)\n",
    "        levenshtein_score = self.levenshtein_distance(phrase1, phrase2)\n",
    "        \n",
    "        # ê°€ì¤‘ í‰ê·  (ìì¹´ë“œ 70%, í¸ì§‘ê±°ë¦¬ 30%)\n",
    "        return jaccard_score * 0.7 + levenshtein_score * 0.3\n",
    "    \n",
    "    def auto_group_keywords(self, keywords: List[Dict], similarity_threshold: float = 0.4, \n",
    "                           min_group_size: int = 2) -> List[Dict]:\n",
    "        \"\"\"í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•œ ìë™ ê·¸ë£¹í™”\"\"\"\n",
    "        groups = []\n",
    "        processed = set()\n",
    "        \n",
    "        for i in range(len(keywords)):\n",
    "            if i in processed:\n",
    "                continue\n",
    "            \n",
    "            current_group = [keywords[i]]\n",
    "            current_group_indices = [i]\n",
    "            processed.add(i)\n",
    "            \n",
    "            # í˜„ì¬ í‚¤ì›Œë“œì™€ ìœ ì‚¬í•œ í‚¤ì›Œë“œë“¤ ì°¾ê¸°\n",
    "            for j in range(i + 1, len(keywords)):\n",
    "                if j in processed:\n",
    "                    continue\n",
    "                \n",
    "                similarity = self.calculate_similarity(\n",
    "                    keywords[i]['phrase'], \n",
    "                    keywords[j]['phrase']\n",
    "                )\n",
    "                \n",
    "                if similarity >= similarity_threshold:\n",
    "                    current_group.append(keywords[j])\n",
    "                    current_group_indices.append(j)\n",
    "                    processed.add(j)\n",
    "            \n",
    "            # ê·¸ë£¹ ì •ë³´ ì €ì¥\n",
    "            groups.append({\n",
    "                'keywords': current_group,\n",
    "                'indices': current_group_indices,\n",
    "                'avg_similarity': self.calculate_group_average_similarity(current_group)\n",
    "            })\n",
    "        \n",
    "        return groups\n",
    "    \n",
    "    def calculate_group_average_similarity(self, group_keywords: List[Dict]) -> float:\n",
    "        \"\"\"ê·¸ë£¹ ë‚´ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°\"\"\"\n",
    "        if len(group_keywords) < 2:\n",
    "            return 1.0\n",
    "        \n",
    "        total_similarity = 0\n",
    "        pair_count = 0\n",
    "        \n",
    "        for i in range(len(group_keywords)):\n",
    "            for j in range(i + 1, len(group_keywords)):\n",
    "                total_similarity += self.calculate_similarity(\n",
    "                    group_keywords[i]['phrase'],\n",
    "                    group_keywords[j]['phrase']\n",
    "                )\n",
    "                pair_count += 1\n",
    "        \n",
    "        return total_similarity / pair_count if pair_count > 0 else 1.0\n",
    "    \n",
    "    def select_representative_keyword(self, group_keywords: List[Dict]) -> Dict:\n",
    "        \"\"\"ê·¸ë£¹ ëŒ€í‘œ í‚¤ì›Œë“œ ì„ ì • (ê°€ì¥ ë†’ì€ ì ìˆ˜)\"\"\"\n",
    "        return max(group_keywords, key=lambda k: k['score'])\n",
    "    \n",
    "    def find_common_words(self, phrases: List[str]) -> List[str]:\n",
    "        \"\"\"ê³µí†µ ë‹¨ì–´ ì°¾ê¸°\"\"\"\n",
    "        word_counts = Counter()\n",
    "        min_occurrence = math.ceil(len(phrases) * 0.6)  # 60% ì´ìƒ ì¶œí˜„\n",
    "        \n",
    "        for phrase in phrases:\n",
    "            words = phrase.split()\n",
    "            for word in words:\n",
    "                word_counts[word] += 1\n",
    "        \n",
    "        common_words = [word for word, count in word_counts.items() \n",
    "                       if count >= min_occurrence]\n",
    "        \n",
    "        # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "        return sorted(common_words, key=lambda w: word_counts[w], reverse=True)\n",
    "    \n",
    "    def generate_group_name(self, group_keywords: List[Dict]) -> str:\n",
    "        \"\"\"ê·¸ë£¹ëª… ìƒì„± - ì›ë˜ ë‹¨ì–´ ìˆœì„œ ìœ ì§€\"\"\"\n",
    "        representative = self.select_representative_keyword(group_keywords)\n",
    "        phrases = [k['phrase'] for k in group_keywords]\n",
    "        \n",
    "        # ê³µí†µ ë‹¨ì–´ ì¶”ì¶œ\n",
    "        common_words = self.find_common_words(phrases)\n",
    "        \n",
    "        if common_words and len(common_words) > 1:\n",
    "            # ëŒ€í‘œ í‚¤ì›Œë“œì—ì„œ ê³µí†µ ë‹¨ì–´ë“¤ì˜ ìˆœì„œ ì°¾ê¸°\n",
    "            rep_words = representative['phrase'].split()\n",
    "            ordered_common = []\n",
    "            \n",
    "            # ëŒ€í‘œ í‚¤ì›Œë“œì˜ ë‹¨ì–´ ìˆœì„œëŒ€ë¡œ ê³µí†µ ë‹¨ì–´ ë°°ì—´\n",
    "            for word in rep_words:\n",
    "                if word in common_words:\n",
    "                    ordered_common.append(word)\n",
    "            \n",
    "            # ë¹ ì§„ ê³µí†µ ë‹¨ì–´ë“¤ ì¶”ê°€\n",
    "            for word in common_words:\n",
    "                if word not in ordered_common:\n",
    "                    ordered_common.append(word)\n",
    "            \n",
    "            if ordered_common:\n",
    "                return ' '.join(ordered_common)\n",
    "        \n",
    "        # ê³µí†µ ë‹¨ì–´ê°€ ì—†ê±°ë‚˜ 1ê°œë©´ ëŒ€í‘œ í‚¤ì›Œë“œ ì‚¬ìš©\n",
    "        return representative['phrase']\n",
    "    \n",
    "    def smart_group_keywords(self, keyword_data: Dict, **options) -> Dict:\n",
    "        \"\"\"ë©”ì¸ ê·¸ë£¹í™” í•¨ìˆ˜\"\"\"\n",
    "        # ê¸°ë³¸ ì˜µì…˜ ì„¤ì •\n",
    "        similarity_threshold = options.get('similarity_threshold', 0.4)\n",
    "        min_group_size = options.get('min_group_size', 2)\n",
    "        max_groups = options.get('max_groups', 15)\n",
    "        \n",
    "        print(\"=== ìë™ í‚¤ì›Œë“œ ê·¸ë£¹í™” ì‹œì‘ (í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ ì œê±° ë²„ì „) ===\")\n",
    "        print(f\"ì›ë³¸ í‚¤ì›Œë“œ ìˆ˜: {len(keyword_data['keywords'])}\")\n",
    "        print(f\"ìœ ì‚¬ë„ ì„ê³„ê°’: {similarity_threshold}\")\n",
    "        print(f\"ìµœì†Œ ê·¸ë£¹ í¬ê¸°: {min_group_size}\")\n",
    "        print(f\"ìœ ì‚¬ë„ ê³„ì‚°: ìì¹´ë“œ(70%) + í¸ì§‘ê±°ë¦¬(30%)\")\n",
    "        \n",
    "        # ìë™ ê·¸ë£¹í™” ìˆ˜í–‰\n",
    "        groups = self.auto_group_keywords(\n",
    "            keyword_data['keywords'], \n",
    "            similarity_threshold, \n",
    "            min_group_size\n",
    "        )\n",
    "        \n",
    "        # ê·¸ë£¹ ì •ë³´ë¥¼ ìµœì¢… í˜•íƒœë¡œ ë³€í™˜\n",
    "        grouped_keywords = []\n",
    "        \n",
    "        for group in groups:\n",
    "            representative = self.select_representative_keyword(group['keywords'])\n",
    "            group_name = self.generate_group_name(group['keywords'])\n",
    "            \n",
    "            # ì˜ˆì‹œ ë¬¸ì¥ ì¤‘ë³µ ì œê±°\n",
    "            all_examples = []\n",
    "            for keyword in group['keywords']:\n",
    "                all_examples.extend(keyword['examples'])\n",
    "            unique_examples = list(dict.fromkeys(all_examples))[:3]\n",
    "            \n",
    "            grouped_keyword = {\n",
    "                'phrase': group_name,\n",
    "                'doc_count': sum(k['doc_count'] for k in group['keywords']),\n",
    "                'score': round(sum(k['score'] for k in group['keywords']), 1),\n",
    "                'examples': unique_examples,\n",
    "                'merged_keywords': [k['phrase'] for k in group['keywords']],\n",
    "                'keyword_count': len(group['keywords']),\n",
    "                'avg_similarity': round(group['avg_similarity'], 3),\n",
    "                'representative_keyword': representative['phrase']\n",
    "            }\n",
    "            \n",
    "            grouped_keywords.append(grouped_keyword)\n",
    "        \n",
    "        # ì ìˆ˜ìˆœ ì •ë ¬\n",
    "        grouped_keywords.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        # ìµœëŒ€ ê·¸ë£¹ ìˆ˜ ì œí•œ\n",
    "        final_keywords = grouped_keywords[:max_groups]\n",
    "        \n",
    "        # ê²°ê³¼ ì¶œë ¥\n",
    "        print(f\"\\n=== ê·¸ë£¹í™” ì™„ë£Œ ===\")\n",
    "        print(f\"ìµœì¢… í‚¤ì›Œë“œ ê·¸ë£¹ ìˆ˜: {len(final_keywords)}\")\n",
    "        compression_rate = round((1 - len(final_keywords) / len(keyword_data['keywords'])) * 100, 1)\n",
    "        print(f\"ì••ì¶•ë¥ : {compression_rate}%\")\n",
    "        \n",
    "        # ìƒìœ„ ê·¸ë£¹ë“¤ ì¶œë ¥\n",
    "        print(\"\\n=== ìƒìœ„ ê·¸ë£¹ë“¤ (ìˆœìˆ˜ ë¬¸ìì—´ ìœ ì‚¬ë„ ê¸°ë°˜) ===\")\n",
    "        for i, group in enumerate(final_keywords[:8], 1):\n",
    "            print(f\"{i}. {group['phrase']} (ì ìˆ˜: {group['score']})\")\n",
    "            print(f\"   - í†µí•©ëœ í‚¤ì›Œë“œ ìˆ˜: {group['keyword_count']}\")\n",
    "            print(f\"   - í‰ê·  ìœ ì‚¬ë„: {group['avg_similarity']}\")\n",
    "            print(f\"   - í†µí•© í‚¤ì›Œë“œ: {', '.join(group['merged_keywords'])}\")\n",
    "            print()\n",
    "        \n",
    "        return {\n",
    "            'year': keyword_data['year'],\n",
    "            'month': keyword_data['month'],\n",
    "            'period': keyword_data['period'],\n",
    "            'total_articles': keyword_data['total_articles'],\n",
    "            'original_keyword_count': len(keyword_data['keywords']),\n",
    "            'grouped_keyword_count': len(final_keywords),\n",
    "            'compression_rate': compression_rate,\n",
    "            'similarity_threshold': similarity_threshold,\n",
    "            'similarity_method': 'jaccard(70%) + levenshtein(30%)',\n",
    "            'keywords': final_keywords\n",
    "        }\n",
    "    \n",
    "    def test_different_thresholds(self, keyword_data: Dict) -> None:\n",
    "        \"\"\"ë‹¤ì–‘í•œ ì„ê³„ê°’ìœ¼ë¡œ í…ŒìŠ¤íŠ¸\"\"\"\n",
    "        thresholds = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "        \n",
    "        print(\"=== ë‹¤ì–‘í•œ ìœ ì‚¬ë„ ì„ê³„ê°’ í…ŒìŠ¤íŠ¸ (í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ ì œê±° ë²„ì „) ===\")\n",
    "        for threshold in thresholds:\n",
    "            result = self.smart_group_keywords(\n",
    "                keyword_data, \n",
    "                similarity_threshold=threshold,\n",
    "                min_group_size=2\n",
    "            )\n",
    "            \n",
    "            print(f\"ì„ê³„ê°’ {threshold}: {result['original_keyword_count']} â†’ \"\n",
    "                  f\"{result['grouped_keyword_count']} ({result['compression_rate']}% ì••ì¶•)\")\n",
    "        print()\n",
    "    \n",
    "    def load_json_file(self, file_path: str) -> Dict:\n",
    "        \"\"\"JSON íŒŒì¼ ë¡œë“œ\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "            return None\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"JSON íŒŒì‹± ì˜¤ë¥˜: {file_path}\")\n",
    "            return None\n",
    "    \n",
    "    def save_grouped_results(self, result: Dict, output_path: str) -> None:\n",
    "        \"\"\"ê·¸ë£¹í™” ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        try:\n",
    "            # ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            \n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "\n",
    "# ìë™í™”ëœ ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜ë“¤\n",
    "def process_single_file(grouper: KeywordGrouper, year: int, month: int, base_input_dir: str, base_output_dir: str, **options):\n",
    "    \"\"\"ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬\"\"\"\n",
    "    # ì…ë ¥ íŒŒì¼ ê²½ë¡œ ìƒì„±\n",
    "    input_file = os.path.join(base_input_dir, f\"{year}_{month:02d}_keywords.json\")\n",
    "    \n",
    "    # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ìƒì„±\n",
    "    output_file = os.path.join(base_output_dir, f\"{year}_{month:02d}_keyword_grouped.json\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ì²˜ë¦¬ ì¤‘: {year}ë…„ {month}ì›”\")\n",
    "    print(f\"ì…ë ¥ íŒŒì¼: {input_file}\")\n",
    "    print(f\"ì¶œë ¥ íŒŒì¼: {output_file}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"âŒ ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_file}\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # íŒŒì¼ ë¡œë“œ\n",
    "        data = grouper.load_json_file(input_file)\n",
    "        if data is None:\n",
    "            return False\n",
    "        \n",
    "        # ê·¸ë£¹í™” ìˆ˜í–‰\n",
    "        result = grouper.smart_group_keywords(data, **options)\n",
    "        \n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        grouper.save_grouped_results(result, output_file)\n",
    "        \n",
    "        print(f\"âœ… ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤: {year}ë…„ {month}ì›”\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def process_year_batch(year: int, months: List[int] = None, **options):\n",
    "    \"\"\"ì—°ê°„ ë°°ì¹˜ ì²˜ë¦¬\"\"\"\n",
    "    if months is None:\n",
    "        months = list(range(1, 13))  # 1ì›”ë¶€í„° 12ì›”ê¹Œì§€\n",
    "    \n",
    "    # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •\n",
    "    base_input_dir = f'/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results'\n",
    "    base_output_dir = f'/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/re_monthly_results_cluster'\n",
    "    \n",
    "    # ì‚¬ìš©ì ì •ì˜ ê²½ë¡œê°€ ìˆìœ¼ë©´ ì ìš©\n",
    "    base_input_dir = options.pop('input_dir', base_input_dir)\n",
    "    base_output_dir = options.pop('output_dir', base_output_dir)\n",
    "    \n",
    "    grouper = KeywordGrouper()\n",
    "    \n",
    "    print(f\"ğŸš€ {year}ë…„ í‚¤ì›Œë“œ ê·¸ë£¹í™” ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘\")\n",
    "    print(f\"ğŸ“‚ ì…ë ¥ ë””ë ‰í† ë¦¬: {base_input_dir}\")\n",
    "    print(f\"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {base_output_dir}\")\n",
    "    print(f\"ğŸ“… ì²˜ë¦¬ ì›”: {months}\")\n",
    "    print(f\"âš™ï¸ ì˜µì…˜: {options}\")\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for month in months:\n",
    "        success = process_single_file(\n",
    "            grouper, year, month, base_input_dir, base_output_dir, **options\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    \n",
    "    # ìµœì¢… ê²°ê³¼ ì¶œë ¥\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ‰ {year}ë…„ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "    print(f\"âœ… ì„±ê³µ: {successful}ê°œ íŒŒì¼\")\n",
    "    print(f\"âŒ ì‹¤íŒ¨: {failed}ê°œ íŒŒì¼\")\n",
    "    print(f\"ğŸ“Š ì„±ê³µë¥ : {successful/(successful+failed)*100:.1f}%\" if (successful+failed) > 0 else \"0%\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "\n",
    "def process_multiple_years(years: List[int], months: List[int] = None, **options):\n",
    "    \"\"\"ë‹¤ë…„ë„ ë°°ì¹˜ ì²˜ë¦¬\"\"\"\n",
    "    print(f\"ğŸ¯ ë‹¤ë…„ë„ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {years}\")\n",
    "    \n",
    "    for year in years:\n",
    "        try:\n",
    "            process_year_batch(year, months, **options)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {year}ë…„ ì²˜ë¦¬ ì¤‘ ì „ì²´ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    \n",
    "    print(f\"ğŸ ëª¨ë“  ë…„ë„ ì²˜ë¦¬ ì™„ë£Œ: {years}\")\n",
    "\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ ë° ë©”ì¸ ì‹¤í–‰ë¶€\n",
    "def main():\n",
    "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
    "    grouper = KeywordGrouper()\n",
    "    \n",
    "    # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)\n",
    "    sample_data = {\n",
    "        \"year\": 2024,\n",
    "        \"month\": 1,\n",
    "        \"period\": \"2024-01-01 ~ 2024-01-31\",\n",
    "        \"total_articles\": 487,\n",
    "        \"keywords\": [\n",
    "            {\n",
    "                \"phrase\": \"ì‹œí—˜ ë°œì‚¬\",\n",
    "                \"doc_count\": 51,\n",
    "                \"score\": 18.9,\n",
    "                \"examples\": [\"ì˜ˆì‹œ1\", \"ì˜ˆì‹œ2\", \"ì˜ˆì‹œ3\"]\n",
    "            },\n",
    "            {\n",
    "                \"phrase\": \"íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬\",\n",
    "                \"doc_count\": 40,\n",
    "                \"score\": 14.5,\n",
    "                \"examples\": [\"ì˜ˆì‹œ4\", \"ì˜ˆì‹œ5\", \"ì˜ˆì‹œ6\"]\n",
    "            },\n",
    "            {\n",
    "                \"phrase\": \"ìˆœí•­ë¯¸ì‚¬ì¼ ë°œì‚¬\",\n",
    "                \"doc_count\": 28,\n",
    "                \"score\": 9.7,\n",
    "                \"examples\": [\"ì˜ˆì‹œ7\", \"ì˜ˆì‹œ8\", \"ì˜ˆì‹œ9\"]\n",
    "            },\n",
    "            {\n",
    "                \"phrase\": \"êµ°ì‚¬ í˜‘ë ¥\",\n",
    "                \"doc_count\": 27,\n",
    "                \"score\": 9.3,\n",
    "                \"examples\": [\"ì˜ˆì‹œ10\", \"ì˜ˆì‹œ11\", \"ì˜ˆì‹œ12\"]\n",
    "            },\n",
    "            {\n",
    "                \"phrase\": \"í•´ìƒ ì‚¬ê²©\",\n",
    "                \"doc_count\": 25,\n",
    "                \"score\": 8.5,\n",
    "                \"examples\": [\"ì˜ˆì‹œ13\", \"ì˜ˆì‹œ14\", \"ì˜ˆì‹œ15\"]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"=== ìë™í™”ëœ í‚¤ì›Œë“œ ê·¸ë£¹í™” ì‹œìŠ¤í…œ ===\")\n",
    "    print(\"ğŸ“ ë³€ê²½ ì‚¬í•­:\")\n",
    "    print(\"- âŒ important_keywords ì œê±°\")\n",
    "    print(\"- âŒ keyword_bonus ì œê±°\") \n",
    "    print(\"- âœ… ìì¹´ë“œ(70%) + í¸ì§‘ê±°ë¦¬(30%) ìœ ì‚¬ë„ë§Œ ì‚¬ìš©\")\n",
    "    print(\"- ğŸš€ ìë™í™”ëœ ë°°ì¹˜ ì²˜ë¦¬ ê¸°ëŠ¥ ì¶”ê°€\")\n",
    "    print()\n",
    "    \n",
    "    # print(\"ğŸ”§ ì‚¬ìš©ë²•:\")\n",
    "    # print(\"1. ë‹¨ì¼ ë…„ë„ ì²˜ë¦¬:\")\n",
    "    # print(\"   process_year_batch(2024)\")\n",
    "    # print()\n",
    "    # print(\"2. íŠ¹ì • ì›”ë§Œ ì²˜ë¦¬:\")\n",
    "    # print(\"   process_year_batch(2024, [1, 2, 3])\")\n",
    "    # print()\n",
    "    # print(\"3. ë‹¤ë…„ë„ ì²˜ë¦¬:\")\n",
    "    # print(\"   process_multiple_years([2023, 2024])\")\n",
    "    # print()\n",
    "    # print(\"4. ì‚¬ìš©ì ì •ì˜ ì˜µì…˜:\")\n",
    "    # print(\"   process_year_batch(2024,\")\n",
    "    # print(\"       similarity_threshold=0.3,\")\n",
    "    # print(\"       min_group_size=2,\")\n",
    "    # print(\"       max_groups=12,\")\n",
    "    # print(\"       input_dir='/custom/input/path',\")\n",
    "    # print(\"       output_dir='/custom/output/path')\")\n",
    "    # print()\n",
    "    \n",
    "    return grouper, sample_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    grouper, sample_data = main()\n",
    "    \n",
    "    # ğŸ¯ ì‹¤ì œ ì²˜ë¦¬ ì‹œì‘ - ì•„ë˜ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”\n",
    "    \n",
    "    # ë°©ë²• 1: 2024ë…„ ì „ì²´ ì²˜ë¦¬ (1ì›”~12ì›”)\n",
    "    process_year_batch(\n",
    "        year=2025,\n",
    "        similarity_threshold=0.4,\n",
    "        min_group_size=2,\n",
    "        max_groups=12\n",
    "    )\n",
    "    \n",
    "    # ë°©ë²• 2: 2023ë…„ ì „ì²´ ì²˜ë¦¬ (1ì›”~12ì›”) - ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©\n",
    "    # process_year_batch(\n",
    "    #     year=2023,\n",
    "    #     similarity_threshold=0.4,\n",
    "    #     min_group_size=2,\n",
    "    #     max_groups=12\n",
    "    # )\n",
    "    \n",
    "    # ë°©ë²• 3: 2023ë…„ê³¼ 2024ë…„ ëª¨ë‘ ì²˜ë¦¬ - ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©\n",
    "    # process_multiple_years(\n",
    "    #     years=[2023, 2024],\n",
    "    #     similarity_threshold=0.4,\n",
    "    #     min_group_size=2,\n",
    "    #     max_groups=12\n",
    "    # )\n",
    "    \n",
    "    # ë°©ë²• 4: íŠ¹ì • ì›”ë§Œ ì²˜ë¦¬ - ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©\n",
    "    # process_year_batch(\n",
    "    #     year=2024,\n",
    "    #     months=[10, 11, 12],  # 10ì›”, 11ì›”, 12ì›”ë§Œ ì²˜ë¦¬\n",
    "    #     similarity_threshold=0.4,\n",
    "    #     min_group_size=2,\n",
    "    #     max_groups=12\n",
    "    # )\n",
    "    \n",
    "    # ë°©ë²• 5: ì‚¬ìš©ì ì •ì˜ ê²½ë¡œë¡œ ì²˜ë¦¬ - ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©\n",
    "    # process_year_batch(\n",
    "    #     year=2024,\n",
    "    #     input_dir='/custom/input/directory',\n",
    "    #     output_dir='/custom/output/directory',\n",
    "    #     similarity_threshold=0.4,\n",
    "    #     min_group_size=2,\n",
    "    #     max_groups=12\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c60f061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ”ï¸ ê¸°ì¡´ TF-IDF ë²¡í„°ë¼ì´ì € íŒŒì¼ '/home/ds4_sia_nolb/#FINAL_POLARIS/04_Event_top10/idf_vectorizer_for_all_corpus.pkl'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.\n",
      "\n",
      "âœ”ï¸ ë‚ ì§œê°€ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì§€ì • ê¸°ê°„ ë‚´ ëª¨ë“  ê¸°ì‚¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ (2024-01-01~2024-02-01): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80810/80810 [00:01<00:00, 80230.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 80810ê°œì˜ ê¸°ì‚¬ ì¤‘ ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ 510ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ ìˆ˜: 510ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í–‰ë™ ë™ì‚¬/í–‰ìœ„ëª…ì‚¬ í•™ìŠµ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 510/510 [00:17<00:00, 29.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ê²°ê³¼: í–‰ë™ë™ì‚¬ 121ê°œ, í–‰ìœ„ëª…ì‚¬ 144ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì‚¬ê±´ êµ¬ ìë™ ì¶”ì¶œ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 510/510 [00:14<00:00, 35.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF ì½”í¼ìŠ¤: 510ê°œ ë¬¸ì„œ\n",
      "TF-IDF ìš©ì–´ìˆ˜: 340845\n",
      "\n",
      "=== ê¸°ê°„ë³„ 'ì‚¬ê±´ êµ¬' TOP 10 (TF-IDF + DF ê²°í•©) ===\n",
      "1. ì‹œí—˜ ë°œì‚¬   (ë¬¸ì„œìˆ˜=51)\n",
      "2. íƒ„ë„ë¯¸ì‚¬ì¼ ë°œì‚¬   (ë¬¸ì„œìˆ˜=38)\n",
      "3. êµ°ì‚¬ í˜‘ë ¥   (ë¬¸ì„œìˆ˜=27)\n",
      "4. ìˆœí•­ë¯¸ì‚¬ì¼ ë°œì‚¬   (ë¬¸ì„œìˆ˜=25)\n",
      "5. í•´ìƒ ì‚¬ê²©   (ë¬¸ì„œìˆ˜=24)\n",
      "6. ê²°ì˜ ìœ„ë°˜   (ë¬¸ì„œìˆ˜=23)\n",
      "7. ë¯¸ì‚¬ì¼ ë°œì‚¬   (ë¬¸ì„œìˆ˜=22)\n",
      "8. ìš°í¬ë¼ì´ë‚˜ ê³µê²© ì‚¬ìš©   (ë¬¸ì„œìˆ˜=18)\n",
      "9. ì•ˆë³´ë¦¬ ê²°ì˜ ìœ„ë°˜   (ë¬¸ì„œìˆ˜=18)\n",
      "10. ë°œì‚¬ ë°íˆë‹¤   (ë¬¸ì„œìˆ˜=21)\n"
     ]
    }
   ],
   "source": [
    "# # ì´ë²¤íŠ¸ í‚¤ì›Œë“œ ì¶”ì¶œí•˜ëŠ” í•µì‹¬ ë¡œì§\n",
    "\n",
    "# import json\n",
    "# import os\n",
    "# import re\n",
    "# from tqdm import tqdm\n",
    "# from collections import Counter, defaultdict\n",
    "# from datetime import datetime\n",
    "# from konlpy.tag import Okt\n",
    "# import numpy as np\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import joblib\n",
    "\n",
    "# # =========================\n",
    "# # ì„¤ì •\n",
    "# # =========================\n",
    "# file_path = '/home/ds4_sia_nolb/#FINAL_POLARIS/03_Bert_summarization/final_preprocessing_data/final_preprocessing.json'\n",
    "# output_file_path = '/home/ds4_sia_nolb/#FINAL_POLARIS/04_Event_top10/top10_file.json'\n",
    "# # TF-IDF ë²¡í„°ë¼ì´ì € ë¶ˆëŸ¬ì˜¤ê¸° ë° ì €ì¥ ê²½ë¡œ (ì „ì²´ ì½”í¼ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½)\n",
    "# TFIDF_VECTORIZER_PATH = '/home/ds4_sia_nolb/#FINAL_POLARIS/04_Event_top10/idf_vectorizer_for_all_corpus.pkl'\n",
    "\n",
    "# # =========================\n",
    "# # í˜•íƒœì†Œ ë¶„ì„ê¸°\n",
    "# # =========================\n",
    "# okt = Okt()\n",
    "\n",
    "# # =========================\n",
    "# # ë‚ ì§œ íŒŒì„œ (ì—¬ëŸ¬ í¬ë§· í—ˆìš©)\n",
    "# # =========================\n",
    "# def parse_date_flexible(s: str):\n",
    "#     if not s or not isinstance(s, str):\n",
    "#         return None\n",
    "#     s = s.strip()\n",
    "\n",
    "#     candidates = [s]\n",
    "#     if \"T\" in s:\n",
    "#         candidates.append(s[:19])\n",
    "#         candidates.append(s[:10])\n",
    "#     if len(s) >= 10:\n",
    "#         candidates.append(s[:10])\n",
    "#     if \"-\" not in s and \".\" not in s and \"/\" not in s and len(s) == 8:\n",
    "#         candidates.append(f\"{s[:4]}-{s[4:6]}-{s[6:8]}\")\n",
    "\n",
    "#     fmts = [\n",
    "#         \"%Y-%m-%d\",\n",
    "#         \"%Y-%m-%d %H:%M:%S\",\n",
    "#         \"%Y-%m-%d %H:%M\",\n",
    "#         \"%Y/%m/%d\",\n",
    "#         \"%Y/%m/%d %H:%M:%S\",\n",
    "#         \"%Y.%m.%d\",\n",
    "#         \"%Y.%m.%d %H:%M:%S\",\n",
    "#         \"%Y.%m.%d %H:%M\",\n",
    "#         \"%Y%m%d\",\n",
    "#         \"%Y-%m-%dT%H:%M:%S\",\n",
    "#     ]\n",
    "\n",
    "#     for cand in candidates:\n",
    "#         for fmt in fmts:\n",
    "#             try:\n",
    "#                 return datetime.strptime(cand, fmt)\n",
    "#             except Exception:\n",
    "#                 pass\n",
    "#     return None\n",
    "\n",
    "# def parse_date(date_str: str) -> datetime:\n",
    "#     d = parse_date_flexible(date_str)\n",
    "#     if d is None:\n",
    "#         raise ValueError(f\"ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {date_str}\")\n",
    "#     return d\n",
    "\n",
    "# def extract_pubdate(article):\n",
    "#     keys = [\"pubDate\", \"pubdate\", \"time\", \"date\", \"published\", \"pub_date\"]\n",
    "#     for k in keys:\n",
    "#         if k in article and article[k]:\n",
    "#             dt = parse_date_flexible(str(article[k]))\n",
    "#             if dt:\n",
    "#                 return dt\n",
    "#     meta = article.get(\"metadata\", {}) or {}\n",
    "#     for k in keys:\n",
    "#         if k in meta and meta[k]:\n",
    "#             dt = parse_date_flexible(str(meta[k]))\n",
    "#             if dt:\n",
    "#                 return dt\n",
    "#     return None\n",
    "\n",
    "# # =========================\n",
    "# # ê¸°ì‚¬ ë¡œë“œ ë° í•„í„°ë§ ìœ í‹¸\n",
    "# # =========================\n",
    "# def load_all_articles(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#             data = json.load(f)\n",
    "#         if not isinstance(data, list):\n",
    "#             raise ValueError(\"JSON ë£¨íŠ¸ëŠ” list ì—¬ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "#         return data\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "#         return None\n",
    "#     except json.JSONDecodeError as e:\n",
    "#         print(f\"JSON íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "#         return None\n",
    "\n",
    "# def filter_articles_by_period(articles, start_date_str, end_date_str):\n",
    "#     sdt = parse_date(start_date_str)\n",
    "#     edt = parse_date(end_date_str)\n",
    "    \n",
    "#     period_articles = []\n",
    "    \n",
    "#     all_articles_count = len(articles)\n",
    "    \n",
    "#     for article in tqdm(articles, desc=f\"ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ({sdt.date()}~{edt.date()})\"):\n",
    "#         pub_date = extract_pubdate(article)\n",
    "#         if pub_date and sdt <= pub_date <= edt:\n",
    "#             period_articles.append(article)\n",
    "    \n",
    "#     print(f\"ì´ {all_articles_count}ê°œì˜ ê¸°ì‚¬ ì¤‘ ì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ {len(period_articles)}ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "#     return period_articles\n",
    "\n",
    "# # =========================\n",
    "# # í…ìŠ¤íŠ¸ ì •ê·œí™” (í‘œê¸° í†µì¼ ì•½ê°„)\n",
    "# # =========================\n",
    "# def normalize_text(t: str) -> str:\n",
    "#     if not t:\n",
    "#         return \"\"\n",
    "#     t = t.replace(\"íƒ„ë„ ë¯¸ì‚¬ì¼\", \"íƒ„ë„ë¯¸ì‚¬ì¼\")\n",
    "#     t = t.replace(\"ìˆœí•­ ë¯¸ì‚¬ì¼\", \"ìˆœí•­ë¯¸ì‚¬ì¼\")\n",
    "#     t = t.replace(\"ê·¹ì´ˆ ìŒì†\", \"ê·¹ì´ˆìŒì†\")\n",
    "#     t = t.replace(\"ì´ˆëŒ€í˜• ë°©ì‚¬í¬\", \"ì´ˆëŒ€í˜•ë°©ì‚¬í¬\")\n",
    "#     return t\n",
    "\n",
    "# # =========================\n",
    "# # ë¶ˆìš©ì–´ & ë‰´ìŠ¤ ë…¸ì´ì¦ˆ\n",
    "# # =========================\n",
    "# BASE_STOP = set([\n",
    "#     'ê°€','ê°„','ê°™ì€','ê°™ì´','ê²ƒ','ê²Œë‹¤ê°€','ê²°êµ­','ê³§','ê´€í•˜ì—¬','ê´€ë ¨','ê´€í•œ','ê·¸','ê·¸ê²ƒ','ê·¸ë…€','ê·¸ë“¤',\n",
    "#     'ê·¸ë¦¬ê³ ','ê·¸ë•Œ','ê·¸ë˜','ê·¸ë˜ì„œ','ê·¸ëŸ¬ë‚˜','ê·¸ëŸ¬ë¯€ë¡œ','ê·¸ëŸ¬í•œ','ê·¸ëŸ°','ê·¸ë ‡ê²Œ','ê·¸ì™¸','ê·¼ê±°ë¡œ','ê¸°íƒ€',\n",
    "#     'ê¹Œì§€ë„','ê¹Œì§€','ë‚˜','ë‚¨ë“¤','ë„ˆ','ëˆ„êµ¬','ë‹¤','ë‹¤ê°€','ë‹¤ë¥¸','ë‹¤ë§Œ','ë‹¤ì†Œ','ë‹¤ìˆ˜','ë‹¤ì‹œ','ë‹¤ìŒ','ë‹¨','ë‹¨ì§€',\n",
    "#     'ë‹¹ì‹ ','ëŒ€','ëŒ€í•´ì„œ','ë”êµ°ë‹¤ë‚˜','ë”êµ¬ë‚˜','ë”ë¼ë„','ë”ìš±ì´','ë„','ë„ë¡œ','ë˜','ë˜ëŠ”','ë˜í•œ','ë•Œ','ë•Œë¬¸',\n",
    "#     'ë¼ë„','ë¼ë©´','ë¼ëŠ”','ë¡œ','ë¡œë¶€í„°','ë¡œì¨','ë¥¼','ë§ˆì €','ë§ˆì¹˜','ë§Œì•½','ë§Œì¼','ë§Œí¼','ëª¨ë‘','ë¬´ì—‡','ë¬´ìŠ¨',\n",
    "#     'ë¬´ì²™','ë¬¼ë¡ ','ë°','ë°–ì—','ë°”ë¡œ','ë³´ë‹¤','ë¿ì´ë‹¤','ì‚¬ëŒ','ì‚¬ì‹¤ì€','ìƒëŒ€ì ìœ¼ë¡œ','ìƒê°','ì„¤ë ¹','ì†Œìœ„','ìˆ˜',\n",
    "#     'ìˆ˜ì¤€','ì‰½ê²Œ','ì‹œëŒ€','ì‹œì‘í•˜ì—¬','ì‹¤ë¡œ','ì‹¤ì œ','ì•„ë‹ˆ','ì•„ë¬´','ì•„ë¬´ë„','ì•„ë¬´ë¦¬','ì•„ë§ˆë„','ì•„ìš¸ëŸ¬','ì•„ì§',\n",
    "#     'ì•ì—ì„œ','ì•ìœ¼ë¡œ','ì–´ëŠ','ì–´ë–¤','ì–´ë–»ê²Œ','ì–´ë””','ì–¸ì œ','ì–¼ë§ˆë‚˜','ì—¬ê¸°','ì—¬ë¶€','ì—­ì‹œ','ì˜ˆ','ì˜¤íˆë ¤',\n",
    "#     'ì™€','ì™œ','ì™¸ì—ë„','ìš”','ìš°ë¦¬','ìš°ì„ ','ì›ë˜','ìœ„í•´ì„œ','ìœ¼ë¡œ','ìœ¼ë¡œë¶€í„°','ìœ¼ë¡œì¨','ì„','ì˜','ì˜ê±°í•˜ì—¬',\n",
    "#     'ì˜ì§€í•˜ì—¬','ì˜í•´','ì˜í•´ì„œ','ì˜í•˜ì—¬','ì´','ì´ê²ƒ','ì´ê³³','ì´ë•Œ','ì´ë¼ê³ ','ì´ëŸ¬í•œ','ì´ëŸ°','ì´ë ‡ê²Œ','ì´ì œ',\n",
    "#     'ì´ì§€ë§Œ','ì´í›„','ì´ìƒ','ì´ë‹¤','ì´ì „','ì¸','ì¼','ì¼ë‹¨','ì¼ë°˜ì ìœ¼ë¡œ','ì„ì‹œë¡œ','ì…ì¥ì—ì„œ','ì','ìê¸°','ìì‹ ',\n",
    "#     'ì ì‹œ','ì €','ì €ê²ƒ','ì €ê¸°','ì €ìª½','ì €í¬','ì „ë¶€','ì „í˜€','ì ì—ì„œ','ì •ë„','ì œ','ì¡°ê¸ˆ','ì¢€','ì£¼ë¡œ','ì£¼ì œ','ì¦‰',\n",
    "#     'ì¦‰ì‹œ','ì§€ê¸ˆ','ì§„ì§œë¡œ','ì°¨ë¼ë¦¬','ì°¸','ì°¸ìœ¼ë¡œ','ì²«ë²ˆì§¸ë¡œ','ìµœê³ ','ìµœëŒ€','ìµœì†Œ','ìµœì‹ ','ìµœì´ˆ','í†µí•˜ì—¬',\n",
    "#     'í†µí•´ì„œ','í‰ê°€','í¬í•¨í•œ','í¬í•¨í•˜ì—¬','í•˜ì§€ë§Œ','í•˜ë©´ì„œ','í•˜ì—¬','í•œ','í•œë•Œ','í•œë²ˆ','í• ','í• ê²ƒì´ë‹¤','í• ìˆ˜ìˆë‹¤',\n",
    "#     'í•¨ê»˜','í•´ë„', 'ë¼ë‹¤', 'ì„œë‹¤', 'ëŒ€í•´', 'ë‚˜ì˜¤ë‹¤', 'í†µí•´', 'ë§ë‹¤', \n",
    "# ])\n",
    "\n",
    "# NEWS_STOP = {\"ê¸°ì\",\"ì—°í•©ë‰´ìŠ¤\",\"ì‚¬ì§„\",\"ì†ë³´\",\"ì¢…í•©\",\"ìë£Œ\",\"ì˜ìƒ\",\"ë‹¨ë…\",\"ì „ë¬¸\",\"ì¸í„°ë·°\",\"ë¸Œë¦¬í•‘\"}\n",
    "\n",
    "# # =========================\n",
    "# # ì—”í„°í‹° ë…¸ì´ì¦ˆ\n",
    "# # =========================\n",
    "# ENTITY_NOISE = {\n",
    "#     \"ë¶í•œ\",\"í•œêµ­\",\"ëŒ€í•œë¯¼êµ­\",\"ë‚¨í•œ\",\"ë¯¸êµ­\",\"ì¤‘êµ­\",\"ì¼ë³¸\",\"ëŸ¬ì‹œì•„\",\"ìš°í¬ë¼ì´ë‚˜\",\"ìœ ì—”\",\"ë‚˜í† \",\"NATO\",\"EU\",\"ìœ ëŸ½ì—°í•©\",\n",
    "#     \"í‘¸í‹´\",\"ë¸”ë¼ë””ë¯¸ë¥´ í‘¸í‹´\",\"ë°”ì´ë“ \",\"ì¡° ë°”ì´ë“ \",\"ì‹œì§„í•‘\",\"ê¹€ì •ì€\",\"ê¹€ì—¬ì •\",\"ë¬¸ì¬ì¸\",\"ìœ¤ì„ì—´\",\"ì‡¼ì´êµ¬\",\"ì ¤ë ŒìŠ¤í‚¤\",\"í†µì‹ \",\"ì¤‘ì•™\",\"ë³´ë„\"\n",
    "# }\n",
    "\n",
    "# # =========================\n",
    "# # í† í°/í…ìŠ¤íŠ¸\n",
    "# # =========================\n",
    "# def pos_tokens(text: str):\n",
    "#     text = normalize_text(text or \"\")\n",
    "#     return okt.pos(text, norm=True, stem=True)\n",
    "\n",
    "# def doc_text(a) -> str:\n",
    "#     return normalize_text(f\"{a.get('title','')} {a.get('summary','')}\")\n",
    "\n",
    "# def tokenizer_for_vectorizer(s: str):\n",
    "#     toks = []\n",
    "#     for w, t in okt.pos(s, norm=True, stem=True):\n",
    "#         if t not in (\"Noun\", \"Verb\"):\n",
    "#             continue\n",
    "#         if len(w) <= 1:\n",
    "#             continue\n",
    "#         if w in BASE_STOP or w in NEWS_STOP:\n",
    "#             continue\n",
    "#         if w.isdigit():\n",
    "#             continue\n",
    "#         toks.append(w)\n",
    "#     return toks\n",
    "\n",
    "# # =========================\n",
    "# # ìë™ í•™ìŠµ: 'í–‰ë™ ë™ì‚¬'ì™€ 'í–‰ìœ„ ëª…ì‚¬'\n",
    "# # =========================\n",
    "# def learn_action_lexicons(articles, min_df_ratio_verbs=0.002, min_df_ratio_nouns=0.002):\n",
    "#     verb_doc_df = Counter()\n",
    "#     action_noun_df = Counter()\n",
    "#     N_docs = len(articles)\n",
    "\n",
    "#     for a in tqdm(articles, desc=\"í–‰ë™ ë™ì‚¬/í–‰ìœ„ëª…ì‚¬ í•™ìŠµ ì¤‘\"):\n",
    "#         title = a.get('title','') or ''\n",
    "#         summary = a.get('summary','') or ''\n",
    "#         p = pos_tokens(f\"{title} {summary}\")\n",
    "\n",
    "#         verbs_in_doc = set()\n",
    "#         action_nouns_in_doc = set()\n",
    "\n",
    "#         for i, (w, t) in enumerate(p):\n",
    "#             if t == \"Verb\":\n",
    "#                 verbs_in_doc.add(w)\n",
    "#             if t == \"Noun\":\n",
    "#                 ahead = [p[j][0] for j in range(i+1, min(i+3, len(p)))]\n",
    "#                 if \"í•˜ë‹¤\" in ahead or \"ë˜ë‹¤\" in ahead:\n",
    "#                     if w not in BASE_STOP and len(w) > 1:\n",
    "#                         action_nouns_in_doc.add(w)\n",
    "\n",
    "#         for v in verbs_in_doc:\n",
    "#             verb_doc_df[v] += 1\n",
    "#         for n in action_nouns_in_doc:\n",
    "#             action_noun_df[n] += 1\n",
    "\n",
    "#     min_df_verbs = max(5, int(N_docs * min_df_ratio_verbs))\n",
    "#     min_df_nouns = max(5, int(N_docs * min_df_ratio_nouns))\n",
    "\n",
    "#     drop_verbs = {\"í•˜ë‹¤\",\"ë˜ë‹¤\",\"ì´ë‹¤\",\"ìˆë‹¤\"}\n",
    "#     verb_set = {v for v,df in verb_doc_df.items() if df >= min_df_verbs and v not in drop_verbs}\n",
    "#     action_nouns = {n for n,df in action_noun_df.items() if df >= min_df_nouns}\n",
    "\n",
    "#     print(f\"í•™ìŠµ ê²°ê³¼: í–‰ë™ë™ì‚¬ {len(verb_set)}ê°œ, í–‰ìœ„ëª…ì‚¬ {len(action_nouns)}ê°œ\")\n",
    "#     return verb_set, action_nouns\n",
    "\n",
    "# def nominalize_verb(v: str) -> str:\n",
    "#     if v.endswith(\"í•˜ë‹¤\"):\n",
    "#         return v[:-2]\n",
    "#     if v.endswith(\"ë˜ë‹¤\"):\n",
    "#         return v[:-2]\n",
    "#     return v\n",
    "\n",
    "# # =========================\n",
    "# # ì‚¬ê±´ êµ¬ í›„ë³´ ìƒì„± + TF-IDF ê²°í•© ë­í‚¹\n",
    "# # =========================\n",
    "# def extract_event_phrases_auto(articles, top_k=20, vectorizer=None):\n",
    "#     N = len(articles)\n",
    "#     if N == 0:\n",
    "#         print(\"âš  ì§€ì • ê¸°ê°„ì— ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "#         return []\n",
    "\n",
    "#     print(f\"\\nì§€ì • ê¸°ê°„ ë‚´ ê¸°ì‚¬ ìˆ˜: {N}ê°œ\")\n",
    "    \n",
    "#     verb_set, action_nouns = learn_action_lexicons(articles)\n",
    "\n",
    "#     phrase_df = Counter()\n",
    "#     phrase_examples = defaultdict(list)\n",
    "\n",
    "#     for a in tqdm(articles, desc=\"ì‚¬ê±´ êµ¬ ìë™ ì¶”ì¶œ ì¤‘\"):\n",
    "#         title = a.get('title','') or ''\n",
    "#         summary = a.get('summary','') or ''\n",
    "#         p = pos_tokens(f\"{title} {summary}\")\n",
    "#         phrases_in_doc = set()\n",
    "\n",
    "#         prev_nouns = []\n",
    "#         L = len(p)\n",
    "#         for i, (w, t) in enumerate(p):\n",
    "#             if t == \"Noun\":\n",
    "#                 if w not in BASE_STOP and len(w) > 1:\n",
    "#                     prev_nouns.append(w)\n",
    "#                     if len(prev_nouns) > 5:\n",
    "#                         prev_nouns = prev_nouns[-5:]\n",
    "\n",
    "#             if t == \"Verb\" and w in verb_set:\n",
    "#                 vnom = nominalize_verb(w)\n",
    "#                 nn = [n for n in reversed(prev_nouns)][:2]\n",
    "#                 if nn:\n",
    "#                     phrases_in_doc.add(f\"{nn[0]} {vnom}\".strip())\n",
    "#                     if len(nn) >= 2:\n",
    "#                         phrases_in_doc.add(f\"{nn[1]} {nn[0]} {vnom}\".strip())\n",
    "#                 else:\n",
    "#                     phrases_in_doc.add(vnom.strip())\n",
    "\n",
    "#             if t == \"Noun\" and w in action_nouns:\n",
    "#                 nn = [n for n in reversed(prev_nouns) if n != w][:2]\n",
    "#                 base = w\n",
    "#                 if nn:\n",
    "#                     phrases_in_doc.add(f\"{nn[0]} {base}\".strip())\n",
    "#                     if len(nn) >= 2:\n",
    "#                         phrases_in_doc.add(f\"{nn[1]} {nn[0]} {base}\".strip())\n",
    "#                 else:\n",
    "#                     phrases_in_doc.add(base.strip())\n",
    "\n",
    "#                 if i+1 < L and p[i+1][1] == \"Noun\" and p[i+1][0] in action_nouns:\n",
    "#                     tail = p[i+1][0]\n",
    "#                     if nn:\n",
    "#                         phrases_in_doc.add(f\"{nn[0]} {base} {tail}\".strip())\n",
    "#                         if len(nn) >= 2:\n",
    "#                             phrases_in_doc.add(f\"{nn[1]} {nn[0]} {base} {tail}\".strip())\n",
    "#                     else:\n",
    "#                         phrases_in_doc.add(f\"{base} {tail}\".strip())\n",
    "\n",
    "#         cleaned = set()\n",
    "#         for ph in phrases_in_doc:\n",
    "#             ph = re.sub(r\"\\s+\", \" \", ph).strip()\n",
    "#             if len(ph.split()) == 1 and len(ph) <= 2:\n",
    "#                 continue\n",
    "#             cleaned.add(ph)\n",
    "\n",
    "#         for ph in cleaned:\n",
    "#             phrase_df[ph] += 1\n",
    "#             if len(phrase_examples[ph]) < 3 and title:\n",
    "#                 phrase_examples[ph].append(title)\n",
    "\n",
    "#     if vectorizer is None:\n",
    "#         print(\"[ì˜¤ë¥˜] TfidfVectorizer ê°ì²´ê°€ ì „ë‹¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "#         return []\n",
    "\n",
    "#     corpus_period = [doc_text(a) for a in articles]\n",
    "#     Xp = vectorizer.transform(corpus_period)\n",
    "#     tfidf_avg = np.asarray(Xp.mean(axis=0)).ravel()\n",
    "#     terms = vectorizer.get_feature_names_out()\n",
    "#     tfidf_dict = {terms[i]: float(tfidf_avg[i]) for i in np.where(tfidf_avg > 0)[0]}\n",
    "\n",
    "#     print(f\"TF-IDF ì½”í¼ìŠ¤: {len(corpus_period)}ê°œ ë¬¸ì„œ\")\n",
    "#     print(f\"TF-IDF ìš©ì–´ìˆ˜: {len(terms)}\")\n",
    "\n",
    "#     def is_entity_only(ph: str) -> bool:\n",
    "#         toks = ph.split()\n",
    "#         if len(toks) <= 2 and any(ent in ph for ent in ENTITY_NOISE):\n",
    "#             return True\n",
    "#         ent_hits = sum(1 for t in toks if any(ent in t for ent in ENTITY_NOISE))\n",
    "#         return (ent_hits >= max(1, len(toks) - 1))\n",
    "\n",
    "#     def generic_penalty(ph: str) -> int:\n",
    "#         generic = {\"ëŒ€í†µë ¹\",\"ìœ„ì›ì¥\",\"ì •ë¶€\",\"ë‹¹êµ­\",\"ê´€ê³„ì\",\"ëŒ€ë³€ì¸\",\"íšŒì˜\",\"ë…¼ì˜\",\"ê°•ì¡°\"}\n",
    "#         return -sum(1 for t in ph.split() if t in generic)\n",
    "\n",
    "#     def phrase_score(ph: str, df_cnt: int) -> float:\n",
    "#         tfidf = tfidf_dict.get(ph, 0.0)\n",
    "#         score = 0.6 * tfidf + 0.4 * float(df_cnt)\n",
    "\n",
    "#         if is_entity_only(ph):\n",
    "#             score -= 6.0\n",
    "#         score += generic_penalty(ph)\n",
    "#         if len(ph.split()) <= 2:\n",
    "#             score -= 1.5\n",
    "#         return score\n",
    "\n",
    "#     scored = []\n",
    "#     for ph, cnt in phrase_df.items():\n",
    "#         if is_entity_only(ph):\n",
    "#             continue\n",
    "#         scored.append( (ph, cnt, phrase_score(ph, cnt)) )\n",
    "\n",
    "#     scored.sort(key=lambda x: (x[2], x[1]), reverse=True)\n",
    "#     ranked = [(ph, cnt, phrase_examples.get(ph, [])) for ph, cnt, _ in scored[:top_k]]\n",
    "#     return ranked\n",
    "\n",
    "# # =========================\n",
    "# # ì „ì²´ ì½”í¼ìŠ¤ìš© TF-IDF ë²¡í„°ë¼ì´ì € ì‚¬ì „ í•™ìŠµ\n",
    "# # =========================\n",
    "# def pre_train_vectorizer(articles, save_path):\n",
    "#     if os.path.exists(save_path):\n",
    "#         print(f\"âœ”ï¸ ê¸°ì¡´ TF-IDF ë²¡í„°ë¼ì´ì € íŒŒì¼ '{save_path}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "#         return joblib.load(save_path)\n",
    "    \n",
    "#     print(f\"ğŸ” ì „ì²´ ì½”í¼ìŠ¤ìš© TF-IDF ë²¡í„°ë¼ì´ì €ë¥¼ ìƒˆë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "#     full_corpus = [doc_text(a) for a in articles]\n",
    "    \n",
    "#     vectorizer = TfidfVectorizer(\n",
    "#         tokenizer=tokenizer_for_vectorizer,\n",
    "#         ngram_range=(1, 3),\n",
    "#         min_df=5,\n",
    "#         max_df=0.85,\n",
    "#         sublinear_tf=True,\n",
    "#         norm='l2'\n",
    "#     )\n",
    "#     vectorizer.fit(full_corpus)\n",
    "#     joblib.dump(vectorizer, save_path)\n",
    "#     print(f\"âœ… ì „ì²´ ì½”í¼ìŠ¤ ê¸°ë°˜ TF-IDF ë²¡í„°ë¼ì´ì €ë¥¼ '{save_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "#     return vectorizer\n",
    "\n",
    "# # =========================\n",
    "# # ì‹¤í–‰ë¶€\n",
    "# # =========================\n",
    "# if __name__ == '__main__':\n",
    "#     try:\n",
    "#         # 1. ì „ì²´ ê¸°ì‚¬ ë°ì´í„°ë¥¼ í•œ ë²ˆë§Œ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "#         all_articles = load_all_articles(file_path)\n",
    "#         if not all_articles:\n",
    "#             print(\"ì „ì²´ ì½”í¼ìŠ¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "#             exit()\n",
    "\n",
    "#         # 2. ì „ì²´ ì½”í¼ìŠ¤ ë°ì´í„°ë¥¼ ì´ìš©í•´ TF-IDF ë²¡í„°ë¼ì´ì €ë¥¼ í•™ìŠµ/ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "#         vectorizer = pre_train_vectorizer(all_articles, TFIDF_VECTORIZER_PATH)\n",
    "        \n",
    "#         # 3. ë‚ ì§œë¥¼ ì‚¬ìš©ìì—ê²Œ ì…ë ¥ë°›ìŠµë‹ˆë‹¤.\n",
    "#         start = input(\"ì‹œì‘ì¼ ì…ë ¥ (YYYY-MM-DD ë˜ëŠ” YYYYMMDD): \").strip()\n",
    "#         end = input(\"ì¢…ë£Œì¼ ì…ë ¥ (YYYY-MM-DD ë˜ëŠ” YYYYMMDD): \").strip()\n",
    "\n",
    "#         print(\"\\nâœ”ï¸ ë‚ ì§œê°€ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì§€ì • ê¸°ê°„ ë‚´ ëª¨ë“  ê¸°ì‚¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "        \n",
    "#         # 4. ë¡œë“œëœ ì „ì²´ ë°ì´í„°ì—ì„œ ì§€ì • ê¸°ê°„ ê¸°ì‚¬ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.\n",
    "#         period_articles = filter_articles_by_period(all_articles, start, end)\n",
    "        \n",
    "#         if not period_articles:\n",
    "#             print(\"ë¶„ì„í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "#         else:\n",
    "#             # 5. ì‚¬ê±´êµ¬ë¥¼ ì¶”ì¶œí•˜ê³  ë­í‚¹ì„ ë§¤ê¹ë‹ˆë‹¤.\n",
    "#             events = extract_event_phrases_auto(\n",
    "#                 period_articles,\n",
    "#                 top_k=10,\n",
    "#                 vectorizer=vectorizer\n",
    "#             )\n",
    "\n",
    "#             print(\"\\n=== ê¸°ê°„ë³„ 'ì‚¬ê±´ êµ¬' TOP 10 (TF-IDF + DF ê²°í•©) ===\")\n",
    "#             if not events:\n",
    "#                 print(\"í•´ë‹¹ ê¸°ê°„ì— ì¶”ì¶œëœ ì‚¬ê±´ êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "#             else:\n",
    "#                 for i, (ph, cnt, examples) in enumerate(events, 1):\n",
    "#                     ex_str = \" | ì˜ˆì‹œ: \" + \" / \".join(examples) if examples else \"\"\n",
    "#                     print(f\"{i}. {ph}   (ë¬¸ì„œìˆ˜={cnt}){ex_str}\")\n",
    "    \n",
    "#     except ValueError as e:\n",
    "#         print(f\"ì˜¤ë¥˜: {e}. ì˜¬ë°”ë¥¸ ë‚ ì§œ í˜•ì‹ì„ ì…ë ¥í•´ì£¼ì„¸ìš”. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
