{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ece1b7",
   "metadata": {},
   "source": [
    "# 핵심이슈 추출의 성능평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c54c279",
   "metadata": {},
   "source": [
    "## 1. 정성적 평가 \n",
    "**숫자나 수치로 나타내기 어려운 부분을, 사람의 주관이나 판단을 바탕으로 평가**\n",
    "- 월별 기사 샘플 ( 약 3~5건 임의로 추출 ) 요약본을 직접 읽고 TF-IDF가 정말 핵심이슈인지 확인\n",
    "- 사람이 보기에 어색하거나 의미 없는 단어가 상위에 있다면 불용어 리스트 재정비\n",
    "- 주관적인 평가가 될 수 있지만 팀원들이 적당히 괜찮다 생각하면 정성적 평가로 들어갈 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5084d412",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5313da4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 키워드 추출 검증 프로세스를 시작합니다.\n",
      "📂 데이터 로딩 중...\n",
      "✅ 총 80810개의 기사를 로드했습니다.\n",
      "🔄 기사별 키워드 추출을 진행합니다...\n",
      "전체 80810개 기사 중 10개 랜덤 샘플링\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "기사별 키워드 추출 중: 100%|██████████| 10/10 [00:01<00:00,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 결과를 './keyword_extraction_validation_results.csv'에 저장했습니다.\n",
      "\n",
      "📊 추출 결과 통계:\n",
      "- 총 처리된 기사 수: 10\n",
      "- 키워드가 추출된 기사 수: 10\n",
      "- 키워드가 추출되지 않은 기사 수: 0\n",
      "- 평균 키워드 개수: 4.80\n",
      "\n",
      "📋 전체 추출 결과:\n",
      "\n",
      "================================================================================\n",
      "📰 기사 1\n",
      "제목: \n",
      "요약: 또 다른 분석업체 TRM 랩스는 북한이 지난해 해킹으로 강탈한 수천만달러 규모의 가상화폐 가치가 최근 몇 주 사이 80∼85% 폭락해 현재 1천만달러(약 130억원)도 안 되는 것으로 추정했다. 지난달 시작된 가상화폐 가치의 갑작스러운 급락으로 북한이 해킹 등으로 현금을 마련하는 능력이 훼손되고 무기 개발 프로그램에 자금을 조달하는 계획도 영향을 받았을 것...\n",
      "🔑 추출된 키워드: 계획 영향 받다 | 해킹 자원 쏟다 | 지적 재무부 따르다 | 자원 쏟다 | 재무부 따르다\n",
      "키워드 개수: 5개\n",
      "\n",
      "================================================================================\n",
      "📰 기사 2\n",
      "제목: \n",
      "요약: 아베 신조 일본 총리는 비핵화에 대한 분명한 언급이 선행돼야 한다는 기존의 입장을 재확인하면서 북미 정상회담에 앞서 4월 미일 정상간 대화를 제안했습니다. 일본 정부는 북미 정상회담 발표에에 일단 환영의 뜻을 표하면서도 경계심를 드러냈습니다. 그는 \"비핵화 논의를 시작할 수 있는 북한의 변화를 높이 평가한다\"면서도, 급변하는 한반도 정세에서 일본이 배제되는...\n",
      "🔑 추출된 키워드: 대한 우려 시키다 | 분석 비핵화 앞세우다 | 입장 확인 겁니다 | 진전 경계 드러내다 | 일본 총리 이르다\n",
      "키워드 개수: 5개\n",
      "\n",
      "================================================================================\n",
      "📰 기사 3\n",
      "제목: \n",
      "요약: 北 \"초대형방사포 시험발사\"…추가발사 예고 북한이 어제(10일) 쏜 발사체가 '초대형 방사포'였다고 밝혔습니다. 북한은 발사체 발사 다음 날 '초대형 방사포'를 시험했다고 밝혔습니다. 지난달 24일에 이어, 이번엔 내륙을 관통하는 식으로 또다시 쏘아 올린 것입니다. <조선중앙TV> \"초대형방사포시험사격은 시험사격 목적에 완전부합되었으며 무기체계완성의 다음 ...\n",
      "🔑 추출된 키워드: 완전 부합 되어다 | 완성 단계 이르다 | 초대형 방사포 밝히다 | 북한 어제 쏘다 | 뚜렷 결정 짓다\n",
      "키워드 개수: 5개\n",
      "\n",
      "================================================================================\n",
      "📰 기사 4\n",
      "제목: \n",
      "요약: 통일부, 행사 엿새 남겨두고 北엔 아직 안 알려…\"적절 시점에 통지 계획\" 통일부·서울시·경기도 공동주최…韓·美·中·日 4개국 아티스트 참여 정부와 지방자치단체가 4·27 판문점 선언 1주년 기념행사를 개최할 예정이지만, 북측의 참여 여부가 확정되지 않아 '반쪽'행사가 될 가능성을 배제하기 어렵게 됐다. 통일부 당국자는 \"이번 행사에 대해 북측에 적절한 시...\n",
      "🔑 추출된 키워드: 행사 가능성 열다 | 행사 엿새 남기다 | 참여 의사 밝히다 | 행사 엿새 알다 | 행사 엿새 두다\n",
      "키워드 개수: 5개\n",
      "\n",
      "================================================================================\n",
      "📰 기사 5\n",
      "제목: \n",
      "요약: 블라디보스토크 국제공항에 착륙 중인 고려항공 투폴레프-204 여객기 북한 국적항공사인 고려항공 소속 여객기가 5년만에 처음으로 중국 상하이에 내려 주목된다고 미국의 북한전문매체인 NK뉴스가 30일 보도했다. 이 매체는 항공기 항로 추적사이트 플라이트레이더24와 플라이트어웨어 자료를 인용, 한국시간으로 전날 오후 10시 10분께 고려항공 투폴레프(Tu)-20...\n",
      "🔑 추출된 키워드: 플라이트 레이더 오다 | 평양 상하이 잇다 | 중국 상하이 내다 | 오전 북한 돌아가다 | 제트기 상하이 머무르다\n",
      "키워드 개수: 5개\n",
      "\n",
      "================================================================================\n",
      "📰 기사 6\n",
      "제목: \n",
      "요약: 2022.12.27 북한 소형무인기가 서울 상공까지 침투했는데도 격추에 실패한 군이 무인기 도발을 상정한 합동방공훈련을 하는 등 대응책을 마련하겠다고 밝혔다. 합동참모본부는 28일 국회 국방위원회의 긴급 현안 질의에서 무인기 대응 실전 교육·훈련 강화와 대응전력 조기 전력화 추진 등 후속 조처를 보고했다. 군은 전날 김승겸 합참의장 주관으로 북한 무인기 영...\n",
      "🔑 추출된 키워드: 대응 마련 밝히다 | 마련 밝히다 | 밝히다\n",
      "키워드 개수: 3개\n",
      "\n",
      "================================================================================\n",
      "📰 기사 7\n",
      "제목: \n",
      "요약: 서울대 통일평화硏 학술회의…\"경제·문화 협력해 변화 끌어내야\" 북한 회의장에 걸린 김일성 김정일 초상화 2021년 6월 20일 열린 북한 노동당 외곽조직 '사회주의여성동맹' 제7차 대회에서 대회장 전면에 걸린 김일성 주석과 김정일 국방위원장의 초상화. 김 교수에 따르면 북한은 지난해 1월 8차 당대회에서 주석단 정면에 걸려있던 김일성 주석 및 김정일 국방위...\n",
      "🔑 추출된 키워드: 국방위원장 초상화 떼다 | 회장 전면 걸리다 | 주석 정면 걸리다 | 유학 위원장 이르다 | 국제화 열망 드러내다\n",
      "키워드 개수: 5개\n",
      "\n",
      "================================================================================\n",
      "📰 기사 8\n",
      "제목: \n",
      "요약: \"미사일 발사, 최고 수뇌부 결심따라 임의 시각·장소서 진행될 것\" 베이징 북한대사관 앞 풍경 = 주중 북한대사관은 15일 문재인 정부 출범과 관련해 남북 합의를 철저히 이행하는 게 중요하다고 밝혔다. 주중 북한대사관은 이날 베이징 대사관에 일부 현재 회견을 갖고 이같이 주장했다. 그는 이어 \"시험 발사는 우리 최고 수뇌부의 결심에 따라서 임의 시각, 임의...\n",
      "🔑 추출된 키워드: 수뇌부 결심 따르다 | 일부 회견 갖다 | 현재 회견 갖다 | 합의 이행 밝히다 | 존중 이행 밝히다\n",
      "키워드 개수: 5개\n",
      "\n",
      "================================================================================\n",
      "📰 기사 9\n",
      "제목: \n",
      "요약: 북 \"극초음속미사일 시험발사…700㎞ 명중\"…김정은 불참 북한이 전날 극초음속 미사일을 시험 발사했다고 확인했다. 조선중앙통신은 6일 \"국방과학원은 1월 5일 극초음속 미사일 시험발사를 진행하였다\"라고 보도했다. 2022.1.6 통일부는 6일 북한이 전날 동해상으로 탄도미사일을 발사한 것과 관련, \"북한의 발사 의도를 어느 한 방향으로 단정하지 않고 있다\"...\n",
      "🔑 추출된 키워드: 무기 부문 대다 | 발사 배경 밝히다 | 의도 방향 않다 | 배경 밝히다 | 부문 대다\n",
      "키워드 개수: 5개\n",
      "\n",
      "================================================================================\n",
      "📰 기사 10\n",
      "제목: \n",
      "요약: 구테흐스 유엔총장, 북한 리용호 면담…\"정치적 해법 강조\" 제72차 유엔총회 참석차 미국 뉴욕을 방문 중인 리용호 북한 외무상이 안토니우 구테흐스 유엔 사무총장을 비공개 접견했습니다. 리 외무상은 현지시간 23일 오후 유엔총회 기조연설을 마친 직후 구테흐스 총장과 약 30분 간 면담했습니다. 구테흐스 총장은 리 외무상에게 한반도 긴장 고조에 우려를 표시하면...\n",
      "🔑 추출된 키워드: 총회 연설 마치다 | 답변 확인 않다 | 연설 마치다 | 확인 않다 | 비공개\n",
      "키워드 개수: 5개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 개별 기사 핵심이슈 추출 검증 코드\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "from konlpy.tag import Okt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "# =========================\n",
    "# 설정\n",
    "# =========================\n",
    "# 파일 경로 설정 (실제 경로로 수정 필요)\n",
    "file_path = '/home/ds4_sia_nolb/#FINAL_POLARIS/04_plus_preprocessing/preprocessing_final_data/final_preprocessing.json'\n",
    "output_csv_path = './keyword_extraction_validation_results.csv'\n",
    "\n",
    "# 샘플 개수 설정 (전체 데이터가 많을 경우 일부만 샘플링)\n",
    "SAMPLE_SIZE = 10  # None으로 설정하면 전체 데이터 처리\n",
    "\n",
    "# =========================\n",
    "# 형태소 분석기\n",
    "# =========================\n",
    "okt = Okt()\n",
    "\n",
    "# =========================\n",
    "# 기존 함수들 재사용\n",
    "# =========================\n",
    "def normalize_text(t: str) -> str:\n",
    "    \"\"\"텍스트 정규화\"\"\"\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    t = t.replace(\"탄도 미사일\", \"탄도미사일\")\n",
    "    t = t.replace(\"순항 미사일\", \"순항미사일\")\n",
    "    t = t.replace(\"극초 음속\", \"극초음속\")\n",
    "    t = t.replace(\"초대형 방사포\", \"초대형방사포\")\n",
    "    return t\n",
    "\n",
    "def pos_tokens(text: str):\n",
    "    \"\"\"형태소 분석\"\"\"\n",
    "    text = normalize_text(text or \"\")\n",
    "    return okt.pos(text, norm=True, stem=True)\n",
    "\n",
    "def doc_text(a) -> str:\n",
    "    \"\"\"문서 텍스트 추출\"\"\"\n",
    "    return normalize_text(f\"{a.get('title','')} {a.get('summary','')}\")\n",
    "\n",
    "# 불용어 정의\n",
    "BASE_STOP = set([\n",
    "    '가','간','같은','같이','것','게다가','결국','곧','관하여','관련','관한','그','그것','그녀','그들',\n",
    "    '그리고','그때','그래','그래서','그러나','그러므로','그러한','그런','그렇게','그외','근거로','기타',\n",
    "    '까지도','까지','나','남들','너','누구','다','다가','다른','다만','다소','다수','다시','다음','단','단지',\n",
    "    '당신','대','대해서','더군다나','더구나','더라도','더욱이','도','도로','또','또는','또한','때','때문',\n",
    "    '라도','라면','라는','로','로부터','로써','를','마저','마치','만약','만일','만큼','모두','무엇','무슨',\n",
    "    '무척','물론','및','밖에','바로','보다','뿐이다','사람','사실은','상대적으로','생각','설령','소위','수',\n",
    "    '수준','쉽게','시대','시작하여','실로','실제','아니','아무','아무도','아무리','아마도','아울러','아직',\n",
    "    '앞에서','앞으로','어느','어떤','어떻게','어디','언제','얼마나','여기','여부','역시','예','오히려',\n",
    "    '와','왜','외에도','요','우리','우선','원래','위해서','으로','으로부터','으로써','을','의','의거하여',\n",
    "    '의지하여','의해','의해서','의하여','이','이것','이곳','이때','이라고','이러한','이런','이렇게','이제',\n",
    "    '이지만','이후','이상','이다','이전','인','일','일단','일반적으로','임시로','입장에서','자','자기','자신',\n",
    "    '잠시','저','저것','저기','저쪽','저희','전부','전혀','점에서','정도','제','조금','좀','주로','주제','즉',\n",
    "    '즉시','지금','진짜로','차라리','참','참으로','첫번째로','최고','최대','최소','최신','최초','통하여',\n",
    "    '통해서','평가','포함한','포함하여','하지만','하면서','하여','한','한때','한번','할','할것이다','할수있다',\n",
    "    '함께','해도', '돼다', '서다', '대해', '나오다', '통해', '맞다', \n",
    "])\n",
    "\n",
    "NEWS_STOP = {\"기자\",\"연합뉴스\",\"사진\",\"속보\",\"종합\",\"자료\",\"영상\",\"단독\",\"전문\",\"인터뷰\",\"브리핑\"}\n",
    "\n",
    "ENTITY_NOISE = {\n",
    "    \"북한\",\"한국\",\"대한민국\",\"남한\",\"미국\",\"중국\",\"일본\",\"러시아\",\"우크라이나\",\"유엔\",\"나토\",\"NATO\",\"EU\",\"유럽연합\",\n",
    "    \"푸틴\",\"블라디미르 푸틴\",\"바이든\",\"조 바이든\",\"시진핑\",\"김정은\",\"김여정\",\"문재인\",\"윤석열\",\"쇼이구\",\"젤렌스키\",\"통신\",\"중앙\",\"보도\"\n",
    "}\n",
    "\n",
    "def tokenizer_for_vectorizer(s: str):\n",
    "    \"\"\"TF-IDF용 토크나이저\"\"\"\n",
    "    toks = []\n",
    "    for w, t in okt.pos(s, norm=True, stem=True):\n",
    "        if t not in (\"Noun\", \"Verb\"):\n",
    "            continue\n",
    "        if len(w) <= 1:\n",
    "            continue\n",
    "        if w in BASE_STOP or w in NEWS_STOP:\n",
    "            continue\n",
    "        if w.isdigit():\n",
    "            continue\n",
    "        toks.append(w)\n",
    "    return toks\n",
    "\n",
    "def learn_action_lexicons_single(article):\n",
    "    \"\"\"단일 기사에서 행동 동사와 행위 명사 추출\"\"\"\n",
    "    title = article.get('title','') or ''\n",
    "    summary = article.get('summary','') or ''\n",
    "    p = pos_tokens(f\"{title} {summary}\")\n",
    "\n",
    "    verb_set = set()\n",
    "    action_nouns = set()\n",
    "    drop_verbs = {\"하다\",\"되다\",\"이다\",\"있다\"}\n",
    "\n",
    "    for i, (w, t) in enumerate(p):\n",
    "        if t == \"Verb\" and w not in drop_verbs and len(w) > 1:\n",
    "            verb_set.add(w)\n",
    "        if t == \"Noun\" and len(w) > 1 and w not in BASE_STOP:\n",
    "            ahead = [p[j][0] for j in range(i+1, min(i+3, len(p)))]\n",
    "            if \"하다\" in ahead or \"되다\" in ahead:\n",
    "                action_nouns.add(w)\n",
    "\n",
    "    return verb_set, action_nouns\n",
    "\n",
    "def nominalize_verb(v: str) -> str:\n",
    "    \"\"\"동사 명사화\"\"\"\n",
    "    if v.endswith(\"하다\"):\n",
    "        return v[:-2]\n",
    "    if v.endswith(\"되다\"):\n",
    "        return v[:-2]\n",
    "    return v\n",
    "\n",
    "def extract_single_article_keywords(article, top_k=5):\n",
    "    \"\"\"단일 기사에서 핵심 키워드 추출\"\"\"\n",
    "    title = article.get('title','') or ''\n",
    "    summary = article.get('summary','') or ''\n",
    "    \n",
    "    if not title and not summary:\n",
    "        return []\n",
    "\n",
    "    # 행동 동사와 행위 명사 학습\n",
    "    verb_set, action_nouns = learn_action_lexicons_single(article)\n",
    "    \n",
    "    # 형태소 분석\n",
    "    p = pos_tokens(f\"{title} {summary}\")\n",
    "    \n",
    "    # 키워드 후보 추출\n",
    "    phrase_candidates = set()\n",
    "    prev_nouns = []\n",
    "    \n",
    "    for i, (w, t) in enumerate(p):\n",
    "        if t == \"Noun\":\n",
    "            if w not in BASE_STOP and len(w) > 1:\n",
    "                prev_nouns.append(w)\n",
    "                if len(prev_nouns) > 3:\n",
    "                    prev_nouns = prev_nouns[-3:]\n",
    "\n",
    "        # 동사 기반 구문 생성\n",
    "        if t == \"Verb\" and w in verb_set:\n",
    "            vnom = nominalize_verb(w)\n",
    "            if vnom and len(vnom) > 1:\n",
    "                # 동사만\n",
    "                phrase_candidates.add(vnom)\n",
    "                # 명사 + 동사\n",
    "                if prev_nouns:\n",
    "                    phrase_candidates.add(f\"{prev_nouns[-1]} {vnom}\")\n",
    "                    if len(prev_nouns) >= 2:\n",
    "                        phrase_candidates.add(f\"{prev_nouns[-2]} {prev_nouns[-1]} {vnom}\")\n",
    "\n",
    "        # 행위 명사 기반 구문 생성\n",
    "        if t == \"Noun\" and w in action_nouns:\n",
    "            # 명사만\n",
    "            phrase_candidates.add(w)\n",
    "            # 앞 명사 + 행위 명사\n",
    "            if prev_nouns and prev_nouns[-1] != w:\n",
    "                phrase_candidates.add(f\"{prev_nouns[-1]} {w}\")\n",
    "                if len(prev_nouns) >= 2:\n",
    "                    phrase_candidates.add(f\"{prev_nouns[-2]} {prev_nouns[-1]} {w}\")\n",
    "\n",
    "    # 단일 문서 TF-IDF 계산\n",
    "    doc_content = doc_text(article)\n",
    "    if not doc_content.strip():\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            tokenizer=tokenizer_for_vectorizer,\n",
    "            ngram_range=(1, 3),\n",
    "            lowercase=False\n",
    "        )\n",
    "        \n",
    "        # 단일 문서라서 TF만 계산 (IDF는 의미없음)\n",
    "        X = vectorizer.fit_transform([doc_content])\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        tfidf_scores = X.toarray()[0]\n",
    "        \n",
    "        # TF 점수 딕셔너리 생성\n",
    "        tf_dict = {feature_names[i]: tfidf_scores[i] for i in range(len(feature_names)) if tfidf_scores[i] > 0}\n",
    "        \n",
    "    except:\n",
    "        tf_dict = {}\n",
    "\n",
    "    # 엔터티 노이즈 필터링 함수\n",
    "    def is_entity_only(ph: str) -> bool:\n",
    "        toks = ph.split()\n",
    "        if len(toks) <= 2 and any(ent in ph for ent in ENTITY_NOISE):\n",
    "            return True\n",
    "        ent_hits = sum(1 for t in toks if any(ent in t for ent in ENTITY_NOISE))\n",
    "        return (ent_hits >= max(1, len(toks) - 1))\n",
    "\n",
    "    # 키워드 점수 계산\n",
    "    scored_phrases = []\n",
    "    for phrase in phrase_candidates:\n",
    "        if is_entity_only(phrase):\n",
    "            continue\n",
    "        if len(phrase.strip()) <= 2:\n",
    "            continue\n",
    "            \n",
    "        # TF 점수 가져오기\n",
    "        tf_score = tf_dict.get(phrase, 0.0)\n",
    "        \n",
    "        # 구문 길이 보너스\n",
    "        length_bonus = len(phrase.split()) * 0.1\n",
    "        \n",
    "        # 최종 점수\n",
    "        final_score = tf_score + length_bonus\n",
    "        \n",
    "        if final_score > 0:\n",
    "            scored_phrases.append((phrase, final_score))\n",
    "\n",
    "    # 점수순 정렬 후 상위 k개 반환\n",
    "    scored_phrases.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [phrase for phrase, score in scored_phrases[:top_k]]\n",
    "\n",
    "def load_articles(file_path):\n",
    "    \"\"\"기사 데이터 로드\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(\"JSON 루트는 list 여야 합니다.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"파일을 찾을 수 없습니다: {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON 파싱 오류: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"알 수 없는 오류 발생: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_articles_for_validation(articles, sample_size=None):\n",
    "    \"\"\"검증을 위한 기사 처리\"\"\"\n",
    "    if sample_size and len(articles) > sample_size:\n",
    "        print(f\"전체 {len(articles)}개 기사 중 {sample_size}개 랜덤 샘플링\")\n",
    "        # 랜덤 시드 설정 (재현 가능한 결과를 위해)\n",
    "        random.seed(42)\n",
    "        articles = random.sample(articles, sample_size)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, article in enumerate(tqdm(articles, desc=\"기사별 키워드 추출 중\")):\n",
    "        title = article.get('title', '') or ''\n",
    "        summary = article.get('summary', '') or ''\n",
    "        \n",
    "        # 제목과 요약이 모두 비어있으면 스킵\n",
    "        if not title.strip() and not summary.strip():\n",
    "            continue\n",
    "            \n",
    "        # 키워드 추출\n",
    "        keywords = extract_single_article_keywords(article, top_k=5)\n",
    "        keywords_str = ' | '.join(keywords) if keywords else '추출된 키워드 없음'\n",
    "        \n",
    "        results.append({\n",
    "            'article_id': i,\n",
    "            'title': title.strip(),\n",
    "            'summary': summary.strip(),\n",
    "            'extracted_keywords': keywords_str,\n",
    "            'num_keywords': len(keywords)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# =========================\n",
    "# 실행부\n",
    "# =========================\n",
    "def main():\n",
    "    print(\"🔍 키워드 추출 검증 프로세스를 시작합니다.\")\n",
    "    \n",
    "    # 1. 데이터 로드\n",
    "    print(\"📂 데이터 로딩 중...\")\n",
    "    articles = load_articles(file_path)\n",
    "    if not articles:\n",
    "        print(\"❌ 데이터를 로드할 수 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"✅ 총 {len(articles)}개의 기사를 로드했습니다.\")\n",
    "    \n",
    "    # 2. 검증 처리\n",
    "    print(\"🔄 기사별 키워드 추출을 진행합니다...\")\n",
    "    results = process_articles_for_validation(articles, SAMPLE_SIZE)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"❌ 처리할 수 있는 기사가 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    # 3. DataFrame으로 변환\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # 4. CSV로 저장\n",
    "    df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"💾 결과를 '{output_csv_path}'에 저장했습니다.\")\n",
    "    \n",
    "    # 5. 간단한 통계 출력\n",
    "    print(\"\\n📊 추출 결과 통계:\")\n",
    "    print(f\"- 총 처리된 기사 수: {len(results)}\")\n",
    "    print(f\"- 키워드가 추출된 기사 수: {len(df[df['num_keywords'] > 0])}\")\n",
    "    print(f\"- 키워드가 추출되지 않은 기사 수: {len(df[df['num_keywords'] == 0])}\")\n",
    "    print(f\"- 평균 키워드 개수: {df['num_keywords'].mean():.2f}\")\n",
    "    \n",
    "    # 6. 모든 결과 상세 출력 (5개만 처리하므로)\n",
    "    print(f\"\\n📋 전체 추출 결과:\")\n",
    "    for i, row in df.iterrows():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"📰 기사 {i+1}\")\n",
    "        print(f\"제목: {row['title']}\")\n",
    "        print(f\"요약: {row['summary'][:200]}{'...' if len(row['summary']) > 200 else ''}\")\n",
    "        print(f\"🔑 추출된 키워드: {row['extracted_keywords']}\")\n",
    "        print(f\"키워드 개수: {row['num_keywords']}개\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    df_results = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715cef27",
   "metadata": {},
   "source": [
    "# 다른 모델과 비교하여 보기\n",
    "\n",
    "### 어떻게 해석하면 좋을지 (발표용 포인트)\n",
    "\n",
    "- **서로 다른 알고리즘의 관점 차이**\n",
    "    - **TF-IDF**: “해당 월 문서에서 상대적으로 자주 등장하는 n-그램”을 찾아요(코퍼스 기반 빈도 가중).\n",
    "    - **YAKE**: 단일 문서(여기서는 월 코퍼스 합본) 내부 분포 특성(위치/대문자/길이/출현 분산 등)을 이용한 **언어 독립적 키프레이즈**.\n",
    "    - **KeyBERT**: 문서 임베딩과 후보구 임베딩의 코사인 유사도로 **의미적 근접성**을 보는 방식.\n",
    "- **검증/비교 지표**\n",
    "    - **자카드(구/토큰 단위)**: 결과 집합의 겹침 정도(= 일관성).\n",
    "    - **스피어만 상관**: 공통 후보들의 **순위 일관성**.\n",
    "    - **교집합 Top 목록**: 프락티컬하게 “모두가 중요하다고 보는 표현”을 빠르게 제시.\n",
    "- **권장 해석 시나리오**\n",
    "    1. **교집합 상위 키워드**는 “핵심 이슈의 신뢰 코어”로 두고,\n",
    "    2. **방법별 고유 상위 키워드**는 보완적 관점(의미 중심/빈도 중심/그래프 중심)으로 **신뢰도 보강/리드 신호 탐색**에 활용하세요.\n",
    "    3. 월별로 **겹침도가 낮아지는 구간**은 이슈 구성이 변하는 **전환점**으로 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f89329b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting krwordrank\n",
      "  Downloading krwordrank-1.0.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting yake\n",
      "  Downloading yake-0.6.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting keybert\n",
      "  Downloading keybert-0.9.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: numpy>=1.18.4 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from krwordrank) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from krwordrank) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from krwordrank) (1.7.1)\n",
      "Requirement already satisfied: click>=6.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from yake) (8.2.1)\n",
      "Collecting jellyfish (from yake)\n",
      "  Downloading jellyfish-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: networkx in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from yake) (3.5)\n",
      "Collecting segtok (from yake)\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting tabulate (from yake)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: rich>=10.4.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from keybert) (14.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from sentence-transformers) (4.56.1)\n",
      "Requirement already satisfied: tqdm in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from sentence-transformers) (2.8.0+cu128)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.9)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from rich>=10.4.0->keybert) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from rich>=10.4.0->keybert) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from scikit-learn>=0.22.1->krwordrank) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from scikit-learn>=0.22.1->krwordrank) (3.6.0)\n",
      "Requirement already satisfied: setuptools in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
      "Downloading krwordrank-1.0.3-py3-none-any.whl (20 kB)\n",
      "Downloading yake-0.6.0-py3-none-any.whl (80 kB)\n",
      "Downloading keybert-0.9.0-py3-none-any.whl (41 kB)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading jellyfish-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
      "Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate, segtok, jellyfish, yake, krwordrank, sentence-transformers, keybert\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [keybert]m5/7\u001b[0m [sentence-transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed jellyfish-1.2.0 keybert-0.9.0 krwordrank-1.0.3 segtok-1.5.11 sentence-transformers-5.1.0 tabulate-0.9.0 yake-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install krwordrank yake keybert sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f2e0f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 문서 수: 80810\n",
      "TF-IDF 벡터라이저 준비 완료\n",
      "\n",
      "======================================================================\n",
      "▶ 2024년 1월 비교 실행\n",
      "📚 2024-01 문서 수: 510\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ab97c7cbfa4fc59146700468c3eee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26d0d476547472d94558bb91b52d046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acee8d1270ef4755a681f1a9effcfee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691f5e2dace541e990458704f8b83e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1599501461464f29b3bf3dfabf1755fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6bab53436e42688a2e2977eefdc3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f6337d4a0340c3bd5b38ed40d1d0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d511334790ba47eea6222f7ef9b9c14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1b8a7b8d0645db88f889ef5ab1967a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18191198b8b445fea6dc2fe4418b2a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_01_methods_keywords.csv\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_01_full_result.json\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_01_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "▶ 2024년 2월 비교 실행\n",
      "📚 2024-02 문서 수: 377\n",
      "📄 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_02_methods_keywords.csv\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_02_full_result.json\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_02_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "▶ 2024년 3월 비교 실행\n",
      "📚 2024-03 문서 수: 350\n",
      "📄 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_03_methods_keywords.csv\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_03_full_result.json\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_03_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "▶ 2024년 4월 비교 실행\n",
      "📚 2024-04 문서 수: 311\n",
      "📄 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_04_methods_keywords.csv\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_04_full_result.json\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_04_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "▶ 2024년 5월 비교 실행\n",
      "📚 2024-05 문서 수: 373\n",
      "📄 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_05_methods_keywords.csv\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_05_full_result.json\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_05_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "▶ 2024년 6월 비교 실행\n",
      "📚 2024-06 문서 수: 493\n",
      "📄 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_06_methods_keywords.csv\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_06_full_result.json\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_06_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "▶ 2024년 7월 비교 실행\n",
      "📚 2024-07 문서 수: 389\n",
      "📄 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_07_methods_keywords.csv\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_07_full_result.json\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_07_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "▶ 2024년 8월 비교 실행\n",
      "📚 2024-08 문서 수: 281\n",
      "📄 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_08_methods_keywords.csv\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_08_full_result.json\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_08_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "▶ 2024년 9월 비교 실행\n",
      "📚 2024-09 문서 수: 444\n",
      "📄 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_09_methods_keywords.csv\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_09_full_result.json\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_09_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "▶ 2024년 10월 비교 실행\n",
      "📚 2024-10 문서 수: 1028\n",
      "📄 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_10_methods_keywords.csv\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_10_full_result.json\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_10_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "▶ 2024년 11월 비교 실행\n",
      "📚 2024-11 문서 수: 692\n",
      "📄 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_11_methods_keywords.csv\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_11_full_result.json\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_11_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "▶ 2024년 12월 비교 실행\n",
      "📚 2024-12 문서 수: 350\n",
      "📄 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_12_methods_keywords.csv\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_12_full_result.json\n",
      "💾 저장: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_12_pairwise_metrics.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "월별 키워드 추출 성능 비교 파이프라인\n",
    "- Methods: TF-IDF, TextRank(krwordrank), YAKE, KeyBERT\n",
    "- Inputs:\n",
    "    1) 전체 기사 JSON (list[dict])  — file_path (제목=metadata.title, 요약=summary)\n",
    "    2) (가능하면) 전체코퍼스 벡터라이저 pkl — TFIDF_VECTORIZER_PATH\n",
    "- Outputs (기본):\n",
    "    /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/benchmarks/\n",
    "      ├─ YYYY_MM_methods_keywords.csv                (방법별 TopK 키워드)\n",
    "      ├─ YYYY_MM_pairwise_metrics.json               (방법쌍 비교 지표)\n",
    "      └─ YYYY_MM_full_result.json                    (모든 원시 결과/지표 종합)\n",
    "\"\"\"\n",
    "\n",
    "import os, re, json, math, calendar, joblib, warnings\n",
    "from typing import List, Dict, Tuple\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# 외부 라이브러리 (설치 필요)\n",
    "from krwordrank.word import KRWordRank\n",
    "import yake\n",
    "from keybert import KeyBERT\n",
    "\n",
    "# =========================\n",
    "# 경로 및 공통 설정\n",
    "# =========================\n",
    "file_path = '/home/ds4_sia_nolb/#FINAL_POLARIS/04_plus_preprocessing/preprocessing_final_data/final_preprocessing.json'\n",
    "BENCH_OUTDIR = '/home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks'\n",
    "os.makedirs(BENCH_OUTDIR, exist_ok=True)\n",
    "\n",
    "TFIDF_VECTORIZER_PATH = '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/idf_vectorizer_for_all_corpus.pkl'\n",
    "TOP_K = 30\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# 불용어/정규화\n",
    "# =========================\n",
    "BASE_STOP = {\n",
    "    '가','간','같은','같이','것','게다가','결국','곧','관하여','관련','관한','그','그것','그녀','그들',\n",
    "    '그리고','그때','그래','그래서','그러나','그러므로','그러한','그런','그렇게','그외','근거로','기타',\n",
    "    '까지도','까지','나','남들','너','누구','다','다가','다른','다만','다소','다수','다시','다음','단','단지',\n",
    "    '당신','대','대해서','더군다나','더구나','더라도','더욱이','도','도로','또','또는','또한','때','때문',\n",
    "    '라도','라면','라는','로','로부터','로써','를','마저','마치','만약','만일','만큼','모두','무엇','무슨',\n",
    "    '무척','물론','및','밖에','바로','보다','뿐이다','사람','사실은','상대적으로','생각','설령','소위','수',\n",
    "    '수준','쉽게','시대','시작하여','실로','실제','아니','아무','아무도','아무리','아마도','아울러','아직',\n",
    "    '앞에서','앞으로','어느','어떤','어떻게','어디','언제','얼마나','여기','여부','역시','예','오히려',\n",
    "    '와','왜','외에도','요','우리','우선','원래','위해서','으로','으로부터','으로써','을','의','의거하여',\n",
    "    '의지하여','의해','의해서','의하여','이','이것','이곳','이때','이라고','이러한','이런','이렇게','이제',\n",
    "    '이지만','이후','이상','이다','이전','인','일','일단','일반적으로','임시로','입장에서','자','자기','자신',\n",
    "    '잠시','저','저것','저기','저쪽','저희','전부','전혀','점에서','정도','제','조금','좀','주로','주제','즉',\n",
    "    '즉시','지금','진짜로','차라리','참','참으로','첫번째로','최고','최대','최소','최신','최초','통하여',\n",
    "    '통해서','평가','포함한','포함하여','하지만','하면서','하여','한','한때','한번','할','할것이다','할수있다',\n",
    "    '함께','해도'\n",
    "}\n",
    "NEWS_STOP = {\"기자\",\"연합뉴스\",\"사진\",\"속보\",\"종합\",\"자료\",\"영상\",\"단독\",\"전문\",\"인터뷰\",\"브리핑\"}\n",
    "CUSTOM_STOPWORDS = BASE_STOP | NEWS_STOP\n",
    "\n",
    "ENTITY_NOISE = {\n",
    "    \"북한\",\"한국\",\"대한민국\",\"남한\",\"미국\",\"중국\",\"일본\",\"러시아\",\"우크라이나\",\"유엔\",\"나토\",\"NATO\",\"EU\",\"유럽연합\",\n",
    "    \"푸틴\",\"블라디미르 푸틴\",\"바이든\",\"조 바이든\",\"시진핑\",\"김정은\",\"김여정\",\"문재인\",\"윤석열\",\"쇼이구\",\"젤렌스키\",\"중앙\",\"통신\",\"보도\",\n",
    "    '돼다','서다','대해','나오다','통해','맞다'\n",
    "}\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    t = t.replace(\"탄도 미사일\",\"탄도미사일\").replace(\"순항 미사일\",\"순항미사일\")\n",
    "    t = t.replace(\"극초 음속\",\"극초음속\").replace(\"초대형 방사포\",\"초대형방사포\")\n",
    "    # 공백 정리\n",
    "    t = re.sub(r\"\\s+\",\" \", t).strip()\n",
    "    return t\n",
    "\n",
    "# =========================\n",
    "# 날짜/입력 로더\n",
    "# =========================\n",
    "def parse_date_flexible(s: str):\n",
    "    if not s or not isinstance(s, str):\n",
    "        return None\n",
    "    s = s.strip()\n",
    "    cands = [s]\n",
    "    if \"T\" in s:\n",
    "        cands += [s[:19], s[:10]]\n",
    "    if len(s) >= 10:\n",
    "        cands.append(s[:10])\n",
    "    if \"-\" not in s and \".\" not in s and \"/\" not in s and len(s) == 8:\n",
    "        cands.append(f\"{s[:4]}-{s[4:6]}-{s[6:8]}\")\n",
    "    fmts = [\n",
    "        \"%Y-%m-%d\",\"%Y-%m-%d %H:%M:%S\",\"%Y-%m-%d %H:%M\",\n",
    "        \"%Y/%m/%d\",\"%Y/%m/%d %H:%M:%S\",\n",
    "        \"%Y.%m.%d\",\"%Y.%m.%d %H:%M:%S\",\"%Y.%m.%d %H:%M\",\n",
    "        \"%Y%m%d\",\"%Y-%m-%dT%H:%M:%S\"\n",
    "    ]\n",
    "    for c in cands:\n",
    "        for f in fmts:\n",
    "            try:\n",
    "                return datetime.strptime(c, f)\n",
    "            except:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "def extract_pubdate(a: dict):\n",
    "    keys = [\"pubDate\",\"pubdate\",\"time\",\"date\",\"published\",\"pub_date\"]\n",
    "    for k in keys:\n",
    "        if k in a and a[k]:\n",
    "            dt = parse_date_flexible(str(a[k]))\n",
    "            if dt: return dt\n",
    "    meta = a.get(\"metadata\", {}) or {}\n",
    "    for k in keys:\n",
    "        if k in meta and meta[k]:\n",
    "            dt = parse_date_flexible(str(meta[k]))\n",
    "            if dt: return dt\n",
    "    return None\n",
    "\n",
    "def doc_text(a: dict) -> str:\n",
    "    title = (a.get('metadata') or {}).get('title','')\n",
    "    summary = a.get('summary','') or ''\n",
    "    return normalize_text(f\"{title} {summary}\")\n",
    "\n",
    "def load_all_articles(path: str) -> List[dict]:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(\"JSON 루트는 list 여야 합니다.\")\n",
    "    return data\n",
    "\n",
    "def monthly_corpus(year: int, month: int, all_articles: List[dict]) -> List[str]:\n",
    "    last_day = calendar.monthrange(year, month)[1]\n",
    "    sdt = datetime(year, month, 1)\n",
    "    edt = datetime(year, month, last_day, 23, 59, 59)\n",
    "    docs = []\n",
    "    for a in all_articles:\n",
    "        d = extract_pubdate(a)\n",
    "        if d and sdt <= d <= edt:\n",
    "            text = doc_text(a)\n",
    "            if text:\n",
    "                docs.append(text)\n",
    "    return docs\n",
    "\n",
    "# =========================\n",
    "# TF-IDF (전체 코퍼스 학습/로드)\n",
    "# =========================\n",
    "\n",
    "# 기존 pkl 호환을 위한 별칭(aliased) - 이전 이름 유지\n",
    "def tokenizer_for_vectorizer(s: str):\n",
    "    return tokenizer_simple_ko(s)\n",
    "\n",
    "def tokenizer_simple_ko(s: str) -> List[str]:\n",
    "    # 아주 간단한 토크나이저: 한글/영문/숫자 단어 기준 + 길이>=2 + 불용어 제외\n",
    "    toks = re.findall(r\"[가-힣A-Za-z0-9]+\", s)\n",
    "    out = []\n",
    "    for w in toks:\n",
    "        if len(w) <= 1: continue\n",
    "        if w in CUSTOM_STOPWORDS: continue\n",
    "        if w.isdigit(): continue\n",
    "        out.append(w)\n",
    "    return out\n",
    "\n",
    "def load_or_train_vectorizer(all_articles: List[dict], path: str) -> TfidfVectorizer:\n",
    "    if os.path.exists(path):\n",
    "        return joblib.load(path)\n",
    "    corpus = [doc_text(a) for a in all_articles]\n",
    "    vec = TfidfVectorizer(\n",
    "        tokenizer=tokenizer_simple_ko,\n",
    "        ngram_range=(1,3),\n",
    "        min_df=5,\n",
    "        max_df=0.85,\n",
    "        sublinear_tf=True,\n",
    "        norm='l2'\n",
    "    )\n",
    "    vec.fit(corpus)\n",
    "    joblib.dump(vec, path)\n",
    "    return vec\n",
    "\n",
    "def tfidf_top_phrases(docs: List[str], vectorizer: TfidfVectorizer, top_k=30) -> List[Tuple[str, float]]:\n",
    "    if not docs:\n",
    "        return []\n",
    "    X = vectorizer.transform(docs)\n",
    "    avg = np.asarray(X.mean(axis=0)).ravel()\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    pairs = [(terms[i], float(avg[i])) for i in np.where(avg>0)[0]]\n",
    "    # 엔터티 노이즈 약벌\n",
    "    def penalty(term):\n",
    "        toks = term.split()\n",
    "        ent_hits = sum(1 for t in toks if any(ent in t for ent in ENTITY_NOISE))\n",
    "        return -0.05 * ent_hits\n",
    "    scored = [(t, s + penalty(t)) for t,s in pairs]\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scored[:top_k]\n",
    "\n",
    "# =========================\n",
    "# TextRank (KRWordRank) → 단어스코어로 구(phrase) 스코어\n",
    "# =========================\n",
    "from krwordrank.word import KRWordRank\n",
    "\n",
    "def textrank_top_phrases(docs: List[str], top_k=30) -> List[Tuple[str, float]]:\n",
    "    if not docs:\n",
    "        return []\n",
    "    # 문서 수가 적으면 min_count를 낮춰야 키워드가 나옵니다.\n",
    "    min_count = 5 if len(docs) >= 100 else 2\n",
    "\n",
    "    kr = KRWordRank(min_count=min_count, max_length=10, verbose=False)\n",
    "\n",
    "    # beta(0~1): 텔레포테이션 가중, max_iter: 반복\n",
    "    try:\n",
    "        # 신버전 호환 (delta 지원)\n",
    "        keywords, rank, _ = kr.extract(docs, beta=0.85, max_iter=50, delta=0.001)\n",
    "    except TypeError:\n",
    "        # 구버전 호환 (delta 미지원)\n",
    "        keywords, rank, _ = kr.extract(docs, beta=0.85, max_iter=50)\n",
    "\n",
    "    # 1~3그램 phrase 스코어링 (단어 rank 합산)\n",
    "    phrases = Counter()\n",
    "    for text in docs:\n",
    "        words = re.findall(r\"[가-힣A-Za-z0-9]+\", text)\n",
    "        words = [w for w in words if w not in CUSTOM_STOPWORDS and len(w) > 1]\n",
    "        for n in (1, 2, 3):\n",
    "            for i in range(len(words) - n + 1):\n",
    "                ph = \" \".join(words[i:i+n])\n",
    "                # 엔터티 노이즈 과다 포함 구 제외\n",
    "                if n <= 2 and any(ent in ph for ent in ENTITY_NOISE):\n",
    "                    continue\n",
    "                score = sum(rank.get(w, 0.0) for w in words[i:i+n])\n",
    "                if score > 0:\n",
    "                    phrases[ph] += score\n",
    "\n",
    "    scored = list(phrases.items())\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scored[:top_k]\n",
    "\n",
    "# =========================\n",
    "# YAKE\n",
    "# =========================\n",
    "def yake_top_phrases(docs: List[str], top_k=30) -> List[Tuple[str, float]]:\n",
    "    if not docs:\n",
    "        return []\n",
    "    text = \"\\n\".join(docs)\n",
    "    # 낮은 점수가 더 좋음 → 1/score로 뒤집어 정렬\n",
    "    kw_extractor = yake.KeywordExtractor(\n",
    "        lan=\"ko\", n=3, # 1~3그램 자동 탐색\n",
    "        dedupLim=0.9, windowsSize=1, top=top_k*3,\n",
    "        features=None\n",
    "    )\n",
    "    candidates = kw_extractor.extract_keywords(text)\n",
    "    # 후보 정리: 불용어/숫자/짧은 토큰 제거\n",
    "    cleaned = []\n",
    "    for phrase, score in candidates:\n",
    "        ph = \" \".join([w for w in re.findall(r\"[가-힣A-Za-z0-9]+\", phrase) if len(w)>1 and w not in CUSTOM_STOPWORDS])\n",
    "        if not ph: continue\n",
    "        cleaned.append((ph, score))\n",
    "    # 중복 축약 (동일 phrase는 최고 점수만 남김)\n",
    "    best = {}\n",
    "    for ph, sc in cleaned:\n",
    "        inv = 1.0/max(sc, 1e-9)\n",
    "        best[ph] = max(best.get(ph, 0.0), inv)\n",
    "    ranked = sorted(best.items(), key=lambda x: x[1], reverse=True)\n",
    "    return ranked[:top_k]\n",
    "\n",
    "# =========================\n",
    "# KeyBERT (멀티링구얼 사전학습 임베딩)\n",
    "# =========================\n",
    "_KEYBERT_MODEL = None\n",
    "def get_keybert():\n",
    "    global _KEYBERT_MODEL\n",
    "    if _KEYBERT_MODEL is None:\n",
    "        # 다국어 모델 (가벼움, ko 지원)\n",
    "        _KEYBERT_MODEL = KeyBERT(model='paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    return _KEYBERT_MODEL\n",
    "\n",
    "def keybert_top_phrases(docs: List[str], top_k=30) -> List[Tuple[str, float]]:\n",
    "    if not docs:\n",
    "        return []\n",
    "    text = \"\\n\".join(docs)\n",
    "    kw = get_keybert()\n",
    "    # KeyBERT 점수: cos sim (높을수록 좋음)\n",
    "    candidates = kw.extract_keywords(\n",
    "        text,\n",
    "        keyphrase_ngram_range=(1,3),\n",
    "        stop_words=list(CUSTOM_STOPWORDS),\n",
    "        top_n=top_k*5\n",
    "    )\n",
    "    # 정리 + 중복 제거\n",
    "    agg = {}\n",
    "    for ph, sc in candidates:\n",
    "        ph2 = \" \".join([w for w in re.findall(r\"[가-힣A-Za-z0-9]+\", ph) if len(w)>1 and w not in CUSTOM_STOPWORDS])\n",
    "        if not ph2: continue\n",
    "        agg[ph2] = max(agg.get(ph2, 0.0), float(sc))\n",
    "    ranked = sorted(agg.items(), key=lambda x: x[1], reverse=True)\n",
    "    return ranked[:top_k]\n",
    "\n",
    "# =========================\n",
    "# 비교 지표/출력\n",
    "# =========================\n",
    "def to_rank_dict(items: List[Tuple[str, float]]) -> Dict[str, int]:\n",
    "    return {ph: i for i,(ph,_) in enumerate(items, start=1)}\n",
    "\n",
    "def jaccard(a: List[str], b: List[str]) -> float:\n",
    "    sa, sb = set(a), set(b)\n",
    "    if not sa and not sb: return 1.0\n",
    "    if not sa or not sb: return 0.0\n",
    "    return len(sa & sb) / len(sa | sb)\n",
    "\n",
    "def token_jaccard(a: List[str], b: List[str]) -> float:\n",
    "    ta = set(sum([ph.split() for ph in a], []))\n",
    "    tb = set(sum([ph.split() for ph in b], []))\n",
    "    if not ta and not tb: return 1.0\n",
    "    if not ta or not tb: return 0.0\n",
    "    return len(ta & tb) / len(ta | tb)\n",
    "\n",
    "def spearman_on_common(a_items: List[Tuple[str,float]], b_items: List[Tuple[str,float]]) -> float:\n",
    "    ra, rb = to_rank_dict(a_items), to_rank_dict(b_items)\n",
    "    common = [ph for ph in ra if ph in rb]\n",
    "    if len(common) < 3:\n",
    "        return float('nan')\n",
    "    xa = [ra[ph] for ph in common]\n",
    "    xb = [rb[ph] for ph in common]\n",
    "    rho, _ = spearmanr(xa, xb)\n",
    "    return float(rho)\n",
    "\n",
    "def intersec_top(a_items, b_items, top=15) -> List[Tuple[str, int, int]]:\n",
    "    ra, rb = to_rank_dict(a_items), to_rank_dict(b_items)\n",
    "    common = [(ph, ra[ph], rb[ph]) for ph in ra if ph in rb]\n",
    "    common.sort(key=lambda x: (x[1]+x[2]))\n",
    "    return common[:top]\n",
    "\n",
    "def save_csv_per_method(year, month, results: Dict[str, List[Tuple[str,float]]], outdir=BENCH_OUTDIR):\n",
    "    rows = []\n",
    "    for m, items in results.items():\n",
    "        for rank, (ph, sc) in enumerate(items, start=1):\n",
    "            rows.append({\"year\":year, \"month\":month, \"method\":m, \"rank\":rank, \"phrase\":ph, \"score\":sc})\n",
    "    path = os.path.join(outdir, f\"{year}_{month:02d}_methods_keywords.csv\")\n",
    "    # CSV 직접 작성(표준 라이브러리)\n",
    "    import csv\n",
    "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        wr = csv.DictWriter(f, fieldnames=[\"year\",\"month\",\"method\",\"rank\",\"phrase\",\"score\"])\n",
    "        wr.writeheader()\n",
    "        wr.writerows(rows)\n",
    "    print(f\"📄 저장: {path}\")\n",
    "\n",
    "def compare_methods(year:int, month:int, all_articles: List[dict], vectorizer: TfidfVectorizer):\n",
    "    docs = monthly_corpus(year, month, all_articles)\n",
    "    print(f\"📚 {year}-{month:02d} 문서 수: {len(docs)}\")\n",
    "    if not docs:\n",
    "        print(\"⚠️ 문서가 없습니다.\")\n",
    "        return\n",
    "\n",
    "    results = {}\n",
    "    # 1) TF-IDF\n",
    "    results[\"tfidf\"]    = tfidf_top_phrases(docs, vectorizer, TOP_K)\n",
    "    # 2) TextRank\n",
    "    results[\"textrank\"] = textrank_top_phrases(docs, TOP_K)\n",
    "    # 3) YAKE\n",
    "    results[\"yake\"]     = yake_top_phrases(docs, TOP_K)\n",
    "    # 4) KeyBERT\n",
    "    results[\"keybert\"]  = keybert_top_phrases(docs, TOP_K)\n",
    "\n",
    "    # 쌍별 비교 지표\n",
    "    methods = list(results.keys())\n",
    "    pairwise = {}\n",
    "    for i in range(len(methods)):\n",
    "        for j in range(i+1, len(methods)):\n",
    "            a, b = methods[i], methods[j]\n",
    "            a_items, b_items = results[a], results[b]\n",
    "            a_ph = [p for p,_ in a_items]\n",
    "            b_ph = [p for p,_ in b_items]\n",
    "            pairwise[f\"{a}_vs_{b}\"] = {\n",
    "                \"jaccard_phrase\": round(jaccard(a_ph, b_ph), 3),\n",
    "                \"jaccard_token\": round(token_jaccard(a_ph, b_ph), 3),\n",
    "                \"spearman_rank_on_common\": (None if math.isnan(spearman_on_common(a_items, b_items)) else round(spearman_on_common(a_items, b_items), 3)),\n",
    "                \"intersection_top\": [\n",
    "                    {\"phrase\": ph, \"rank_in_\"+a: ra, \"rank_in_\"+b: rb}\n",
    "                    for ph, ra, rb in intersec_top(a_items, b_items, top=15)\n",
    "                ]\n",
    "            }\n",
    "\n",
    "    # 저장\n",
    "    save_csv_per_method(year, month, results, BENCH_OUTDIR)\n",
    "\n",
    "    # JSON 종합 저장\n",
    "    out_full = {\n",
    "        \"year\": year, \"month\": month,\n",
    "        \"n_docs\": len(docs),\n",
    "        \"top_k\": TOP_K,\n",
    "        \"results\": {\n",
    "            m: [{\"phrase\": ph, \"score\": float(sc)} for ph, sc in items]\n",
    "            for m, items in results.items()\n",
    "        },\n",
    "        \"pairwise_metrics\": pairwise\n",
    "    }\n",
    "    jpath = os.path.join(BENCH_OUTDIR, f\"{year}_{month:02d}_full_result.json\")\n",
    "    with open(jpath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out_full, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"💾 저장: {jpath}\")\n",
    "\n",
    "    # 별도: pairwise만 저장\n",
    "    ppath = os.path.join(BENCH_OUTDIR, f\"{year}_{month:02d}_pairwise_metrics.json\")\n",
    "    with open(ppath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out_full[\"pairwise_metrics\"], f, ensure_ascii=False, indent=2)\n",
    "    print(f\"💾 저장: {ppath}\")\n",
    "\n",
    "def main():\n",
    "    # 1) 전체 기사 로드\n",
    "    all_articles = load_all_articles(file_path)\n",
    "    print(f\"전체 문서 수: {len(all_articles)}\")\n",
    "\n",
    "    # 2) TF-IDF 벡터라이저 로드/학습\n",
    "    vectorizer = load_or_train_vectorizer(all_articles, TFIDF_VECTORIZER_PATH)\n",
    "    print(\"TF-IDF 벡터라이저 준비 완료\")\n",
    "\n",
    "    # 3) 실행 대상 연월 설정 (예: 2024년 1~12월)\n",
    "    year = 2024\n",
    "    target_months = list(range(1,13))\n",
    "\n",
    "    for m in target_months:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"▶ {year}년 {m}월 비교 실행\")\n",
    "        compare_methods(year, m, all_articles, vectorizer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed6b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
