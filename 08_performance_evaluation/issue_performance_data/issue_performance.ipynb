{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ece1b7",
   "metadata": {},
   "source": [
    "# í•µì‹¬ì´ìŠˆ ì¶”ì¶œì˜ ì„±ëŠ¥í‰ê°€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c54c279",
   "metadata": {},
   "source": [
    "## 1. ì •ì„±ì  í‰ê°€ \n",
    "**ìˆ«ìë‚˜ ìˆ˜ì¹˜ë¡œ ë‚˜íƒ€ë‚´ê¸° ì–´ë ¤ìš´ ë¶€ë¶„ì„, ì‚¬ëŒì˜ ì£¼ê´€ì´ë‚˜ íŒë‹¨ì„ ë°”íƒ•ìœ¼ë¡œ í‰ê°€**\n",
    "- ì›”ë³„ ê¸°ì‚¬ ìƒ˜í”Œ ( ì•½ 3~5ê±´ ì„ì˜ë¡œ ì¶”ì¶œ ) ìš”ì•½ë³¸ì„ ì§ì ‘ ì½ê³  TF-IDFê°€ ì •ë§ í•µì‹¬ì´ìŠˆì¸ì§€ í™•ì¸\n",
    "- ì‚¬ëŒì´ ë³´ê¸°ì— ì–´ìƒ‰í•˜ê±°ë‚˜ ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ê°€ ìƒìœ„ì— ìˆë‹¤ë©´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ì¬ì •ë¹„\n",
    "- ì£¼ê´€ì ì¸ í‰ê°€ê°€ ë  ìˆ˜ ìˆì§€ë§Œ íŒ€ì›ë“¤ì´ ì ë‹¹íˆ ê´œì°®ë‹¤ ìƒê°í•˜ë©´ ì •ì„±ì  í‰ê°€ë¡œ ë“¤ì–´ê°ˆ ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5084d412",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5313da4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...\n",
      "âœ… ì´ 80810ê°œì˜ ê¸°ì‚¬ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n",
      "ğŸ”„ ê¸°ì‚¬ë³„ í‚¤ì›Œë“œ ì¶”ì¶œì„ ì§„í–‰í•©ë‹ˆë‹¤...\n",
      "ì „ì²´ 80810ê°œ ê¸°ì‚¬ ì¤‘ 10ê°œ ëœë¤ ìƒ˜í”Œë§\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê¸°ì‚¬ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ ê²°ê³¼ë¥¼ './keyword_extraction_validation_results.csv'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“Š ì¶”ì¶œ ê²°ê³¼ í†µê³„:\n",
      "- ì´ ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜: 10\n",
      "- í‚¤ì›Œë“œê°€ ì¶”ì¶œëœ ê¸°ì‚¬ ìˆ˜: 10\n",
      "- í‚¤ì›Œë“œê°€ ì¶”ì¶œë˜ì§€ ì•Šì€ ê¸°ì‚¬ ìˆ˜: 0\n",
      "- í‰ê·  í‚¤ì›Œë“œ ê°œìˆ˜: 4.80\n",
      "\n",
      "ğŸ“‹ ì „ì²´ ì¶”ì¶œ ê²°ê³¼:\n",
      "\n",
      "================================================================================\n",
      "ğŸ“° ê¸°ì‚¬ 1\n",
      "ì œëª©: \n",
      "ìš”ì•½: ë˜ ë‹¤ë¥¸ ë¶„ì„ì—…ì²´ TRM ë©ìŠ¤ëŠ” ë¶í•œì´ ì§€ë‚œí•´ í•´í‚¹ìœ¼ë¡œ ê°•íƒˆí•œ ìˆ˜ì²œë§Œë‹¬ëŸ¬ ê·œëª¨ì˜ ê°€ìƒí™”í ê°€ì¹˜ê°€ ìµœê·¼ ëª‡ ì£¼ ì‚¬ì´ 80âˆ¼85% í­ë½í•´ í˜„ì¬ 1ì²œë§Œë‹¬ëŸ¬(ì•½ 130ì–µì›)ë„ ì•ˆ ë˜ëŠ” ê²ƒìœ¼ë¡œ ì¶”ì •í–ˆë‹¤. ì§€ë‚œë‹¬ ì‹œì‘ëœ ê°€ìƒí™”í ê°€ì¹˜ì˜ ê°‘ì‘ìŠ¤ëŸ¬ìš´ ê¸‰ë½ìœ¼ë¡œ ë¶í•œì´ í•´í‚¹ ë“±ìœ¼ë¡œ í˜„ê¸ˆì„ ë§ˆë ¨í•˜ëŠ” ëŠ¥ë ¥ì´ í›¼ì†ë˜ê³  ë¬´ê¸° ê°œë°œ í”„ë¡œê·¸ë¨ì— ìê¸ˆì„ ì¡°ë‹¬í•˜ëŠ” ê³„íšë„ ì˜í–¥ì„ ë°›ì•˜ì„ ê²ƒ...\n",
      "ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: ê³„íš ì˜í–¥ ë°›ë‹¤ | í•´í‚¹ ìì› ìŸë‹¤ | ì§€ì  ì¬ë¬´ë¶€ ë”°ë¥´ë‹¤ | ìì› ìŸë‹¤ | ì¬ë¬´ë¶€ ë”°ë¥´ë‹¤\n",
      "í‚¤ì›Œë“œ ê°œìˆ˜: 5ê°œ\n",
      "\n",
      "================================================================================\n",
      "ğŸ“° ê¸°ì‚¬ 2\n",
      "ì œëª©: \n",
      "ìš”ì•½: ì•„ë²  ì‹ ì¡° ì¼ë³¸ ì´ë¦¬ëŠ” ë¹„í•µí™”ì— ëŒ€í•œ ë¶„ëª…í•œ ì–¸ê¸‰ì´ ì„ í–‰ë¼ì•¼ í•œë‹¤ëŠ” ê¸°ì¡´ì˜ ì…ì¥ì„ ì¬í™•ì¸í•˜ë©´ì„œ ë¶ë¯¸ ì •ìƒíšŒë‹´ì— ì•ì„œ 4ì›” ë¯¸ì¼ ì •ìƒê°„ ëŒ€í™”ë¥¼ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. ì¼ë³¸ ì •ë¶€ëŠ” ë¶ë¯¸ ì •ìƒíšŒë‹´ ë°œí‘œì—ì— ì¼ë‹¨ í™˜ì˜ì˜ ëœ»ì„ í‘œí•˜ë©´ì„œë„ ê²½ê³„ì‹¬ë¥¼ ë“œëŸ¬ëƒˆìŠµë‹ˆë‹¤. ê·¸ëŠ” \"ë¹„í•µí™” ë…¼ì˜ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆëŠ” ë¶í•œì˜ ë³€í™”ë¥¼ ë†’ì´ í‰ê°€í•œë‹¤\"ë©´ì„œë„, ê¸‰ë³€í•˜ëŠ” í•œë°˜ë„ ì •ì„¸ì—ì„œ ì¼ë³¸ì´ ë°°ì œë˜ëŠ”...\n",
      "ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: ëŒ€í•œ ìš°ë ¤ ì‹œí‚¤ë‹¤ | ë¶„ì„ ë¹„í•µí™” ì•ì„¸ìš°ë‹¤ | ì…ì¥ í™•ì¸ ê²ë‹ˆë‹¤ | ì§„ì „ ê²½ê³„ ë“œëŸ¬ë‚´ë‹¤ | ì¼ë³¸ ì´ë¦¬ ì´ë¥´ë‹¤\n",
      "í‚¤ì›Œë“œ ê°œìˆ˜: 5ê°œ\n",
      "\n",
      "================================================================================\n",
      "ğŸ“° ê¸°ì‚¬ 3\n",
      "ì œëª©: \n",
      "ìš”ì•½: åŒ— \"ì´ˆëŒ€í˜•ë°©ì‚¬í¬ ì‹œí—˜ë°œì‚¬\"â€¦ì¶”ê°€ë°œì‚¬ ì˜ˆê³  ë¶í•œì´ ì–´ì œ(10ì¼) ìœ ë°œì‚¬ì²´ê°€ 'ì´ˆëŒ€í˜• ë°©ì‚¬í¬'ì˜€ë‹¤ê³  ë°í˜”ìŠµë‹ˆë‹¤. ë¶í•œì€ ë°œì‚¬ì²´ ë°œì‚¬ ë‹¤ìŒ ë‚  'ì´ˆëŒ€í˜• ë°©ì‚¬í¬'ë¥¼ ì‹œí—˜í–ˆë‹¤ê³  ë°í˜”ìŠµë‹ˆë‹¤. ì§€ë‚œë‹¬ 24ì¼ì— ì´ì–´, ì´ë²ˆì—” ë‚´ë¥™ì„ ê´€í†µí•˜ëŠ” ì‹ìœ¼ë¡œ ë˜ë‹¤ì‹œ ì˜ì•„ ì˜¬ë¦° ê²ƒì…ë‹ˆë‹¤. <ì¡°ì„ ì¤‘ì•™TV> \"ì´ˆëŒ€í˜•ë°©ì‚¬í¬ì‹œí—˜ì‚¬ê²©ì€ ì‹œí—˜ì‚¬ê²© ëª©ì ì— ì™„ì „ë¶€í•©ë˜ì—ˆìœ¼ë©° ë¬´ê¸°ì²´ê³„ì™„ì„±ì˜ ë‹¤ìŒ ...\n",
      "ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: ì™„ì „ ë¶€í•© ë˜ì–´ë‹¤ | ì™„ì„± ë‹¨ê³„ ì´ë¥´ë‹¤ | ì´ˆëŒ€í˜• ë°©ì‚¬í¬ ë°íˆë‹¤ | ë¶í•œ ì–´ì œ ì˜ë‹¤ | ëšœë · ê²°ì • ì§“ë‹¤\n",
      "í‚¤ì›Œë“œ ê°œìˆ˜: 5ê°œ\n",
      "\n",
      "================================================================================\n",
      "ğŸ“° ê¸°ì‚¬ 4\n",
      "ì œëª©: \n",
      "ìš”ì•½: í†µì¼ë¶€, í–‰ì‚¬ ì—¿ìƒˆ ë‚¨ê²¨ë‘ê³  åŒ—ì—” ì•„ì§ ì•ˆ ì•Œë ¤â€¦\"ì ì ˆ ì‹œì ì— í†µì§€ ê³„íš\" í†µì¼ë¶€Â·ì„œìš¸ì‹œÂ·ê²½ê¸°ë„ ê³µë™ì£¼ìµœâ€¦éŸ“Â·ç¾Â·ä¸­Â·æ—¥ 4ê°œêµ­ ì•„í‹°ìŠ¤íŠ¸ ì°¸ì—¬ ì •ë¶€ì™€ ì§€ë°©ìì¹˜ë‹¨ì²´ê°€ 4Â·27 íŒë¬¸ì  ì„ ì–¸ 1ì£¼ë…„ ê¸°ë…í–‰ì‚¬ë¥¼ ê°œìµœí•  ì˜ˆì •ì´ì§€ë§Œ, ë¶ì¸¡ì˜ ì°¸ì—¬ ì—¬ë¶€ê°€ í™•ì •ë˜ì§€ ì•Šì•„ 'ë°˜ìª½'í–‰ì‚¬ê°€ ë  ê°€ëŠ¥ì„±ì„ ë°°ì œí•˜ê¸° ì–´ë µê²Œ ëë‹¤. í†µì¼ë¶€ ë‹¹êµ­ìëŠ” \"ì´ë²ˆ í–‰ì‚¬ì— ëŒ€í•´ ë¶ì¸¡ì— ì ì ˆí•œ ì‹œ...\n",
      "ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: í–‰ì‚¬ ê°€ëŠ¥ì„± ì—´ë‹¤ | í–‰ì‚¬ ì—¿ìƒˆ ë‚¨ê¸°ë‹¤ | ì°¸ì—¬ ì˜ì‚¬ ë°íˆë‹¤ | í–‰ì‚¬ ì—¿ìƒˆ ì•Œë‹¤ | í–‰ì‚¬ ì—¿ìƒˆ ë‘ë‹¤\n",
      "í‚¤ì›Œë“œ ê°œìˆ˜: 5ê°œ\n",
      "\n",
      "================================================================================\n",
      "ğŸ“° ê¸°ì‚¬ 5\n",
      "ì œëª©: \n",
      "ìš”ì•½: ë¸”ë¼ë””ë³´ìŠ¤í† í¬ êµ­ì œê³µí•­ì— ì°©ë¥™ ì¤‘ì¸ ê³ ë ¤í•­ê³µ íˆ¬í´ë ˆí”„-204 ì—¬ê°ê¸° ë¶í•œ êµ­ì í•­ê³µì‚¬ì¸ ê³ ë ¤í•­ê³µ ì†Œì† ì—¬ê°ê¸°ê°€ 5ë…„ë§Œì— ì²˜ìŒìœ¼ë¡œ ì¤‘êµ­ ìƒí•˜ì´ì— ë‚´ë ¤ ì£¼ëª©ëœë‹¤ê³  ë¯¸êµ­ì˜ ë¶í•œì „ë¬¸ë§¤ì²´ì¸ NKë‰´ìŠ¤ê°€ 30ì¼ ë³´ë„í–ˆë‹¤. ì´ ë§¤ì²´ëŠ” í•­ê³µê¸° í•­ë¡œ ì¶”ì ì‚¬ì´íŠ¸ í”Œë¼ì´íŠ¸ë ˆì´ë”24ì™€ í”Œë¼ì´íŠ¸ì–´ì›¨ì–´ ìë£Œë¥¼ ì¸ìš©, í•œêµ­ì‹œê°„ìœ¼ë¡œ ì „ë‚  ì˜¤í›„ 10ì‹œ 10ë¶„ê»˜ ê³ ë ¤í•­ê³µ íˆ¬í´ë ˆí”„(Tu)-20...\n",
      "ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: í”Œë¼ì´íŠ¸ ë ˆì´ë” ì˜¤ë‹¤ | í‰ì–‘ ìƒí•˜ì´ ì‡ë‹¤ | ì¤‘êµ­ ìƒí•˜ì´ ë‚´ë‹¤ | ì˜¤ì „ ë¶í•œ ëŒì•„ê°€ë‹¤ | ì œíŠ¸ê¸° ìƒí•˜ì´ ë¨¸ë¬´ë¥´ë‹¤\n",
      "í‚¤ì›Œë“œ ê°œìˆ˜: 5ê°œ\n",
      "\n",
      "================================================================================\n",
      "ğŸ“° ê¸°ì‚¬ 6\n",
      "ì œëª©: \n",
      "ìš”ì•½: 2022.12.27 ë¶í•œ ì†Œí˜•ë¬´ì¸ê¸°ê°€ ì„œìš¸ ìƒê³µê¹Œì§€ ì¹¨íˆ¬í–ˆëŠ”ë°ë„ ê²©ì¶”ì— ì‹¤íŒ¨í•œ êµ°ì´ ë¬´ì¸ê¸° ë„ë°œì„ ìƒì •í•œ í•©ë™ë°©ê³µí›ˆë ¨ì„ í•˜ëŠ” ë“± ëŒ€ì‘ì±…ì„ ë§ˆë ¨í•˜ê² ë‹¤ê³  ë°í˜”ë‹¤. í•©ë™ì°¸ëª¨ë³¸ë¶€ëŠ” 28ì¼ êµ­íšŒ êµ­ë°©ìœ„ì›íšŒì˜ ê¸´ê¸‰ í˜„ì•ˆ ì§ˆì˜ì—ì„œ ë¬´ì¸ê¸° ëŒ€ì‘ ì‹¤ì „ êµìœ¡Â·í›ˆë ¨ ê°•í™”ì™€ ëŒ€ì‘ì „ë ¥ ì¡°ê¸° ì „ë ¥í™” ì¶”ì§„ ë“± í›„ì† ì¡°ì²˜ë¥¼ ë³´ê³ í–ˆë‹¤. êµ°ì€ ì „ë‚  ê¹€ìŠ¹ê²¸ í•©ì°¸ì˜ì¥ ì£¼ê´€ìœ¼ë¡œ ë¶í•œ ë¬´ì¸ê¸° ì˜...\n",
      "ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: ëŒ€ì‘ ë§ˆë ¨ ë°íˆë‹¤ | ë§ˆë ¨ ë°íˆë‹¤ | ë°íˆë‹¤\n",
      "í‚¤ì›Œë“œ ê°œìˆ˜: 3ê°œ\n",
      "\n",
      "================================================================================\n",
      "ğŸ“° ê¸°ì‚¬ 7\n",
      "ì œëª©: \n",
      "ìš”ì•½: ì„œìš¸ëŒ€ í†µì¼í‰í™”ç¡ í•™ìˆ íšŒì˜â€¦\"ê²½ì œÂ·ë¬¸í™” í˜‘ë ¥í•´ ë³€í™” ëŒì–´ë‚´ì•¼\" ë¶í•œ íšŒì˜ì¥ì— ê±¸ë¦° ê¹€ì¼ì„± ê¹€ì •ì¼ ì´ˆìƒí™” 2021ë…„ 6ì›” 20ì¼ ì—´ë¦° ë¶í•œ ë…¸ë™ë‹¹ ì™¸ê³½ì¡°ì§ 'ì‚¬íšŒì£¼ì˜ì—¬ì„±ë™ë§¹' ì œ7ì°¨ ëŒ€íšŒì—ì„œ ëŒ€íšŒì¥ ì „ë©´ì— ê±¸ë¦° ê¹€ì¼ì„± ì£¼ì„ê³¼ ê¹€ì •ì¼ êµ­ë°©ìœ„ì›ì¥ì˜ ì´ˆìƒí™”. ê¹€ êµìˆ˜ì— ë”°ë¥´ë©´ ë¶í•œì€ ì§€ë‚œí•´ 1ì›” 8ì°¨ ë‹¹ëŒ€íšŒì—ì„œ ì£¼ì„ë‹¨ ì •ë©´ì— ê±¸ë ¤ìˆë˜ ê¹€ì¼ì„± ì£¼ì„ ë° ê¹€ì •ì¼ êµ­ë°©ìœ„...\n",
      "ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: êµ­ë°©ìœ„ì›ì¥ ì´ˆìƒí™” ë–¼ë‹¤ | íšŒì¥ ì „ë©´ ê±¸ë¦¬ë‹¤ | ì£¼ì„ ì •ë©´ ê±¸ë¦¬ë‹¤ | ìœ í•™ ìœ„ì›ì¥ ì´ë¥´ë‹¤ | êµ­ì œí™” ì—´ë§ ë“œëŸ¬ë‚´ë‹¤\n",
      "í‚¤ì›Œë“œ ê°œìˆ˜: 5ê°œ\n",
      "\n",
      "================================================================================\n",
      "ğŸ“° ê¸°ì‚¬ 8\n",
      "ì œëª©: \n",
      "ìš”ì•½: \"ë¯¸ì‚¬ì¼ ë°œì‚¬, ìµœê³  ìˆ˜ë‡Œë¶€ ê²°ì‹¬ë”°ë¼ ì„ì˜ ì‹œê°Â·ì¥ì†Œì„œ ì§„í–‰ë  ê²ƒ\" ë² ì´ì§• ë¶í•œëŒ€ì‚¬ê´€ ì• í’ê²½ = ì£¼ì¤‘ ë¶í•œëŒ€ì‚¬ê´€ì€ 15ì¼ ë¬¸ì¬ì¸ ì •ë¶€ ì¶œë²”ê³¼ ê´€ë ¨í•´ ë‚¨ë¶ í•©ì˜ë¥¼ ì² ì €íˆ ì´í–‰í•˜ëŠ” ê²Œ ì¤‘ìš”í•˜ë‹¤ê³  ë°í˜”ë‹¤. ì£¼ì¤‘ ë¶í•œëŒ€ì‚¬ê´€ì€ ì´ë‚  ë² ì´ì§• ëŒ€ì‚¬ê´€ì— ì¼ë¶€ í˜„ì¬ íšŒê²¬ì„ ê°–ê³  ì´ê°™ì´ ì£¼ì¥í–ˆë‹¤. ê·¸ëŠ” ì´ì–´ \"ì‹œí—˜ ë°œì‚¬ëŠ” ìš°ë¦¬ ìµœê³  ìˆ˜ë‡Œë¶€ì˜ ê²°ì‹¬ì— ë”°ë¼ì„œ ì„ì˜ ì‹œê°, ì„ì˜...\n",
      "ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: ìˆ˜ë‡Œë¶€ ê²°ì‹¬ ë”°ë¥´ë‹¤ | ì¼ë¶€ íšŒê²¬ ê°–ë‹¤ | í˜„ì¬ íšŒê²¬ ê°–ë‹¤ | í•©ì˜ ì´í–‰ ë°íˆë‹¤ | ì¡´ì¤‘ ì´í–‰ ë°íˆë‹¤\n",
      "í‚¤ì›Œë“œ ê°œìˆ˜: 5ê°œ\n",
      "\n",
      "================================================================================\n",
      "ğŸ“° ê¸°ì‚¬ 9\n",
      "ì œëª©: \n",
      "ìš”ì•½: ë¶ \"ê·¹ì´ˆìŒì†ë¯¸ì‚¬ì¼ ì‹œí—˜ë°œì‚¬â€¦700ã ëª…ì¤‘\"â€¦ê¹€ì •ì€ ë¶ˆì°¸ ë¶í•œì´ ì „ë‚  ê·¹ì´ˆìŒì† ë¯¸ì‚¬ì¼ì„ ì‹œí—˜ ë°œì‚¬í–ˆë‹¤ê³  í™•ì¸í–ˆë‹¤. ì¡°ì„ ì¤‘ì•™í†µì‹ ì€ 6ì¼ \"êµ­ë°©ê³¼í•™ì›ì€ 1ì›” 5ì¼ ê·¹ì´ˆìŒì† ë¯¸ì‚¬ì¼ ì‹œí—˜ë°œì‚¬ë¥¼ ì§„í–‰í•˜ì˜€ë‹¤\"ë¼ê³  ë³´ë„í–ˆë‹¤. 2022.1.6 í†µì¼ë¶€ëŠ” 6ì¼ ë¶í•œì´ ì „ë‚  ë™í•´ìƒìœ¼ë¡œ íƒ„ë„ë¯¸ì‚¬ì¼ì„ ë°œì‚¬í•œ ê²ƒê³¼ ê´€ë ¨, \"ë¶í•œì˜ ë°œì‚¬ ì˜ë„ë¥¼ ì–´ëŠ í•œ ë°©í–¥ìœ¼ë¡œ ë‹¨ì •í•˜ì§€ ì•Šê³  ìˆë‹¤\"...\n",
      "ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: ë¬´ê¸° ë¶€ë¬¸ ëŒ€ë‹¤ | ë°œì‚¬ ë°°ê²½ ë°íˆë‹¤ | ì˜ë„ ë°©í–¥ ì•Šë‹¤ | ë°°ê²½ ë°íˆë‹¤ | ë¶€ë¬¸ ëŒ€ë‹¤\n",
      "í‚¤ì›Œë“œ ê°œìˆ˜: 5ê°œ\n",
      "\n",
      "================================================================================\n",
      "ğŸ“° ê¸°ì‚¬ 10\n",
      "ì œëª©: \n",
      "ìš”ì•½: êµ¬í…ŒíìŠ¤ ìœ ì—”ì´ì¥, ë¶í•œ ë¦¬ìš©í˜¸ ë©´ë‹´â€¦\"ì •ì¹˜ì  í•´ë²• ê°•ì¡°\" ì œ72ì°¨ ìœ ì—”ì´íšŒ ì°¸ì„ì°¨ ë¯¸êµ­ ë‰´ìš•ì„ ë°©ë¬¸ ì¤‘ì¸ ë¦¬ìš©í˜¸ ë¶í•œ ì™¸ë¬´ìƒì´ ì•ˆí† ë‹ˆìš° êµ¬í…ŒíìŠ¤ ìœ ì—” ì‚¬ë¬´ì´ì¥ì„ ë¹„ê³µê°œ ì ‘ê²¬í–ˆìŠµë‹ˆë‹¤. ë¦¬ ì™¸ë¬´ìƒì€ í˜„ì§€ì‹œê°„ 23ì¼ ì˜¤í›„ ìœ ì—”ì´íšŒ ê¸°ì¡°ì—°ì„¤ì„ ë§ˆì¹œ ì§í›„ êµ¬í…ŒíìŠ¤ ì´ì¥ê³¼ ì•½ 30ë¶„ ê°„ ë©´ë‹´í–ˆìŠµë‹ˆë‹¤. êµ¬í…ŒíìŠ¤ ì´ì¥ì€ ë¦¬ ì™¸ë¬´ìƒì—ê²Œ í•œë°˜ë„ ê¸´ì¥ ê³ ì¡°ì— ìš°ë ¤ë¥¼ í‘œì‹œí•˜ë©´...\n",
      "ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: ì´íšŒ ì—°ì„¤ ë§ˆì¹˜ë‹¤ | ë‹µë³€ í™•ì¸ ì•Šë‹¤ | ì—°ì„¤ ë§ˆì¹˜ë‹¤ | í™•ì¸ ì•Šë‹¤ | ë¹„ê³µê°œ\n",
      "í‚¤ì›Œë“œ ê°œìˆ˜: 5ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ê°œë³„ ê¸°ì‚¬ í•µì‹¬ì´ìŠˆ ì¶”ì¶œ ê²€ì¦ ì½”ë“œ\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "from konlpy.tag import Okt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "# =========================\n",
    "# ì„¤ì •\n",
    "# =========================\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì • (ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”)\n",
    "file_path = '/home/ds4_sia_nolb/#FINAL_POLARIS/04_plus_preprocessing/preprocessing_final_data/final_preprocessing.json'\n",
    "output_csv_path = './keyword_extraction_validation_results.csv'\n",
    "\n",
    "# ìƒ˜í”Œ ê°œìˆ˜ ì„¤ì • (ì „ì²´ ë°ì´í„°ê°€ ë§ì„ ê²½ìš° ì¼ë¶€ë§Œ ìƒ˜í”Œë§)\n",
    "SAMPLE_SIZE = 10  # Noneìœ¼ë¡œ ì„¤ì •í•˜ë©´ ì „ì²´ ë°ì´í„° ì²˜ë¦¬\n",
    "\n",
    "# =========================\n",
    "# í˜•íƒœì†Œ ë¶„ì„ê¸°\n",
    "# =========================\n",
    "okt = Okt()\n",
    "\n",
    "# =========================\n",
    "# ê¸°ì¡´ í•¨ìˆ˜ë“¤ ì¬ì‚¬ìš©\n",
    "# =========================\n",
    "def normalize_text(t: str) -> str:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ì •ê·œí™”\"\"\"\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    t = t.replace(\"íƒ„ë„ ë¯¸ì‚¬ì¼\", \"íƒ„ë„ë¯¸ì‚¬ì¼\")\n",
    "    t = t.replace(\"ìˆœí•­ ë¯¸ì‚¬ì¼\", \"ìˆœí•­ë¯¸ì‚¬ì¼\")\n",
    "    t = t.replace(\"ê·¹ì´ˆ ìŒì†\", \"ê·¹ì´ˆìŒì†\")\n",
    "    t = t.replace(\"ì´ˆëŒ€í˜• ë°©ì‚¬í¬\", \"ì´ˆëŒ€í˜•ë°©ì‚¬í¬\")\n",
    "    return t\n",
    "\n",
    "def pos_tokens(text: str):\n",
    "    \"\"\"í˜•íƒœì†Œ ë¶„ì„\"\"\"\n",
    "    text = normalize_text(text or \"\")\n",
    "    return okt.pos(text, norm=True, stem=True)\n",
    "\n",
    "def doc_text(a) -> str:\n",
    "    \"\"\"ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    return normalize_text(f\"{a.get('title','')} {a.get('summary','')}\")\n",
    "\n",
    "# ë¶ˆìš©ì–´ ì •ì˜\n",
    "BASE_STOP = set([\n",
    "    'ê°€','ê°„','ê°™ì€','ê°™ì´','ê²ƒ','ê²Œë‹¤ê°€','ê²°êµ­','ê³§','ê´€í•˜ì—¬','ê´€ë ¨','ê´€í•œ','ê·¸','ê·¸ê²ƒ','ê·¸ë…€','ê·¸ë“¤',\n",
    "    'ê·¸ë¦¬ê³ ','ê·¸ë•Œ','ê·¸ë˜','ê·¸ë˜ì„œ','ê·¸ëŸ¬ë‚˜','ê·¸ëŸ¬ë¯€ë¡œ','ê·¸ëŸ¬í•œ','ê·¸ëŸ°','ê·¸ë ‡ê²Œ','ê·¸ì™¸','ê·¼ê±°ë¡œ','ê¸°íƒ€',\n",
    "    'ê¹Œì§€ë„','ê¹Œì§€','ë‚˜','ë‚¨ë“¤','ë„ˆ','ëˆ„êµ¬','ë‹¤','ë‹¤ê°€','ë‹¤ë¥¸','ë‹¤ë§Œ','ë‹¤ì†Œ','ë‹¤ìˆ˜','ë‹¤ì‹œ','ë‹¤ìŒ','ë‹¨','ë‹¨ì§€',\n",
    "    'ë‹¹ì‹ ','ëŒ€','ëŒ€í•´ì„œ','ë”êµ°ë‹¤ë‚˜','ë”êµ¬ë‚˜','ë”ë¼ë„','ë”ìš±ì´','ë„','ë„ë¡œ','ë˜','ë˜ëŠ”','ë˜í•œ','ë•Œ','ë•Œë¬¸',\n",
    "    'ë¼ë„','ë¼ë©´','ë¼ëŠ”','ë¡œ','ë¡œë¶€í„°','ë¡œì¨','ë¥¼','ë§ˆì €','ë§ˆì¹˜','ë§Œì•½','ë§Œì¼','ë§Œí¼','ëª¨ë‘','ë¬´ì—‡','ë¬´ìŠ¨',\n",
    "    'ë¬´ì²™','ë¬¼ë¡ ','ë°','ë°–ì—','ë°”ë¡œ','ë³´ë‹¤','ë¿ì´ë‹¤','ì‚¬ëŒ','ì‚¬ì‹¤ì€','ìƒëŒ€ì ìœ¼ë¡œ','ìƒê°','ì„¤ë ¹','ì†Œìœ„','ìˆ˜',\n",
    "    'ìˆ˜ì¤€','ì‰½ê²Œ','ì‹œëŒ€','ì‹œì‘í•˜ì—¬','ì‹¤ë¡œ','ì‹¤ì œ','ì•„ë‹ˆ','ì•„ë¬´','ì•„ë¬´ë„','ì•„ë¬´ë¦¬','ì•„ë§ˆë„','ì•„ìš¸ëŸ¬','ì•„ì§',\n",
    "    'ì•ì—ì„œ','ì•ìœ¼ë¡œ','ì–´ëŠ','ì–´ë–¤','ì–´ë–»ê²Œ','ì–´ë””','ì–¸ì œ','ì–¼ë§ˆë‚˜','ì—¬ê¸°','ì—¬ë¶€','ì—­ì‹œ','ì˜ˆ','ì˜¤íˆë ¤',\n",
    "    'ì™€','ì™œ','ì™¸ì—ë„','ìš”','ìš°ë¦¬','ìš°ì„ ','ì›ë˜','ìœ„í•´ì„œ','ìœ¼ë¡œ','ìœ¼ë¡œë¶€í„°','ìœ¼ë¡œì¨','ì„','ì˜','ì˜ê±°í•˜ì—¬',\n",
    "    'ì˜ì§€í•˜ì—¬','ì˜í•´','ì˜í•´ì„œ','ì˜í•˜ì—¬','ì´','ì´ê²ƒ','ì´ê³³','ì´ë•Œ','ì´ë¼ê³ ','ì´ëŸ¬í•œ','ì´ëŸ°','ì´ë ‡ê²Œ','ì´ì œ',\n",
    "    'ì´ì§€ë§Œ','ì´í›„','ì´ìƒ','ì´ë‹¤','ì´ì „','ì¸','ì¼','ì¼ë‹¨','ì¼ë°˜ì ìœ¼ë¡œ','ì„ì‹œë¡œ','ì…ì¥ì—ì„œ','ì','ìê¸°','ìì‹ ',\n",
    "    'ì ì‹œ','ì €','ì €ê²ƒ','ì €ê¸°','ì €ìª½','ì €í¬','ì „ë¶€','ì „í˜€','ì ì—ì„œ','ì •ë„','ì œ','ì¡°ê¸ˆ','ì¢€','ì£¼ë¡œ','ì£¼ì œ','ì¦‰',\n",
    "    'ì¦‰ì‹œ','ì§€ê¸ˆ','ì§„ì§œë¡œ','ì°¨ë¼ë¦¬','ì°¸','ì°¸ìœ¼ë¡œ','ì²«ë²ˆì§¸ë¡œ','ìµœê³ ','ìµœëŒ€','ìµœì†Œ','ìµœì‹ ','ìµœì´ˆ','í†µí•˜ì—¬',\n",
    "    'í†µí•´ì„œ','í‰ê°€','í¬í•¨í•œ','í¬í•¨í•˜ì—¬','í•˜ì§€ë§Œ','í•˜ë©´ì„œ','í•˜ì—¬','í•œ','í•œë•Œ','í•œë²ˆ','í• ','í• ê²ƒì´ë‹¤','í• ìˆ˜ìˆë‹¤',\n",
    "    'í•¨ê»˜','í•´ë„', 'ë¼ë‹¤', 'ì„œë‹¤', 'ëŒ€í•´', 'ë‚˜ì˜¤ë‹¤', 'í†µí•´', 'ë§ë‹¤', \n",
    "])\n",
    "\n",
    "NEWS_STOP = {\"ê¸°ì\",\"ì—°í•©ë‰´ìŠ¤\",\"ì‚¬ì§„\",\"ì†ë³´\",\"ì¢…í•©\",\"ìë£Œ\",\"ì˜ìƒ\",\"ë‹¨ë…\",\"ì „ë¬¸\",\"ì¸í„°ë·°\",\"ë¸Œë¦¬í•‘\"}\n",
    "\n",
    "ENTITY_NOISE = {\n",
    "    \"ë¶í•œ\",\"í•œêµ­\",\"ëŒ€í•œë¯¼êµ­\",\"ë‚¨í•œ\",\"ë¯¸êµ­\",\"ì¤‘êµ­\",\"ì¼ë³¸\",\"ëŸ¬ì‹œì•„\",\"ìš°í¬ë¼ì´ë‚˜\",\"ìœ ì—”\",\"ë‚˜í† \",\"NATO\",\"EU\",\"ìœ ëŸ½ì—°í•©\",\n",
    "    \"í‘¸í‹´\",\"ë¸”ë¼ë””ë¯¸ë¥´ í‘¸í‹´\",\"ë°”ì´ë“ \",\"ì¡° ë°”ì´ë“ \",\"ì‹œì§„í•‘\",\"ê¹€ì •ì€\",\"ê¹€ì—¬ì •\",\"ë¬¸ì¬ì¸\",\"ìœ¤ì„ì—´\",\"ì‡¼ì´êµ¬\",\"ì ¤ë ŒìŠ¤í‚¤\",\"í†µì‹ \",\"ì¤‘ì•™\",\"ë³´ë„\"\n",
    "}\n",
    "\n",
    "def tokenizer_for_vectorizer(s: str):\n",
    "    \"\"\"TF-IDFìš© í† í¬ë‚˜ì´ì €\"\"\"\n",
    "    toks = []\n",
    "    for w, t in okt.pos(s, norm=True, stem=True):\n",
    "        if t not in (\"Noun\", \"Verb\"):\n",
    "            continue\n",
    "        if len(w) <= 1:\n",
    "            continue\n",
    "        if w in BASE_STOP or w in NEWS_STOP:\n",
    "            continue\n",
    "        if w.isdigit():\n",
    "            continue\n",
    "        toks.append(w)\n",
    "    return toks\n",
    "\n",
    "def learn_action_lexicons_single(article):\n",
    "    \"\"\"ë‹¨ì¼ ê¸°ì‚¬ì—ì„œ í–‰ë™ ë™ì‚¬ì™€ í–‰ìœ„ ëª…ì‚¬ ì¶”ì¶œ\"\"\"\n",
    "    title = article.get('title','') or ''\n",
    "    summary = article.get('summary','') or ''\n",
    "    p = pos_tokens(f\"{title} {summary}\")\n",
    "\n",
    "    verb_set = set()\n",
    "    action_nouns = set()\n",
    "    drop_verbs = {\"í•˜ë‹¤\",\"ë˜ë‹¤\",\"ì´ë‹¤\",\"ìˆë‹¤\"}\n",
    "\n",
    "    for i, (w, t) in enumerate(p):\n",
    "        if t == \"Verb\" and w not in drop_verbs and len(w) > 1:\n",
    "            verb_set.add(w)\n",
    "        if t == \"Noun\" and len(w) > 1 and w not in BASE_STOP:\n",
    "            ahead = [p[j][0] for j in range(i+1, min(i+3, len(p)))]\n",
    "            if \"í•˜ë‹¤\" in ahead or \"ë˜ë‹¤\" in ahead:\n",
    "                action_nouns.add(w)\n",
    "\n",
    "    return verb_set, action_nouns\n",
    "\n",
    "def nominalize_verb(v: str) -> str:\n",
    "    \"\"\"ë™ì‚¬ ëª…ì‚¬í™”\"\"\"\n",
    "    if v.endswith(\"í•˜ë‹¤\"):\n",
    "        return v[:-2]\n",
    "    if v.endswith(\"ë˜ë‹¤\"):\n",
    "        return v[:-2]\n",
    "    return v\n",
    "\n",
    "def extract_single_article_keywords(article, top_k=5):\n",
    "    \"\"\"ë‹¨ì¼ ê¸°ì‚¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ\"\"\"\n",
    "    title = article.get('title','') or ''\n",
    "    summary = article.get('summary','') or ''\n",
    "    \n",
    "    if not title and not summary:\n",
    "        return []\n",
    "\n",
    "    # í–‰ë™ ë™ì‚¬ì™€ í–‰ìœ„ ëª…ì‚¬ í•™ìŠµ\n",
    "    verb_set, action_nouns = learn_action_lexicons_single(article)\n",
    "    \n",
    "    # í˜•íƒœì†Œ ë¶„ì„\n",
    "    p = pos_tokens(f\"{title} {summary}\")\n",
    "    \n",
    "    # í‚¤ì›Œë“œ í›„ë³´ ì¶”ì¶œ\n",
    "    phrase_candidates = set()\n",
    "    prev_nouns = []\n",
    "    \n",
    "    for i, (w, t) in enumerate(p):\n",
    "        if t == \"Noun\":\n",
    "            if w not in BASE_STOP and len(w) > 1:\n",
    "                prev_nouns.append(w)\n",
    "                if len(prev_nouns) > 3:\n",
    "                    prev_nouns = prev_nouns[-3:]\n",
    "\n",
    "        # ë™ì‚¬ ê¸°ë°˜ êµ¬ë¬¸ ìƒì„±\n",
    "        if t == \"Verb\" and w in verb_set:\n",
    "            vnom = nominalize_verb(w)\n",
    "            if vnom and len(vnom) > 1:\n",
    "                # ë™ì‚¬ë§Œ\n",
    "                phrase_candidates.add(vnom)\n",
    "                # ëª…ì‚¬ + ë™ì‚¬\n",
    "                if prev_nouns:\n",
    "                    phrase_candidates.add(f\"{prev_nouns[-1]} {vnom}\")\n",
    "                    if len(prev_nouns) >= 2:\n",
    "                        phrase_candidates.add(f\"{prev_nouns[-2]} {prev_nouns[-1]} {vnom}\")\n",
    "\n",
    "        # í–‰ìœ„ ëª…ì‚¬ ê¸°ë°˜ êµ¬ë¬¸ ìƒì„±\n",
    "        if t == \"Noun\" and w in action_nouns:\n",
    "            # ëª…ì‚¬ë§Œ\n",
    "            phrase_candidates.add(w)\n",
    "            # ì• ëª…ì‚¬ + í–‰ìœ„ ëª…ì‚¬\n",
    "            if prev_nouns and prev_nouns[-1] != w:\n",
    "                phrase_candidates.add(f\"{prev_nouns[-1]} {w}\")\n",
    "                if len(prev_nouns) >= 2:\n",
    "                    phrase_candidates.add(f\"{prev_nouns[-2]} {prev_nouns[-1]} {w}\")\n",
    "\n",
    "    # ë‹¨ì¼ ë¬¸ì„œ TF-IDF ê³„ì‚°\n",
    "    doc_content = doc_text(article)\n",
    "    if not doc_content.strip():\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            tokenizer=tokenizer_for_vectorizer,\n",
    "            ngram_range=(1, 3),\n",
    "            lowercase=False\n",
    "        )\n",
    "        \n",
    "        # ë‹¨ì¼ ë¬¸ì„œë¼ì„œ TFë§Œ ê³„ì‚° (IDFëŠ” ì˜ë¯¸ì—†ìŒ)\n",
    "        X = vectorizer.fit_transform([doc_content])\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        tfidf_scores = X.toarray()[0]\n",
    "        \n",
    "        # TF ì ìˆ˜ ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "        tf_dict = {feature_names[i]: tfidf_scores[i] for i in range(len(feature_names)) if tfidf_scores[i] > 0}\n",
    "        \n",
    "    except:\n",
    "        tf_dict = {}\n",
    "\n",
    "    # ì—”í„°í‹° ë…¸ì´ì¦ˆ í•„í„°ë§ í•¨ìˆ˜\n",
    "    def is_entity_only(ph: str) -> bool:\n",
    "        toks = ph.split()\n",
    "        if len(toks) <= 2 and any(ent in ph for ent in ENTITY_NOISE):\n",
    "            return True\n",
    "        ent_hits = sum(1 for t in toks if any(ent in t for ent in ENTITY_NOISE))\n",
    "        return (ent_hits >= max(1, len(toks) - 1))\n",
    "\n",
    "    # í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚°\n",
    "    scored_phrases = []\n",
    "    for phrase in phrase_candidates:\n",
    "        if is_entity_only(phrase):\n",
    "            continue\n",
    "        if len(phrase.strip()) <= 2:\n",
    "            continue\n",
    "            \n",
    "        # TF ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°\n",
    "        tf_score = tf_dict.get(phrase, 0.0)\n",
    "        \n",
    "        # êµ¬ë¬¸ ê¸¸ì´ ë³´ë„ˆìŠ¤\n",
    "        length_bonus = len(phrase.split()) * 0.1\n",
    "        \n",
    "        # ìµœì¢… ì ìˆ˜\n",
    "        final_score = tf_score + length_bonus\n",
    "        \n",
    "        if final_score > 0:\n",
    "            scored_phrases.append((phrase, final_score))\n",
    "\n",
    "    # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ kê°œ ë°˜í™˜\n",
    "    scored_phrases.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [phrase for phrase, score in scored_phrases[:top_k]]\n",
    "\n",
    "def load_articles(file_path):\n",
    "    \"\"\"ê¸°ì‚¬ ë°ì´í„° ë¡œë“œ\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(\"JSON ë£¨íŠ¸ëŠ” list ì—¬ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_articles_for_validation(articles, sample_size=None):\n",
    "    \"\"\"ê²€ì¦ì„ ìœ„í•œ ê¸°ì‚¬ ì²˜ë¦¬\"\"\"\n",
    "    if sample_size and len(articles) > sample_size:\n",
    "        print(f\"ì „ì²´ {len(articles)}ê°œ ê¸°ì‚¬ ì¤‘ {sample_size}ê°œ ëœë¤ ìƒ˜í”Œë§\")\n",
    "        # ëœë¤ ì‹œë“œ ì„¤ì • (ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´)\n",
    "        random.seed(42)\n",
    "        articles = random.sample(articles, sample_size)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, article in enumerate(tqdm(articles, desc=\"ê¸°ì‚¬ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘\")):\n",
    "        title = article.get('title', '') or ''\n",
    "        summary = article.get('summary', '') or ''\n",
    "        \n",
    "        # ì œëª©ê³¼ ìš”ì•½ì´ ëª¨ë‘ ë¹„ì–´ìˆìœ¼ë©´ ìŠ¤í‚µ\n",
    "        if not title.strip() and not summary.strip():\n",
    "            continue\n",
    "            \n",
    "        # í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "        keywords = extract_single_article_keywords(article, top_k=5)\n",
    "        keywords_str = ' | '.join(keywords) if keywords else 'ì¶”ì¶œëœ í‚¤ì›Œë“œ ì—†ìŒ'\n",
    "        \n",
    "        results.append({\n",
    "            'article_id': i,\n",
    "            'title': title.strip(),\n",
    "            'summary': summary.strip(),\n",
    "            'extracted_keywords': keywords_str,\n",
    "            'num_keywords': len(keywords)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# =========================\n",
    "# ì‹¤í–‰ë¶€\n",
    "# =========================\n",
    "def main():\n",
    "    print(\"ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    # 1. ë°ì´í„° ë¡œë“œ\n",
    "    print(\"ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "    articles = load_articles(file_path)\n",
    "    if not articles:\n",
    "        print(\"âŒ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"âœ… ì´ {len(articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # 2. ê²€ì¦ ì²˜ë¦¬\n",
    "    print(\"ğŸ”„ ê¸°ì‚¬ë³„ í‚¤ì›Œë“œ ì¶”ì¶œì„ ì§„í–‰í•©ë‹ˆë‹¤...\")\n",
    "    results = process_articles_for_validation(articles, SAMPLE_SIZE)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"âŒ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    # 3. DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # 4. CSVë¡œ ì €ì¥\n",
    "    df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"ğŸ’¾ ê²°ê³¼ë¥¼ '{output_csv_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # 5. ê°„ë‹¨í•œ í†µê³„ ì¶œë ¥\n",
    "    print(\"\\nğŸ“Š ì¶”ì¶œ ê²°ê³¼ í†µê³„:\")\n",
    "    print(f\"- ì´ ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜: {len(results)}\")\n",
    "    print(f\"- í‚¤ì›Œë“œê°€ ì¶”ì¶œëœ ê¸°ì‚¬ ìˆ˜: {len(df[df['num_keywords'] > 0])}\")\n",
    "    print(f\"- í‚¤ì›Œë“œê°€ ì¶”ì¶œë˜ì§€ ì•Šì€ ê¸°ì‚¬ ìˆ˜: {len(df[df['num_keywords'] == 0])}\")\n",
    "    print(f\"- í‰ê·  í‚¤ì›Œë“œ ê°œìˆ˜: {df['num_keywords'].mean():.2f}\")\n",
    "    \n",
    "    # 6. ëª¨ë“  ê²°ê³¼ ìƒì„¸ ì¶œë ¥ (5ê°œë§Œ ì²˜ë¦¬í•˜ë¯€ë¡œ)\n",
    "    print(f\"\\nğŸ“‹ ì „ì²´ ì¶”ì¶œ ê²°ê³¼:\")\n",
    "    for i, row in df.iterrows():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ“° ê¸°ì‚¬ {i+1}\")\n",
    "        print(f\"ì œëª©: {row['title']}\")\n",
    "        print(f\"ìš”ì•½: {row['summary'][:200]}{'...' if len(row['summary']) > 200 else ''}\")\n",
    "        print(f\"ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {row['extracted_keywords']}\")\n",
    "        print(f\"í‚¤ì›Œë“œ ê°œìˆ˜: {row['num_keywords']}ê°œ\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    df_results = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715cef27",
   "metadata": {},
   "source": [
    "# ë‹¤ë¥¸ ëª¨ë¸ê³¼ ë¹„êµí•˜ì—¬ ë³´ê¸°\n",
    "\n",
    "### ì–´ë–»ê²Œ í•´ì„í•˜ë©´ ì¢‹ì„ì§€ (ë°œí‘œìš© í¬ì¸íŠ¸)\n",
    "\n",
    "- **ì„œë¡œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì˜ ê´€ì  ì°¨ì´**\n",
    "    - **TF-IDF**: â€œí•´ë‹¹ ì›” ë¬¸ì„œì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ ìì£¼ ë“±ì¥í•˜ëŠ” n-ê·¸ë¨â€ì„ ì°¾ì•„ìš”(ì½”í¼ìŠ¤ ê¸°ë°˜ ë¹ˆë„ ê°€ì¤‘).\n",
    "    - **YAKE**: ë‹¨ì¼ ë¬¸ì„œ(ì—¬ê¸°ì„œëŠ” ì›” ì½”í¼ìŠ¤ í•©ë³¸) ë‚´ë¶€ ë¶„í¬ íŠ¹ì„±(ìœ„ì¹˜/ëŒ€ë¬¸ì/ê¸¸ì´/ì¶œí˜„ ë¶„ì‚° ë“±)ì„ ì´ìš©í•œ **ì–¸ì–´ ë…ë¦½ì  í‚¤í”„ë ˆì´ì¦ˆ**.\n",
    "    - **KeyBERT**: ë¬¸ì„œ ì„ë² ë”©ê³¼ í›„ë³´êµ¬ ì„ë² ë”©ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ **ì˜ë¯¸ì  ê·¼ì ‘ì„±**ì„ ë³´ëŠ” ë°©ì‹.\n",
    "- **ê²€ì¦/ë¹„êµ ì§€í‘œ**\n",
    "    - **ìì¹´ë“œ(êµ¬/í† í° ë‹¨ìœ„)**: ê²°ê³¼ ì§‘í•©ì˜ ê²¹ì¹¨ ì •ë„(= ì¼ê´€ì„±).\n",
    "    - **ìŠ¤í”¼ì–´ë§Œ ìƒê´€**: ê³µí†µ í›„ë³´ë“¤ì˜ **ìˆœìœ„ ì¼ê´€ì„±**.\n",
    "    - **êµì§‘í•© Top ëª©ë¡**: í”„ë½í‹°ì»¬í•˜ê²Œ â€œëª¨ë‘ê°€ ì¤‘ìš”í•˜ë‹¤ê³  ë³´ëŠ” í‘œí˜„â€ì„ ë¹ ë¥´ê²Œ ì œì‹œ.\n",
    "- **ê¶Œì¥ í•´ì„ ì‹œë‚˜ë¦¬ì˜¤**\n",
    "    1. **êµì§‘í•© ìƒìœ„ í‚¤ì›Œë“œ**ëŠ” â€œí•µì‹¬ ì´ìŠˆì˜ ì‹ ë¢° ì½”ì–´â€ë¡œ ë‘ê³ ,\n",
    "    2. **ë°©ë²•ë³„ ê³ ìœ  ìƒìœ„ í‚¤ì›Œë“œ**ëŠ” ë³´ì™„ì  ê´€ì (ì˜ë¯¸ ì¤‘ì‹¬/ë¹ˆë„ ì¤‘ì‹¬/ê·¸ë˜í”„ ì¤‘ì‹¬)ìœ¼ë¡œ **ì‹ ë¢°ë„ ë³´ê°•/ë¦¬ë“œ ì‹ í˜¸ íƒìƒ‰**ì— í™œìš©í•˜ì„¸ìš”.\n",
    "    3. ì›”ë³„ë¡œ **ê²¹ì¹¨ë„ê°€ ë‚®ì•„ì§€ëŠ” êµ¬ê°„**ì€ ì´ìŠˆ êµ¬ì„±ì´ ë³€í•˜ëŠ” **ì „í™˜ì **ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f89329b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting krwordrank\n",
      "  Downloading krwordrank-1.0.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting yake\n",
      "  Downloading yake-0.6.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting keybert\n",
      "  Downloading keybert-0.9.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: numpy>=1.18.4 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from krwordrank) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from krwordrank) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from krwordrank) (1.7.1)\n",
      "Requirement already satisfied: click>=6.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from yake) (8.2.1)\n",
      "Collecting jellyfish (from yake)\n",
      "  Downloading jellyfish-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: networkx in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from yake) (3.5)\n",
      "Collecting segtok (from yake)\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting tabulate (from yake)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: rich>=10.4.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from keybert) (14.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from sentence-transformers) (4.56.1)\n",
      "Requirement already satisfied: tqdm in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from sentence-transformers) (2.8.0+cu128)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.9)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from rich>=10.4.0->keybert) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from rich>=10.4.0->keybert) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from scikit-learn>=0.22.1->krwordrank) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from scikit-learn>=0.22.1->krwordrank) (3.6.0)\n",
      "Requirement already satisfied: setuptools in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ds4_sia_nolb/miniconda3/envs/sia/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
      "Downloading krwordrank-1.0.3-py3-none-any.whl (20 kB)\n",
      "Downloading yake-0.6.0-py3-none-any.whl (80 kB)\n",
      "Downloading keybert-0.9.0-py3-none-any.whl (41 kB)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading jellyfish-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
      "Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate, segtok, jellyfish, yake, krwordrank, sentence-transformers, keybert\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7/7\u001b[0m [keybert]m5/7\u001b[0m [sentence-transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed jellyfish-1.2.0 keybert-0.9.0 krwordrank-1.0.3 segtok-1.5.11 sentence-transformers-5.1.0 tabulate-0.9.0 yake-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install krwordrank yake keybert sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f2e0f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²´ ë¬¸ì„œ ìˆ˜: 80810\n",
      "TF-IDF ë²¡í„°ë¼ì´ì € ì¤€ë¹„ ì™„ë£Œ\n",
      "\n",
      "======================================================================\n",
      "â–¶ 2024ë…„ 1ì›” ë¹„êµ ì‹¤í–‰\n",
      "ğŸ“š 2024-01 ë¬¸ì„œ ìˆ˜: 510\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ab97c7cbfa4fc59146700468c3eee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26d0d476547472d94558bb91b52d046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acee8d1270ef4755a681f1a9effcfee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691f5e2dace541e990458704f8b83e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1599501461464f29b3bf3dfabf1755fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6bab53436e42688a2e2977eefdc3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f6337d4a0340c3bd5b38ed40d1d0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d511334790ba47eea6222f7ef9b9c14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1b8a7b8d0645db88f889ef5ab1967a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18191198b8b445fea6dc2fe4418b2a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_01_methods_keywords.csv\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_01_full_result.json\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_01_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "â–¶ 2024ë…„ 2ì›” ë¹„êµ ì‹¤í–‰\n",
      "ğŸ“š 2024-02 ë¬¸ì„œ ìˆ˜: 377\n",
      "ğŸ“„ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_02_methods_keywords.csv\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_02_full_result.json\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_02_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "â–¶ 2024ë…„ 3ì›” ë¹„êµ ì‹¤í–‰\n",
      "ğŸ“š 2024-03 ë¬¸ì„œ ìˆ˜: 350\n",
      "ğŸ“„ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_03_methods_keywords.csv\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_03_full_result.json\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_03_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "â–¶ 2024ë…„ 4ì›” ë¹„êµ ì‹¤í–‰\n",
      "ğŸ“š 2024-04 ë¬¸ì„œ ìˆ˜: 311\n",
      "ğŸ“„ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_04_methods_keywords.csv\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_04_full_result.json\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_04_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "â–¶ 2024ë…„ 5ì›” ë¹„êµ ì‹¤í–‰\n",
      "ğŸ“š 2024-05 ë¬¸ì„œ ìˆ˜: 373\n",
      "ğŸ“„ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_05_methods_keywords.csv\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_05_full_result.json\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_05_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "â–¶ 2024ë…„ 6ì›” ë¹„êµ ì‹¤í–‰\n",
      "ğŸ“š 2024-06 ë¬¸ì„œ ìˆ˜: 493\n",
      "ğŸ“„ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_06_methods_keywords.csv\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_06_full_result.json\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_06_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "â–¶ 2024ë…„ 7ì›” ë¹„êµ ì‹¤í–‰\n",
      "ğŸ“š 2024-07 ë¬¸ì„œ ìˆ˜: 389\n",
      "ğŸ“„ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_07_methods_keywords.csv\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_07_full_result.json\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_07_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "â–¶ 2024ë…„ 8ì›” ë¹„êµ ì‹¤í–‰\n",
      "ğŸ“š 2024-08 ë¬¸ì„œ ìˆ˜: 281\n",
      "ğŸ“„ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_08_methods_keywords.csv\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_08_full_result.json\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_08_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "â–¶ 2024ë…„ 9ì›” ë¹„êµ ì‹¤í–‰\n",
      "ğŸ“š 2024-09 ë¬¸ì„œ ìˆ˜: 444\n",
      "ğŸ“„ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_09_methods_keywords.csv\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_09_full_result.json\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_09_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "â–¶ 2024ë…„ 10ì›” ë¹„êµ ì‹¤í–‰\n",
      "ğŸ“š 2024-10 ë¬¸ì„œ ìˆ˜: 1028\n",
      "ğŸ“„ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_10_methods_keywords.csv\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_10_full_result.json\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_10_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "â–¶ 2024ë…„ 11ì›” ë¹„êµ ì‹¤í–‰\n",
      "ğŸ“š 2024-11 ë¬¸ì„œ ìˆ˜: 692\n",
      "ğŸ“„ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_11_methods_keywords.csv\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_11_full_result.json\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_11_pairwise_metrics.json\n",
      "\n",
      "======================================================================\n",
      "â–¶ 2024ë…„ 12ì›” ë¹„êµ ì‹¤í–‰\n",
      "ğŸ“š 2024-12 ë¬¸ì„œ ìˆ˜: 350\n",
      "ğŸ“„ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_12_methods_keywords.csv\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_12_full_result.json\n",
      "ğŸ’¾ ì €ì¥: /home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks/2024_12_pairwise_metrics.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "ì›”ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ ì„±ëŠ¥ ë¹„êµ íŒŒì´í”„ë¼ì¸\n",
    "- Methods: TF-IDF, TextRank(krwordrank), YAKE, KeyBERT\n",
    "- Inputs:\n",
    "    1) ì „ì²´ ê¸°ì‚¬ JSON (list[dict])  â€” file_path (ì œëª©=metadata.title, ìš”ì•½=summary)\n",
    "    2) (ê°€ëŠ¥í•˜ë©´) ì „ì²´ì½”í¼ìŠ¤ ë²¡í„°ë¼ì´ì € pkl â€” TFIDF_VECTORIZER_PATH\n",
    "- Outputs (ê¸°ë³¸):\n",
    "    /home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/benchmarks/\n",
    "      â”œâ”€ YYYY_MM_methods_keywords.csv                (ë°©ë²•ë³„ TopK í‚¤ì›Œë“œ)\n",
    "      â”œâ”€ YYYY_MM_pairwise_metrics.json               (ë°©ë²•ìŒ ë¹„êµ ì§€í‘œ)\n",
    "      â””â”€ YYYY_MM_full_result.json                    (ëª¨ë“  ì›ì‹œ ê²°ê³¼/ì§€í‘œ ì¢…í•©)\n",
    "\"\"\"\n",
    "\n",
    "import os, re, json, math, calendar, joblib, warnings\n",
    "from typing import List, Dict, Tuple\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„¤ì¹˜ í•„ìš”)\n",
    "from krwordrank.word import KRWordRank\n",
    "import yake\n",
    "from keybert import KeyBERT\n",
    "\n",
    "# =========================\n",
    "# ê²½ë¡œ ë° ê³µí†µ ì„¤ì •\n",
    "# =========================\n",
    "file_path = '/home/ds4_sia_nolb/#FINAL_POLARIS/04_plus_preprocessing/preprocessing_final_data/final_preprocessing.json'\n",
    "BENCH_OUTDIR = '/home/ds4_sia_nolb/#FINAL_POLARIS/08_performance_evaluation/issue_performance_data/benchmarks'\n",
    "os.makedirs(BENCH_OUTDIR, exist_ok=True)\n",
    "\n",
    "TFIDF_VECTORIZER_PATH = '/home/ds4_sia_nolb/#FINAL_POLARIS/05_Event_top10/idf_vectorizer_for_all_corpus.pkl'\n",
    "TOP_K = 30\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# ë¶ˆìš©ì–´/ì •ê·œí™”\n",
    "# =========================\n",
    "BASE_STOP = {\n",
    "    'ê°€','ê°„','ê°™ì€','ê°™ì´','ê²ƒ','ê²Œë‹¤ê°€','ê²°êµ­','ê³§','ê´€í•˜ì—¬','ê´€ë ¨','ê´€í•œ','ê·¸','ê·¸ê²ƒ','ê·¸ë…€','ê·¸ë“¤',\n",
    "    'ê·¸ë¦¬ê³ ','ê·¸ë•Œ','ê·¸ë˜','ê·¸ë˜ì„œ','ê·¸ëŸ¬ë‚˜','ê·¸ëŸ¬ë¯€ë¡œ','ê·¸ëŸ¬í•œ','ê·¸ëŸ°','ê·¸ë ‡ê²Œ','ê·¸ì™¸','ê·¼ê±°ë¡œ','ê¸°íƒ€',\n",
    "    'ê¹Œì§€ë„','ê¹Œì§€','ë‚˜','ë‚¨ë“¤','ë„ˆ','ëˆ„êµ¬','ë‹¤','ë‹¤ê°€','ë‹¤ë¥¸','ë‹¤ë§Œ','ë‹¤ì†Œ','ë‹¤ìˆ˜','ë‹¤ì‹œ','ë‹¤ìŒ','ë‹¨','ë‹¨ì§€',\n",
    "    'ë‹¹ì‹ ','ëŒ€','ëŒ€í•´ì„œ','ë”êµ°ë‹¤ë‚˜','ë”êµ¬ë‚˜','ë”ë¼ë„','ë”ìš±ì´','ë„','ë„ë¡œ','ë˜','ë˜ëŠ”','ë˜í•œ','ë•Œ','ë•Œë¬¸',\n",
    "    'ë¼ë„','ë¼ë©´','ë¼ëŠ”','ë¡œ','ë¡œë¶€í„°','ë¡œì¨','ë¥¼','ë§ˆì €','ë§ˆì¹˜','ë§Œì•½','ë§Œì¼','ë§Œí¼','ëª¨ë‘','ë¬´ì—‡','ë¬´ìŠ¨',\n",
    "    'ë¬´ì²™','ë¬¼ë¡ ','ë°','ë°–ì—','ë°”ë¡œ','ë³´ë‹¤','ë¿ì´ë‹¤','ì‚¬ëŒ','ì‚¬ì‹¤ì€','ìƒëŒ€ì ìœ¼ë¡œ','ìƒê°','ì„¤ë ¹','ì†Œìœ„','ìˆ˜',\n",
    "    'ìˆ˜ì¤€','ì‰½ê²Œ','ì‹œëŒ€','ì‹œì‘í•˜ì—¬','ì‹¤ë¡œ','ì‹¤ì œ','ì•„ë‹ˆ','ì•„ë¬´','ì•„ë¬´ë„','ì•„ë¬´ë¦¬','ì•„ë§ˆë„','ì•„ìš¸ëŸ¬','ì•„ì§',\n",
    "    'ì•ì—ì„œ','ì•ìœ¼ë¡œ','ì–´ëŠ','ì–´ë–¤','ì–´ë–»ê²Œ','ì–´ë””','ì–¸ì œ','ì–¼ë§ˆë‚˜','ì—¬ê¸°','ì—¬ë¶€','ì—­ì‹œ','ì˜ˆ','ì˜¤íˆë ¤',\n",
    "    'ì™€','ì™œ','ì™¸ì—ë„','ìš”','ìš°ë¦¬','ìš°ì„ ','ì›ë˜','ìœ„í•´ì„œ','ìœ¼ë¡œ','ìœ¼ë¡œë¶€í„°','ìœ¼ë¡œì¨','ì„','ì˜','ì˜ê±°í•˜ì—¬',\n",
    "    'ì˜ì§€í•˜ì—¬','ì˜í•´','ì˜í•´ì„œ','ì˜í•˜ì—¬','ì´','ì´ê²ƒ','ì´ê³³','ì´ë•Œ','ì´ë¼ê³ ','ì´ëŸ¬í•œ','ì´ëŸ°','ì´ë ‡ê²Œ','ì´ì œ',\n",
    "    'ì´ì§€ë§Œ','ì´í›„','ì´ìƒ','ì´ë‹¤','ì´ì „','ì¸','ì¼','ì¼ë‹¨','ì¼ë°˜ì ìœ¼ë¡œ','ì„ì‹œë¡œ','ì…ì¥ì—ì„œ','ì','ìê¸°','ìì‹ ',\n",
    "    'ì ì‹œ','ì €','ì €ê²ƒ','ì €ê¸°','ì €ìª½','ì €í¬','ì „ë¶€','ì „í˜€','ì ì—ì„œ','ì •ë„','ì œ','ì¡°ê¸ˆ','ì¢€','ì£¼ë¡œ','ì£¼ì œ','ì¦‰',\n",
    "    'ì¦‰ì‹œ','ì§€ê¸ˆ','ì§„ì§œë¡œ','ì°¨ë¼ë¦¬','ì°¸','ì°¸ìœ¼ë¡œ','ì²«ë²ˆì§¸ë¡œ','ìµœê³ ','ìµœëŒ€','ìµœì†Œ','ìµœì‹ ','ìµœì´ˆ','í†µí•˜ì—¬',\n",
    "    'í†µí•´ì„œ','í‰ê°€','í¬í•¨í•œ','í¬í•¨í•˜ì—¬','í•˜ì§€ë§Œ','í•˜ë©´ì„œ','í•˜ì—¬','í•œ','í•œë•Œ','í•œë²ˆ','í• ','í• ê²ƒì´ë‹¤','í• ìˆ˜ìˆë‹¤',\n",
    "    'í•¨ê»˜','í•´ë„'\n",
    "}\n",
    "NEWS_STOP = {\"ê¸°ì\",\"ì—°í•©ë‰´ìŠ¤\",\"ì‚¬ì§„\",\"ì†ë³´\",\"ì¢…í•©\",\"ìë£Œ\",\"ì˜ìƒ\",\"ë‹¨ë…\",\"ì „ë¬¸\",\"ì¸í„°ë·°\",\"ë¸Œë¦¬í•‘\"}\n",
    "CUSTOM_STOPWORDS = BASE_STOP | NEWS_STOP\n",
    "\n",
    "ENTITY_NOISE = {\n",
    "    \"ë¶í•œ\",\"í•œêµ­\",\"ëŒ€í•œë¯¼êµ­\",\"ë‚¨í•œ\",\"ë¯¸êµ­\",\"ì¤‘êµ­\",\"ì¼ë³¸\",\"ëŸ¬ì‹œì•„\",\"ìš°í¬ë¼ì´ë‚˜\",\"ìœ ì—”\",\"ë‚˜í† \",\"NATO\",\"EU\",\"ìœ ëŸ½ì—°í•©\",\n",
    "    \"í‘¸í‹´\",\"ë¸”ë¼ë””ë¯¸ë¥´ í‘¸í‹´\",\"ë°”ì´ë“ \",\"ì¡° ë°”ì´ë“ \",\"ì‹œì§„í•‘\",\"ê¹€ì •ì€\",\"ê¹€ì—¬ì •\",\"ë¬¸ì¬ì¸\",\"ìœ¤ì„ì—´\",\"ì‡¼ì´êµ¬\",\"ì ¤ë ŒìŠ¤í‚¤\",\"ì¤‘ì•™\",\"í†µì‹ \",\"ë³´ë„\",\n",
    "    'ë¼ë‹¤','ì„œë‹¤','ëŒ€í•´','ë‚˜ì˜¤ë‹¤','í†µí•´','ë§ë‹¤'\n",
    "}\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    t = t.replace(\"íƒ„ë„ ë¯¸ì‚¬ì¼\",\"íƒ„ë„ë¯¸ì‚¬ì¼\").replace(\"ìˆœí•­ ë¯¸ì‚¬ì¼\",\"ìˆœí•­ë¯¸ì‚¬ì¼\")\n",
    "    t = t.replace(\"ê·¹ì´ˆ ìŒì†\",\"ê·¹ì´ˆìŒì†\").replace(\"ì´ˆëŒ€í˜• ë°©ì‚¬í¬\",\"ì´ˆëŒ€í˜•ë°©ì‚¬í¬\")\n",
    "    # ê³µë°± ì •ë¦¬\n",
    "    t = re.sub(r\"\\s+\",\" \", t).strip()\n",
    "    return t\n",
    "\n",
    "# =========================\n",
    "# ë‚ ì§œ/ì…ë ¥ ë¡œë”\n",
    "# =========================\n",
    "def parse_date_flexible(s: str):\n",
    "    if not s or not isinstance(s, str):\n",
    "        return None\n",
    "    s = s.strip()\n",
    "    cands = [s]\n",
    "    if \"T\" in s:\n",
    "        cands += [s[:19], s[:10]]\n",
    "    if len(s) >= 10:\n",
    "        cands.append(s[:10])\n",
    "    if \"-\" not in s and \".\" not in s and \"/\" not in s and len(s) == 8:\n",
    "        cands.append(f\"{s[:4]}-{s[4:6]}-{s[6:8]}\")\n",
    "    fmts = [\n",
    "        \"%Y-%m-%d\",\"%Y-%m-%d %H:%M:%S\",\"%Y-%m-%d %H:%M\",\n",
    "        \"%Y/%m/%d\",\"%Y/%m/%d %H:%M:%S\",\n",
    "        \"%Y.%m.%d\",\"%Y.%m.%d %H:%M:%S\",\"%Y.%m.%d %H:%M\",\n",
    "        \"%Y%m%d\",\"%Y-%m-%dT%H:%M:%S\"\n",
    "    ]\n",
    "    for c in cands:\n",
    "        for f in fmts:\n",
    "            try:\n",
    "                return datetime.strptime(c, f)\n",
    "            except:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "def extract_pubdate(a: dict):\n",
    "    keys = [\"pubDate\",\"pubdate\",\"time\",\"date\",\"published\",\"pub_date\"]\n",
    "    for k in keys:\n",
    "        if k in a and a[k]:\n",
    "            dt = parse_date_flexible(str(a[k]))\n",
    "            if dt: return dt\n",
    "    meta = a.get(\"metadata\", {}) or {}\n",
    "    for k in keys:\n",
    "        if k in meta and meta[k]:\n",
    "            dt = parse_date_flexible(str(meta[k]))\n",
    "            if dt: return dt\n",
    "    return None\n",
    "\n",
    "def doc_text(a: dict) -> str:\n",
    "    title = (a.get('metadata') or {}).get('title','')\n",
    "    summary = a.get('summary','') or ''\n",
    "    return normalize_text(f\"{title} {summary}\")\n",
    "\n",
    "def load_all_articles(path: str) -> List[dict]:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(\"JSON ë£¨íŠ¸ëŠ” list ì—¬ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "    return data\n",
    "\n",
    "def monthly_corpus(year: int, month: int, all_articles: List[dict]) -> List[str]:\n",
    "    last_day = calendar.monthrange(year, month)[1]\n",
    "    sdt = datetime(year, month, 1)\n",
    "    edt = datetime(year, month, last_day, 23, 59, 59)\n",
    "    docs = []\n",
    "    for a in all_articles:\n",
    "        d = extract_pubdate(a)\n",
    "        if d and sdt <= d <= edt:\n",
    "            text = doc_text(a)\n",
    "            if text:\n",
    "                docs.append(text)\n",
    "    return docs\n",
    "\n",
    "# =========================\n",
    "# TF-IDF (ì „ì²´ ì½”í¼ìŠ¤ í•™ìŠµ/ë¡œë“œ)\n",
    "# =========================\n",
    "\n",
    "# ê¸°ì¡´ pkl í˜¸í™˜ì„ ìœ„í•œ ë³„ì¹­(aliased) - ì´ì „ ì´ë¦„ ìœ ì§€\n",
    "def tokenizer_for_vectorizer(s: str):\n",
    "    return tokenizer_simple_ko(s)\n",
    "\n",
    "def tokenizer_simple_ko(s: str) -> List[str]:\n",
    "    # ì•„ì£¼ ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì €: í•œê¸€/ì˜ë¬¸/ìˆ«ì ë‹¨ì–´ ê¸°ì¤€ + ê¸¸ì´>=2 + ë¶ˆìš©ì–´ ì œì™¸\n",
    "    toks = re.findall(r\"[ê°€-í£A-Za-z0-9]+\", s)\n",
    "    out = []\n",
    "    for w in toks:\n",
    "        if len(w) <= 1: continue\n",
    "        if w in CUSTOM_STOPWORDS: continue\n",
    "        if w.isdigit(): continue\n",
    "        out.append(w)\n",
    "    return out\n",
    "\n",
    "def load_or_train_vectorizer(all_articles: List[dict], path: str) -> TfidfVectorizer:\n",
    "    if os.path.exists(path):\n",
    "        return joblib.load(path)\n",
    "    corpus = [doc_text(a) for a in all_articles]\n",
    "    vec = TfidfVectorizer(\n",
    "        tokenizer=tokenizer_simple_ko,\n",
    "        ngram_range=(1,3),\n",
    "        min_df=5,\n",
    "        max_df=0.85,\n",
    "        sublinear_tf=True,\n",
    "        norm='l2'\n",
    "    )\n",
    "    vec.fit(corpus)\n",
    "    joblib.dump(vec, path)\n",
    "    return vec\n",
    "\n",
    "def tfidf_top_phrases(docs: List[str], vectorizer: TfidfVectorizer, top_k=30) -> List[Tuple[str, float]]:\n",
    "    if not docs:\n",
    "        return []\n",
    "    X = vectorizer.transform(docs)\n",
    "    avg = np.asarray(X.mean(axis=0)).ravel()\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    pairs = [(terms[i], float(avg[i])) for i in np.where(avg>0)[0]]\n",
    "    # ì—”í„°í‹° ë…¸ì´ì¦ˆ ì•½ë²Œ\n",
    "    def penalty(term):\n",
    "        toks = term.split()\n",
    "        ent_hits = sum(1 for t in toks if any(ent in t for ent in ENTITY_NOISE))\n",
    "        return -0.05 * ent_hits\n",
    "    scored = [(t, s + penalty(t)) for t,s in pairs]\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scored[:top_k]\n",
    "\n",
    "# =========================\n",
    "# TextRank (KRWordRank) â†’ ë‹¨ì–´ìŠ¤ì½”ì–´ë¡œ êµ¬(phrase) ìŠ¤ì½”ì–´\n",
    "# =========================\n",
    "from krwordrank.word import KRWordRank\n",
    "\n",
    "def textrank_top_phrases(docs: List[str], top_k=30) -> List[Tuple[str, float]]:\n",
    "    if not docs:\n",
    "        return []\n",
    "    # ë¬¸ì„œ ìˆ˜ê°€ ì ìœ¼ë©´ min_countë¥¼ ë‚®ì¶°ì•¼ í‚¤ì›Œë“œê°€ ë‚˜ì˜µë‹ˆë‹¤.\n",
    "    min_count = 5 if len(docs) >= 100 else 2\n",
    "\n",
    "    kr = KRWordRank(min_count=min_count, max_length=10, verbose=False)\n",
    "\n",
    "    # beta(0~1): í…”ë ˆí¬í…Œì´ì…˜ ê°€ì¤‘, max_iter: ë°˜ë³µ\n",
    "    try:\n",
    "        # ì‹ ë²„ì „ í˜¸í™˜ (delta ì§€ì›)\n",
    "        keywords, rank, _ = kr.extract(docs, beta=0.85, max_iter=50, delta=0.001)\n",
    "    except TypeError:\n",
    "        # êµ¬ë²„ì „ í˜¸í™˜ (delta ë¯¸ì§€ì›)\n",
    "        keywords, rank, _ = kr.extract(docs, beta=0.85, max_iter=50)\n",
    "\n",
    "    # 1~3ê·¸ë¨ phrase ìŠ¤ì½”ì–´ë§ (ë‹¨ì–´ rank í•©ì‚°)\n",
    "    phrases = Counter()\n",
    "    for text in docs:\n",
    "        words = re.findall(r\"[ê°€-í£A-Za-z0-9]+\", text)\n",
    "        words = [w for w in words if w not in CUSTOM_STOPWORDS and len(w) > 1]\n",
    "        for n in (1, 2, 3):\n",
    "            for i in range(len(words) - n + 1):\n",
    "                ph = \" \".join(words[i:i+n])\n",
    "                # ì—”í„°í‹° ë…¸ì´ì¦ˆ ê³¼ë‹¤ í¬í•¨ êµ¬ ì œì™¸\n",
    "                if n <= 2 and any(ent in ph for ent in ENTITY_NOISE):\n",
    "                    continue\n",
    "                score = sum(rank.get(w, 0.0) for w in words[i:i+n])\n",
    "                if score > 0:\n",
    "                    phrases[ph] += score\n",
    "\n",
    "    scored = list(phrases.items())\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scored[:top_k]\n",
    "\n",
    "# =========================\n",
    "# YAKE\n",
    "# =========================\n",
    "def yake_top_phrases(docs: List[str], top_k=30) -> List[Tuple[str, float]]:\n",
    "    if not docs:\n",
    "        return []\n",
    "    text = \"\\n\".join(docs)\n",
    "    # ë‚®ì€ ì ìˆ˜ê°€ ë” ì¢‹ìŒ â†’ 1/scoreë¡œ ë’¤ì§‘ì–´ ì •ë ¬\n",
    "    kw_extractor = yake.KeywordExtractor(\n",
    "        lan=\"ko\", n=3, # 1~3ê·¸ë¨ ìë™ íƒìƒ‰\n",
    "        dedupLim=0.9, windowsSize=1, top=top_k*3,\n",
    "        features=None\n",
    "    )\n",
    "    candidates = kw_extractor.extract_keywords(text)\n",
    "    # í›„ë³´ ì •ë¦¬: ë¶ˆìš©ì–´/ìˆ«ì/ì§§ì€ í† í° ì œê±°\n",
    "    cleaned = []\n",
    "    for phrase, score in candidates:\n",
    "        ph = \" \".join([w for w in re.findall(r\"[ê°€-í£A-Za-z0-9]+\", phrase) if len(w)>1 and w not in CUSTOM_STOPWORDS])\n",
    "        if not ph: continue\n",
    "        cleaned.append((ph, score))\n",
    "    # ì¤‘ë³µ ì¶•ì•½ (ë™ì¼ phraseëŠ” ìµœê³  ì ìˆ˜ë§Œ ë‚¨ê¹€)\n",
    "    best = {}\n",
    "    for ph, sc in cleaned:\n",
    "        inv = 1.0/max(sc, 1e-9)\n",
    "        best[ph] = max(best.get(ph, 0.0), inv)\n",
    "    ranked = sorted(best.items(), key=lambda x: x[1], reverse=True)\n",
    "    return ranked[:top_k]\n",
    "\n",
    "# =========================\n",
    "# KeyBERT (ë©€í‹°ë§êµ¬ì–¼ ì‚¬ì „í•™ìŠµ ì„ë² ë”©)\n",
    "# =========================\n",
    "_KEYBERT_MODEL = None\n",
    "def get_keybert():\n",
    "    global _KEYBERT_MODEL\n",
    "    if _KEYBERT_MODEL is None:\n",
    "        # ë‹¤êµ­ì–´ ëª¨ë¸ (ê°€ë²¼ì›€, ko ì§€ì›)\n",
    "        _KEYBERT_MODEL = KeyBERT(model='paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    return _KEYBERT_MODEL\n",
    "\n",
    "def keybert_top_phrases(docs: List[str], top_k=30) -> List[Tuple[str, float]]:\n",
    "    if not docs:\n",
    "        return []\n",
    "    text = \"\\n\".join(docs)\n",
    "    kw = get_keybert()\n",
    "    # KeyBERT ì ìˆ˜: cos sim (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)\n",
    "    candidates = kw.extract_keywords(\n",
    "        text,\n",
    "        keyphrase_ngram_range=(1,3),\n",
    "        stop_words=list(CUSTOM_STOPWORDS),\n",
    "        top_n=top_k*5\n",
    "    )\n",
    "    # ì •ë¦¬ + ì¤‘ë³µ ì œê±°\n",
    "    agg = {}\n",
    "    for ph, sc in candidates:\n",
    "        ph2 = \" \".join([w for w in re.findall(r\"[ê°€-í£A-Za-z0-9]+\", ph) if len(w)>1 and w not in CUSTOM_STOPWORDS])\n",
    "        if not ph2: continue\n",
    "        agg[ph2] = max(agg.get(ph2, 0.0), float(sc))\n",
    "    ranked = sorted(agg.items(), key=lambda x: x[1], reverse=True)\n",
    "    return ranked[:top_k]\n",
    "\n",
    "# =========================\n",
    "# ë¹„êµ ì§€í‘œ/ì¶œë ¥\n",
    "# =========================\n",
    "def to_rank_dict(items: List[Tuple[str, float]]) -> Dict[str, int]:\n",
    "    return {ph: i for i,(ph,_) in enumerate(items, start=1)}\n",
    "\n",
    "def jaccard(a: List[str], b: List[str]) -> float:\n",
    "    sa, sb = set(a), set(b)\n",
    "    if not sa and not sb: return 1.0\n",
    "    if not sa or not sb: return 0.0\n",
    "    return len(sa & sb) / len(sa | sb)\n",
    "\n",
    "def token_jaccard(a: List[str], b: List[str]) -> float:\n",
    "    ta = set(sum([ph.split() for ph in a], []))\n",
    "    tb = set(sum([ph.split() for ph in b], []))\n",
    "    if not ta and not tb: return 1.0\n",
    "    if not ta or not tb: return 0.0\n",
    "    return len(ta & tb) / len(ta | tb)\n",
    "\n",
    "def spearman_on_common(a_items: List[Tuple[str,float]], b_items: List[Tuple[str,float]]) -> float:\n",
    "    ra, rb = to_rank_dict(a_items), to_rank_dict(b_items)\n",
    "    common = [ph for ph in ra if ph in rb]\n",
    "    if len(common) < 3:\n",
    "        return float('nan')\n",
    "    xa = [ra[ph] for ph in common]\n",
    "    xb = [rb[ph] for ph in common]\n",
    "    rho, _ = spearmanr(xa, xb)\n",
    "    return float(rho)\n",
    "\n",
    "def intersec_top(a_items, b_items, top=15) -> List[Tuple[str, int, int]]:\n",
    "    ra, rb = to_rank_dict(a_items), to_rank_dict(b_items)\n",
    "    common = [(ph, ra[ph], rb[ph]) for ph in ra if ph in rb]\n",
    "    common.sort(key=lambda x: (x[1]+x[2]))\n",
    "    return common[:top]\n",
    "\n",
    "def save_csv_per_method(year, month, results: Dict[str, List[Tuple[str,float]]], outdir=BENCH_OUTDIR):\n",
    "    rows = []\n",
    "    for m, items in results.items():\n",
    "        for rank, (ph, sc) in enumerate(items, start=1):\n",
    "            rows.append({\"year\":year, \"month\":month, \"method\":m, \"rank\":rank, \"phrase\":ph, \"score\":sc})\n",
    "    path = os.path.join(outdir, f\"{year}_{month:02d}_methods_keywords.csv\")\n",
    "    # CSV ì§ì ‘ ì‘ì„±(í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬)\n",
    "    import csv\n",
    "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        wr = csv.DictWriter(f, fieldnames=[\"year\",\"month\",\"method\",\"rank\",\"phrase\",\"score\"])\n",
    "        wr.writeheader()\n",
    "        wr.writerows(rows)\n",
    "    print(f\"ğŸ“„ ì €ì¥: {path}\")\n",
    "\n",
    "def compare_methods(year:int, month:int, all_articles: List[dict], vectorizer: TfidfVectorizer):\n",
    "    docs = monthly_corpus(year, month, all_articles)\n",
    "    print(f\"ğŸ“š {year}-{month:02d} ë¬¸ì„œ ìˆ˜: {len(docs)}\")\n",
    "    if not docs:\n",
    "        print(\"âš ï¸ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    results = {}\n",
    "    # 1) TF-IDF\n",
    "    results[\"tfidf\"]    = tfidf_top_phrases(docs, vectorizer, TOP_K)\n",
    "    # 2) TextRank\n",
    "    results[\"textrank\"] = textrank_top_phrases(docs, TOP_K)\n",
    "    # 3) YAKE\n",
    "    results[\"yake\"]     = yake_top_phrases(docs, TOP_K)\n",
    "    # 4) KeyBERT\n",
    "    results[\"keybert\"]  = keybert_top_phrases(docs, TOP_K)\n",
    "\n",
    "    # ìŒë³„ ë¹„êµ ì§€í‘œ\n",
    "    methods = list(results.keys())\n",
    "    pairwise = {}\n",
    "    for i in range(len(methods)):\n",
    "        for j in range(i+1, len(methods)):\n",
    "            a, b = methods[i], methods[j]\n",
    "            a_items, b_items = results[a], results[b]\n",
    "            a_ph = [p for p,_ in a_items]\n",
    "            b_ph = [p for p,_ in b_items]\n",
    "            pairwise[f\"{a}_vs_{b}\"] = {\n",
    "                \"jaccard_phrase\": round(jaccard(a_ph, b_ph), 3),\n",
    "                \"jaccard_token\": round(token_jaccard(a_ph, b_ph), 3),\n",
    "                \"spearman_rank_on_common\": (None if math.isnan(spearman_on_common(a_items, b_items)) else round(spearman_on_common(a_items, b_items), 3)),\n",
    "                \"intersection_top\": [\n",
    "                    {\"phrase\": ph, \"rank_in_\"+a: ra, \"rank_in_\"+b: rb}\n",
    "                    for ph, ra, rb in intersec_top(a_items, b_items, top=15)\n",
    "                ]\n",
    "            }\n",
    "\n",
    "    # ì €ì¥\n",
    "    save_csv_per_method(year, month, results, BENCH_OUTDIR)\n",
    "\n",
    "    # JSON ì¢…í•© ì €ì¥\n",
    "    out_full = {\n",
    "        \"year\": year, \"month\": month,\n",
    "        \"n_docs\": len(docs),\n",
    "        \"top_k\": TOP_K,\n",
    "        \"results\": {\n",
    "            m: [{\"phrase\": ph, \"score\": float(sc)} for ph, sc in items]\n",
    "            for m, items in results.items()\n",
    "        },\n",
    "        \"pairwise_metrics\": pairwise\n",
    "    }\n",
    "    jpath = os.path.join(BENCH_OUTDIR, f\"{year}_{month:02d}_full_result.json\")\n",
    "    with open(jpath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out_full, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"ğŸ’¾ ì €ì¥: {jpath}\")\n",
    "\n",
    "    # ë³„ë„: pairwiseë§Œ ì €ì¥\n",
    "    ppath = os.path.join(BENCH_OUTDIR, f\"{year}_{month:02d}_pairwise_metrics.json\")\n",
    "    with open(ppath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out_full[\"pairwise_metrics\"], f, ensure_ascii=False, indent=2)\n",
    "    print(f\"ğŸ’¾ ì €ì¥: {ppath}\")\n",
    "\n",
    "def main():\n",
    "    # 1) ì „ì²´ ê¸°ì‚¬ ë¡œë“œ\n",
    "    all_articles = load_all_articles(file_path)\n",
    "    print(f\"ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(all_articles)}\")\n",
    "\n",
    "    # 2) TF-IDF ë²¡í„°ë¼ì´ì € ë¡œë“œ/í•™ìŠµ\n",
    "    vectorizer = load_or_train_vectorizer(all_articles, TFIDF_VECTORIZER_PATH)\n",
    "    print(\"TF-IDF ë²¡í„°ë¼ì´ì € ì¤€ë¹„ ì™„ë£Œ\")\n",
    "\n",
    "    # 3) ì‹¤í–‰ ëŒ€ìƒ ì—°ì›” ì„¤ì • (ì˜ˆ: 2024ë…„ 1~12ì›”)\n",
    "    year = 2024\n",
    "    target_months = list(range(1,13))\n",
    "\n",
    "    for m in target_months:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"â–¶ {year}ë…„ {m}ì›” ë¹„êµ ì‹¤í–‰\")\n",
    "        compare_methods(year, m, all_articles, vectorizer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed6b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
